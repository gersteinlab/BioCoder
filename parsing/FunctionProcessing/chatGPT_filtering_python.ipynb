{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T01:15:20.130017069Z",
     "start_time": "2023-04-19T01:15:19.336990130Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken as tk\n",
    "import tenacity\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "enc = tk.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "data = pd.read_json('/home/ubuntu/CodeGen/BCE/FileStructure/Python/PythonSelected/all_functions.json')\n",
    "token_limit = 3500\n",
    "openai.api_key = api_key\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "gpt_outputs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T01:19:24.880603680Z",
     "start_time": "2023-04-19T01:19:24.744743303Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input Shape: (9543, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['filePath', 'numLines', 'lineStart', 'lineEnd', 'numParams',\n",
       "       'signature', 'comment', 'numCommentLines', 'content', 'parentClass',\n",
       "       'packageName', 'imports', 'repository', 'tokenCount', 'summary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Original Input Shape: \" + str(data.shape))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/9543 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '45616f69caf14889bd80178562de6a28b4d67c793f31663abc7c4951dafad020.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[39mreturn\u001b[39;00m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m tqdm(data\u001b[39m.\u001b[39miterrows(), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing rows\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m   gpt_outputs\u001b[39m.\u001b[39mappend(get_gpt_response(index))\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mget_gpt_response\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_gpt_response\u001b[39m(index):\n\u001b[1;32m     12\u001b[0m   summary \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mindex\u001b[39m}\u001b[39;49;00m\u001b[39m.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m     summary \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m     15\u001b[0m   completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     16\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     messages\u001b[39m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39m}]\n\u001b[1;32m     43\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '45616f69caf14889bd80178562de6a28b4d67c793f31663abc7c4951dafad020.txt'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import tenacity\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "gpt_outputs = []\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(15),\n",
    "    wait=wait_random_exponential(multiplier=2, min=2, max=150),\n",
    ")\n",
    "def get_gpt_response(index):\n",
    "  summary = ''\n",
    "  with open(f'{index}.txt', 'r') as f:\n",
    "    summary = f.read()\n",
    "  completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": f\"\"\"I will provide you with a prompt to a function below these instructions. You will output exactly as follows, with the list as well. Text encased in <like this> will be replaced by your response, and text encased in (like this) is just a description for the response that you do not need to type up:\n",
    "  (a) <Boolean> (Is it bioinformatics related?)\n",
    "  (b) <words> (Give a list of 5 keywords of why it is bioinformatics related)\n",
    "  (c) <integer> (Your confidence from 0 to 100 that your response in A is accurate, so for example, if you believe strongly that it is not bioinformatics related, you should also rate a high confidence level)\n",
    "  The code must explicitly reference some bioinformatics methodology, terminology, or process. For example, an AVL Tree would not be a valid bioinformatics function, while a FASTQ processor would. The keywords are defined as important words that allowed you to make the determination that the function is bioinformatics related. The confidence should be your estimate of how confident you are of your responses.\n",
    "\n",
    "  Make sure that in your response is explicitly as follows in the directions. Part A should only be one word and a boolean, either True or False. Part B should only be 5 words, no additional information, Part C should only be a single integer, from 0 to 100, it is a measure of your confidence in your response to Part A.\n",
    "\n",
    "  After selecting keywords, please reverify that the words you used to make the decision for Part A is actually bioinformatics related.\n",
    "  Again, as clarification, I will be providing the function. \n",
    "\n",
    "  The responses should be formatted as a list:\n",
    "  Entry 1: The response to part A converted into a string\n",
    "  Entry 2: A list of 5 words which are strings from the response to Part B\n",
    "  Entry 3: The integer response to part C converted to a string\n",
    "  Therefore, your output should follow this guideline. This will be your only output, there should be no additional outputs beyond the one highlighted in this prompt.\n",
    "\n",
    "  Prompt begins here:\n",
    "  Here is the function:\n",
    "  {data.iloc[index]['content']}\n",
    "  \\n Here is the summary:\n",
    "  {summary}\n",
    "  Prompt ends here.\n",
    "  Give the output to the above code encased in \"Prompt begins here:\" and \"Prompt ends here.\" Your keyword search should only encompass the words in the prompt, and ensure that keywords are related to bioinformatics, not statistics.\n",
    "  \"\"\"}]\n",
    "  )\n",
    "  return completion.choices[0].message['content']\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=len(data), desc=\"Processing rows\"):\n",
    "  gpt_outputs.append(get_gpt_response(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1026\n",
      "(1027, 22)\n",
      "Index(['filePath', 'totalNumLines', 'lineStart', 'lineEnd', 'numParameters',\n",
      "       'signature', 'comments', 'numComments', 'content', 'parentClass',\n",
      "       'packageName', 'imports', 'repoAuthor', 'repoName', 'numChar',\n",
      "       'addImports', 'returnType', 'license', 'numCommentLines', 'prompt',\n",
      "       'good_length', 'Token Length'],\n",
      "      dtype='object')\n",
      "                                               filePath  totalNumLines   \n",
      "1044          ./github-repos/spladder/spladder/reads.py             14  \\\n",
      "1045   ./github-repos/spladder/spladder/viz/coverage.py             41   \n",
      "1046   ./github-repos/spladder/spladder/viz/coverage.py             64   \n",
      "1047   ./github-repos/spladder/spladder/viz/coverage.py            135   \n",
      "1048  ./github-repos/paleomix/paleomix/common/fileut...             12   \n",
      "\n",
      "      lineStart  lineEnd  numParameters   \n",
      "1044        635      648              3  \\\n",
      "1045        125      165             21   \n",
      "1046        168      231             13   \n",
      "1047        233      367             33   \n",
      "1048         56       67              2   \n",
      "\n",
      "                                              signature   \n",
      "1044         def get_intron_range(introns, start, stop)  \\\n",
      "1045  def heatmap_from_bam(chrm, start, stop, files,...   \n",
      "1046  def cov_from_segments(gene, seg_counts, edge_c...   \n",
      "1047  def cov_from_bam(chrm, start, stop, files, sub...   \n",
      "1048                        def swap_ext(filename, ext)   \n",
      "\n",
      "                                               comments  numComments   \n",
      "1044  Given a sorted list of introns, return the sub...            1  \\\n",
      "1045  This function takes a list of bam files and a ...            1   \n",
      "1046  This function takes a gene and its correspondi...            1   \n",
      "1047  This function takes a list of bam files and a ...            1   \n",
      "1048  Replaces the existing extension of a filename ...            2   \n",
      "\n",
      "                                                content  parentClass  ...   \n",
      "1044  def get_intron_range(introns, start, stop):\\n ...          NaN  ...  \\\n",
      "1045  def heatmap_from_bam(chrm, start, stop, files,...          NaN  ...   \n",
      "1046  def cov_from_segments(gene, seg_counts, edge_c...          NaN  ...   \n",
      "1047  def cov_from_bam(chrm, start, stop, files, sub...          NaN  ...   \n",
      "1048  def swap_ext(filename: str, ext: str) ->str:\\n...          NaN  ...   \n",
      "\n",
      "          repoAuthor  repoName numChar addImports  returnType license   \n",
      "1044       ratschlab  spladder     313       None         NaN   other  \\\n",
      "1045       ratschlab  spladder    1287       None         NaN   other   \n",
      "1046       ratschlab  spladder    2206       None         NaN   other   \n",
      "1047       ratschlab  spladder    5083       None         NaN   other   \n",
      "1048  MikkelSchubert  paleomix     461       None         NaN     mit   \n",
      "\n",
      "      numCommentLines                                             prompt   \n",
      "1044              NaN  []\\nNone\\nGiven a sorted list of introns, retu...  \\\n",
      "1045              NaN  []\\nNone\\nThis function takes a list of bam fi...   \n",
      "1046              NaN  []\\nNone\\nThis function takes a gene and its c...   \n",
      "1047              NaN  []\\nNone\\nThis function takes a list of bam fi...   \n",
      "1048              NaN  []\\nNone\\nReplaces the existing extension of a...   \n",
      "\n",
      "      good_length Token Length  \n",
      "1044         True          131  \n",
      "1045         True          440  \n",
      "1046         True          606  \n",
      "1047         True         1483  \n",
      "1048         True          155  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Index(['filePath', 'totalNumLines', 'lineStart', 'lineEnd', 'numParameters',\n",
      "       'signature', 'comments', 'numComments', 'content', 'parentClass',\n",
      "       'packageName', 'imports', 'repoAuthor', 'repoName', 'numChar',\n",
      "       'addImports', 'returnType', 'license', 'numCommentLines', 'prompt',\n",
      "       'good_length', 'Token Length'],\n",
      "      dtype='object')\n",
      "1026\n",
      "Index(['filePath', 'totalNumLines', 'lineStart', 'lineEnd', 'numParameters',\n",
      "       'signature', 'comments', 'numComments', 'content', 'parentClass',\n",
      "       'packageName', 'imports', 'repoAuthor', 'repoName', 'numChar',\n",
      "       'addImports', 'returnType', 'license', 'numCommentLines', 'prompt',\n",
      "       'good_length', 'Token Length', 'gpt_output'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(len(gpt_outputs))\n",
    "gpt_outputs = [value for value in gpt_outputs if value != 'AAAAAAAAA']\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "print(data.tail(5))\n",
    "\n",
    "print(data.columns)\n",
    "print(len(gpt_outputs))\n",
    "data.drop(data.tail(1).index, inplace=True)\n",
    "data['gpt_output'] = gpt_outputs\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1026, 23)\n",
      "Index(['filePath', 'totalNumLines', 'lineStart', 'lineEnd', 'numParameters',\n",
      "       'signature', 'comments', 'numComments', 'content', 'parentClass',\n",
      "       'packageName', 'imports', 'repoAuthor', 'repoName', 'numChar',\n",
      "       'addImports', 'returnType', 'license', 'numCommentLines', 'prompt',\n",
      "       'good_length', 'Token Length', 'gpt_output'],\n",
      "      dtype='object')\n",
      "                                              filePath  totalNumLines   \n",
      "1043          ./github-repos/spladder/spladder/hdf5.py             39  \\\n",
      "1044         ./github-repos/spladder/spladder/reads.py             14   \n",
      "1045  ./github-repos/spladder/spladder/viz/coverage.py             41   \n",
      "1046  ./github-repos/spladder/spladder/viz/coverage.py             64   \n",
      "1047  ./github-repos/spladder/spladder/viz/coverage.py            135   \n",
      "\n",
      "      lineStart  lineEnd  numParameters   \n",
      "1043          4       42              5  \\\n",
      "1044        635      648              3   \n",
      "1045        125      165             21   \n",
      "1046        168      231             13   \n",
      "1047        233      367             33   \n",
      "\n",
      "                                              signature   \n",
      "1043   def appendToHDF5(file, data, name, faxis, daxis)  \\\n",
      "1044         def get_intron_range(introns, start, stop)   \n",
      "1045  def heatmap_from_bam(chrm, start, stop, files,...   \n",
      "1046  def cov_from_segments(gene, seg_counts, edge_c...   \n",
      "1047  def cov_from_bam(chrm, start, stop, files, sub...   \n",
      "\n",
      "                                               comments  numComments   \n",
      "1043  Goal of this function is to append more data t...            2  \\\n",
      "1044  Given a sorted list of introns, return the sub...            1   \n",
      "1045  This function takes a list of bam files and a ...            1   \n",
      "1046  This function takes a gene and its correspondi...            1   \n",
      "1047  This function takes a list of bam files and a ...            1   \n",
      "\n",
      "                                                content  parentClass  ...   \n",
      "1043  def appendToHDF5(file, data, name, faxis=0, da...          NaN  ...  \\\n",
      "1044  def get_intron_range(introns, start, stop):\\n ...          NaN  ...   \n",
      "1045  def heatmap_from_bam(chrm, start, stop, files,...          NaN  ...   \n",
      "1046  def cov_from_segments(gene, seg_counts, edge_c...          NaN  ...   \n",
      "1047  def cov_from_bam(chrm, start, stop, files, sub...          NaN  ...   \n",
      "\n",
      "      repoName numChar addImports returnType  license numCommentLines   \n",
      "1043  spladder    1081       None        NaN    other             NaN  \\\n",
      "1044  spladder     313       None        NaN    other             NaN   \n",
      "1045  spladder    1287       None        NaN    other             NaN   \n",
      "1046  spladder    2206       None        NaN    other             NaN   \n",
      "1047  spladder    5083       None        NaN    other             NaN   \n",
      "\n",
      "                                                 prompt good_length   \n",
      "1043  []\\nNone\\nGoal of this function is to append m...        True  \\\n",
      "1044  []\\nNone\\nGiven a sorted list of introns, retu...        True   \n",
      "1045  []\\nNone\\nThis function takes a list of bam fi...        True   \n",
      "1046  []\\nNone\\nThis function takes a gene and its c...        True   \n",
      "1047  []\\nNone\\nThis function takes a list of bam fi...        True   \n",
      "\n",
      "      Token Length                                         gpt_output  \n",
      "1043           374  ['True', ['dictionary', 'recursively', 'copy',...  \n",
      "1044           131  ['True', ['BED', 'compressed', 'contigs', 'seq...  \n",
      "1045           440  ['True', ['BED', 'BAM file', 'linearly', 'proc...  \n",
      "1046           606  ['True', ['OptionParser', 'logging', 'config',...  \n",
      "1047          1483  ['True', ['list', 'line-breaks', 'minimize', '...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.columns)\n",
    "print(data.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0T\n",
      "Transform width into a half-window size.\n",
      "\n",
      "`width` is either a fraction of the length of `x` or an integer size of the\n",
      "whole window. The output half-window size is truncated to the length of `x`\n",
      "if needed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1T\n",
      "Convolve a weighted window over a weighted signal array.\n",
      "\n",
      "Source: https://stackoverflow.com/a/46232913/10049\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2T\n",
      "Convolve a weighted window over array `signal`.\n",
      "\n",
      "Input array is assumed padded by `_pad_array`; output has padding removed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3T\n",
      "Choose a reasonable window size given the signal.\n",
      "\n",
      "Inspired by Silverman's rule: bandwidth is proportional to signal's standard\n",
      "deviation and the length of the signal ^ 4/5.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4T\n",
      "Smooth the values in `x` with the Kaiser windowed filter.\n",
      "\n",
      "See: https://en.wikipedia.org/wiki/Kaiser_window\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "x : array-like\n",
      "    1-dimensional numeric data set.\n",
      "width : float\n",
      "    Fraction of x's total length to include in the rolling window (i.e. the\n",
      "    proportional window width), or the integer size of the window.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5T\n",
      "Savitzky-Golay smoothing.\n",
      "\n",
      "Fitted polynomial order is typically much less than half the window width.\n",
      "\n",
      "`total_width` overrides `n_iter`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7T\n",
      "Given a 1-D array `x` and the specification of a slice of `x` from\n",
      "`window_start` to `window_stop`, create an interpolating polynomial of the\n",
      "sliced sub-array, and evaluate that polynomial from `interp_start` to\n",
      "`interp_stop`.  Put the result into the corresponding slice of `y`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8T\n",
      "Detect outliers as a multiple of the IQR from the median.\n",
      "\n",
      "By convention, \"outliers\" are points more than 1.5 * IQR from the median,\n",
      "and \"extremes\" or extreme outliers are those more than 3.0 * IQR.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9T\n",
      "MAD-Median rule for detecting outliers.\n",
      "\n",
      "X_i is an outlier if::\n",
      "\n",
      "     | X_i - M |\n",
      "    _____________  > K ~= 2.24\n",
      "\n",
      "     MAD / 0.6745\n",
      "\n",
      "where $K = sqrt( X^2_{0.975,1} )$,\n",
      "the square root of the 0.975 quantile of a chi-squared distribution with 1\n",
      "degree of freedom.\n",
      "\n",
      "This is a very robust rule with the highest possible breakdown point of 0.5.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "np.array\n",
      "    A boolean array of the same size as `a`, where outlier indices are True.\n",
      "\n",
      "References\n",
      "----------\n",
      "- Davies & Gather (1993) The Identification of Multiple Outliers.\n",
      "- Rand R. Wilcox (2012) Introduction to robust estimation and hypothesis\n",
      "  testing. Ch.3: Estimating measures of location and scale.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10T\n",
      "Detect outliers as a multiple of the IQR from the median.\n",
      "\n",
      "By convention, \"outliers\" are points more than 1.5 * IQR from the median (~2\n",
      "SD if values are normally distributed), and \"extremes\" or extreme outliers\n",
      "are those more than 3.0 * IQR (~4 SD).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11T\n",
      "Detect outliers by multiples of a quantile in a window.\n",
      "\n",
      "Outliers are the array elements outside `m` times the `q`'th\n",
      "quantile of deviations from the smoothed trend line, as calculated from\n",
      "the trend line residuals. (For example, take the magnitude of the 95th\n",
      "quantile times 5, and mark any elements greater than that value as\n",
      "outliers.)\n",
      "\n",
      "This is the smoothing method used in BIC-seq (doi:10.1073/pnas.1110574108)\n",
      "with the parameters width=200, q=.95, m=5 for WGS.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "np.array\n",
      "    A boolean array of the same size as `x`, where outlier indices are True.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12T\n",
      "Detect outliers by stdev within a rolling window.\n",
      "\n",
      "Outliers are the array elements outside `stdevs` standard deviations from\n",
      "the smoothed trend line, as calculated from the trend line residuals.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "np.array\n",
      "    A boolean array of the same size as `x`, where outlier indices are True.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13T\n",
      "Markov-cluster control samples by their read depths' correlation.\n",
      "\n",
      "Each of the matrices in the resulting iterable (list) can be processed the\n",
      "same as the input to calculate average log2 and spread values for that\n",
      "cluster.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "samples : array\n",
      "    Matrix of samples' read depths or normalized log2 values, as columns.\n",
      "inflation : float\n",
      "    Inflation parameter for MCL. Must be >1; higher more granular clusters.\n",
      "by_pca : bool\n",
      "    If true, similarity is by PCA; otherwise, by Pearson correlation.\n",
      "\n",
      "Return\n",
      "------\n",
      "results : list\n",
      "    A list of matrices representing non-overlapping column-subsets of the\n",
      "    input, where each set of samples represents a cluster.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14T\n",
      "Apply cluster inflation with the given element-wise exponent.\n",
      "\n",
      "From the mcl manual:\n",
      "\n",
      "This value is the main handle for affecting cluster granularity.\n",
      "This parameter is the usually only one that may require tuning.\n",
      "\n",
      "By default it is set to 2.0 and this is a good way to start. If you want to\n",
      "explore cluster structure in graphs with MCL, vary this parameter to obtain\n",
      "clusterings at different levels of granularity.  It is usually chosen\n",
      "somewhere in the range [1.2-5.0]. -I 5.0 will tend to result in fine-grained\n",
      "clusterings, and -I 1.2 will tend to result in very coarse grained\n",
      "clusterings. A good set of starting values is 1.4, 2, 4, and 6.\n",
      "Your mileage will vary depending on the characteristics of your data.\n",
      "\n",
      "Low values for -I, like -I 1.2, will use more CPU/RAM resources.\n",
      "\n",
      "Use mcl's cluster validation tools 'clm dist' and 'clm info' to test the\n",
      "quality and coherency of your clusterings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "16T\n",
      "Extract clusters from the matrix.\n",
      "\n",
      "Interpretation: \"Attractors\" are the non-zero elements of the matrix\n",
      "diagonal. The nodes in the same row as each attractor form a cluster.\n",
      "\n",
      "Overlapping clusterings produced by MCL are extremely rare, and always a\n",
      "result of symmetry in the input graph.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "result : list\n",
      "    A list of arrays of sample indices. The indices in each list item\n",
      "    indicate the elements of that cluster; the length of the list is the\n",
      "    number of clusters.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17T\n",
      "Remove many small entries while retaining most of M's stochastic mass.\n",
      "\n",
      "After pruning, vectors are rescaled to be stochastic again.\n",
      "(stochastic: values are all non-negative and sum to 1.)\n",
      "\n",
      "This step is purely to keep computation tractable in mcl by making the\n",
      "matrix more sparse (i.e. full of zeros), enabling sparse-matrix tricks to\n",
      "work.\n",
      "\n",
      "----\n",
      "\n",
      "mcl:\n",
      "    The default setting is something like -P 4000 -S 500 -R 600, where:\n",
      "\n",
      "  -P <int> (1/cutoff)\n",
      "  -S <int> (selection number)\n",
      "  -R <int> (recover number)\n",
      "  ---\n",
      "  -pct <pct> (recover percentage)\n",
      "  -p <num> (cutoff)\n",
      "\n",
      "After computing a new (column stochastic) matrix vector during expansion\n",
      "(which  is  matrix  multiplication c.q.  squaring), the vector is\n",
      "successively exposed to different pruning strategies. Pruning effectively\n",
      "perturbs the MCL process a little in order to obtain matrices that are\n",
      "genuinely sparse, thus keeping the computation tractable.\n",
      "\n",
      "mcl proceeds as follows:\n",
      "\n",
      "First, entries that are smaller than cutoff are\n",
      "removed, resulting in a vector with  at most 1/cutoff entries.\n",
      "\n",
      "    * The cutoff can be supplied either by -p, or as the inverse value by\n",
      "    -P.  The latter is more intuitive, if your intuition is like mine (P\n",
      "    stands for precision or pruning).\n",
      "\n",
      "Second, if the remaining stochastic mass (i.e. the sum of all remaining\n",
      "entries) is less than <pct>/100 and the number of remaining entries is\n",
      "less than <r> (as specified by the -R flag), mcl will try to regain ground\n",
      "by recovering the largest discarded entries. If recovery was not necessary,\n",
      "mcl tries to prune the vector further down to at most s entries (if\n",
      "applicable), as specified by the -S flag. If this results in a vector that\n",
      "satisfies the recovery condition then recovery is attempted, exactly as\n",
      "described above. The latter will not occur of course if <r> <= <s>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18T\n",
      "Principal component analysis using scikit-learn.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "data : 2D NumPy array\n",
      "n_components : int\n",
      "\n",
      "Returns: PCA-transformed data with `n_components` columns.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19T\n",
      "Principal component analysis using numpy eigenvalues.\n",
      "\n",
      "Source:\n",
      "https://stackoverflow.com/questions/13224362/principal-component-analysis-pca-in-python\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "data : 2D NumPy array\n",
      "n_components : int\n",
      "\n",
      "Returns: PCA-transformed data with `n_components` columns.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "20T\n",
      "Scatter plot first 2 components, colorized by cluster.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "M : np.array\n",
      "    PCA'd matrix. Rows are samples.\n",
      "cluster_indices : iterable of np.array\n",
      "    Indices of samples in each cluster, as present in M.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "21T\n",
      "Get chromosome names, lengths, and number of mapped/unmapped reads.\n",
      "\n",
      "Use the BAM index (.bai) to get the number of reads and size of each\n",
      "chromosome. Contigs with no mapped reads are skipped.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "23T\n",
      "Ensure a BAM file is indexed, to enable fast traversal & lookup.\n",
      "\n",
      "For MySample.bam, samtools will look for an index in these files, in order:\n",
      "\n",
      "- MySample.bam.bai\n",
      "- MySample.bai\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "24T\n",
      "Test if the reads in a BAM file are sorted as expected.\n",
      "\n",
      "by_name=True: reads are expected to be sorted by query name. Consecutive\n",
      "read IDs are in alphabetical order, and read pairs appear together.\n",
      "\n",
      "by_name=False: reads are sorted by position. Consecutive reads have\n",
      "increasing position.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "25T\n",
      "Get (median) read length from first few reads in a BAM file.\n",
      "\n",
      "Illumina reads all have the same length; other sequencers might not.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "bam : str or pysam.AlignmentFile\n",
      "    Filename or pysam-opened BAM file.\n",
      "n : int\n",
      "    Number of reads used to calculate median read length.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "26T\n",
      "Reduce multi-accession interval labels to the minimum consistent.\n",
      "\n",
      "So: BED or interval_list files have a label for every region. We want this\n",
      "to be a short, unique string, like the gene name. But if an interval list is\n",
      "instead a series of accessions, including additional accessions for\n",
      "sub-regions of the gene, we can extract a single accession that covers the\n",
      "maximum number of consecutive regions that share this accession.\n",
      "\n",
      "e.g.::\n",
      "\n",
      "    ...\n",
      "    mRNA|JX093079,ens|ENST00000342066,mRNA|JX093077,ref|SAMD11,mRNA|AF161376,mRNA|JX093104\n",
      "    ens|ENST00000483767,mRNA|AF161376,ccds|CCDS3.1,ref|NOC2L\n",
      "    ...\n",
      "\n",
      "becomes::\n",
      "\n",
      "    ...\n",
      "    mRNA|AF161376\n",
      "    mRNA|AF161376\n",
      "    ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "27T\n",
      "Tally genomic locations of each targeted gene.\n",
      "\n",
      "Return a dict of chromosomes to a list of tuples: (gene name, starts, end),\n",
      "where gene name is a string, starts is a sorted list of probe start\n",
      "positions, and end is the last probe's end position as an integer. (The\n",
      "endpoints are redundant since probes are adjacent.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "28T\n",
      "Identify genes where average bin copy ratio value exceeds `threshold`.\n",
      "\n",
      "NB: Adjust the sample's sex-chromosome log2 values beforehand with shift_xx,\n",
      "otherwise all chrX/chrY genes may be reported gained/lost.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "29T\n",
      "Identify genes where segmented copy ratio exceeds `threshold`.\n",
      "\n",
      "In the output table, show each segment's weight and probes as segment_weight\n",
      "and segment_probes, alongside the gene-level weight and probes.\n",
      "\n",
      "NB: Adjust the sample's sex-chromosome log2 values beforehand with shift_xx,\n",
      "otherwise all chrX/chrY genes may be reported gained/lost.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "30T\n",
      "Group probe and coverage data by gene.\n",
      "\n",
      "Return an iterable of genes, in chromosomal order, associated with their\n",
      "location and coverages:\n",
      "\n",
      "    [(gene, chrom, start, end, [coverages]), ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "31T\n",
      "Estimators of scale: standard deviation, MAD, biweight midvariance.\n",
      "\n",
      "Calculates all of these values for an array of deviations and returns them\n",
      "as a tuple.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "32T\n",
      "Flag the bins with excessively low or inconsistent coverage.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "np.array\n",
      "    A boolean array where True indicates bins that failed the checks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "33T\n",
      "Smooth out biases according to the trait specified by sort_key.\n",
      "\n",
      "E.g. correct GC-biased bins by windowed averaging across similar-GC\n",
      "bins; or for similar interval sizes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "34T\n",
      "Quantify the \"edge effect\" of the target tile and its neighbors.\n",
      "\n",
      "The result is proportional to the change in the target's coverage due to\n",
      "these edge effects, i.e. the expected loss of coverage near the target\n",
      "edges and, if there are close neighboring tiles, gain of coverage due\n",
      "to \"spill over\" reads from the neighbor tiles.\n",
      "\n",
      "(This is not the actual change in coverage. This is just a tribute.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "35T\n",
      "Calculate coverage losses at the edges of baited regions.\n",
      "\n",
      "Letting i = insert size and t = target size, the proportional loss of\n",
      "coverage near the two edges of the baited region (combined) is:\n",
      "\n",
      ".. math :: i/2t\n",
      "\n",
      "If the \"shoulders\" extend outside the bait $(t < i), reduce by:\n",
      "\n",
      ".. math :: (i-t)^2 / 4it\n",
      "\n",
      "on each side, or (i-t)^2 / 2it total.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "36T\n",
      "Calculate coverage gain from neighboring baits' flanking reads.\n",
      "\n",
      "Letting i = insert size, t = target size, g = gap to neighboring bait,\n",
      "the gain of coverage due to a nearby bait, if g < i, is::\n",
      "\n",
      ".. math :: (i-g)^2 / 4it\n",
      "\n",
      "If the neighbor flank extends beyond the target (t+g < i), reduce by::\n",
      "\n",
      ".. math :: (i-t-g)^2 / 4it\n",
      "\n",
      "If a neighbor overlaps the target, treat it as adjacent (gap size 0).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "37T\n",
      "Calculate weights for each bin.\n",
      "\n",
      "Bin weight is an estimate of (1 - variance) and within the range\n",
      "``(0, 1]``.\n",
      "\n",
      "Weights are derived from:\n",
      "\n",
      "- Each bin's size\n",
      "- Sample's genome-wide average (on/off-target) coverage depth\n",
      "- Sample's genome-wide observed (on/off-target) bin variances\n",
      "\n",
      "And with a pooled reference:\n",
      "\n",
      "- Each bin's coverage depth in the reference\n",
      "- The \"spread\" column of the reference (approx. stdev)\n",
      "\n",
      "These estimates of variance assume the number of aligned reads per bin\n",
      "follows a Poisson distribution, approximately log-normal.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "cnarr : CopyNumArray\n",
      "    Sample bins.\n",
      "ref_match : CopyNumArray\n",
      "    Reference bins.\n",
      "log2_key : string\n",
      "    The 'log2' column name in the reference to use. A clustered reference\n",
      "    may have a suffix indicating the cluster, e.g. \"log2_1\".\n",
      "spread_key : string\n",
      "    The 'spread' or 'spread_<cluster_id>' column name to use.\n",
      "epsilon : float\n",
      "    Minimum value for bin weights, to avoid 0-weight bins causing errors\n",
      "    later during segmentation. (CBS doesn't allow 0-weight bins.)\n",
      "\n",
      "Returns: The input `cnarr` with a `weight` column added.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "38T\n",
      "Calculate depth of a region via pysam count.\n",
      "\n",
      "i.e. counting the number of read starts in a region, then scaling for read\n",
      "length and region width to estimate depth.\n",
      "\n",
      "Coordinates are 0-based, per pysam.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "39T\n",
      "Calculate depth of all regions in a BED file via samtools (pysam) bedcov.\n",
      "\n",
      "i.e. mean pileup depth across each region.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "40T\n",
      "Determine which 'bedcov' output columns to keep.\n",
      "\n",
      "Format is the input BED plus a final appended column with the count of\n",
      "basepairs mapped within each row's region. The input BED might have 3\n",
      "columns (regions without names), 4 (named regions), or more (arbitrary\n",
      "columns after 'gene').\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "41T\n",
      "Filter probes to only include high-quality, transcribed genes.\n",
      "\n",
      "The human genome has ~25,000 protein coding genes, yet the RSEM output\n",
      "includes ~58,000 rows. Some of these rows correspond to read counts over\n",
      "control probes (e.g.  spike-in sequences). Some rows correspond to poorly\n",
      "mapped genes in contigs that have not been linked to the 24 chromosomes\n",
      "(e.g. HLA region). Others correspond to pseudo-genes and non-coding genes.\n",
      "For the purposes of copy number inference, these rows are best removed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "42T\n",
      "Read gene info from BioMart, and optionally TCGA, into a dataframe.\n",
      "\n",
      "RSEM only outputs the Ensembl ID. We have used the BioMART tool in Ensembl\n",
      "to export a list of all Ensembl genes with a RefSeq mRNA (meaning it is high\n",
      "quality, curated, and bona fide gene) and resides on chromosomes 1-22, X, or\n",
      "Y. The tool also outputs the GC content of the gene, chromosomal coordinates\n",
      "of the gene, and HUGO gene symbol.\n",
      "\n",
      "The gene resource input can be obtained from a resource bundle we provide\n",
      "(for reference genome hg19) or generated from BioMart.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "43T\n",
      "Convert an Ensembl Transcript Support Level (TSL) code to an integer.\n",
      "\n",
      "The code has the format \"tsl([1-5]|NA)\".\n",
      "\n",
      "See: https://www.ensembl.org/Help/Glossary?id=492\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "44T\n",
      "Emit the \"best\" index from a group of the same Ensembl ID.\n",
      "\n",
      "The RSEM gene rows are the data of interest, and they're associated with\n",
      "Ensembl IDs to indicate the transcribed gene being measured in each row.\n",
      "The BioMart table of gene info can have duplicate rows for Ensembl ID, which\n",
      "would cause duplicate rows in RSEM import if joined naively.  So, we need to\n",
      "select a single row for each group of \"gene info\" rows with the same Ensembl\n",
      "ID (column 'gene_id').\n",
      "\n",
      "The keys we can use for this are:\n",
      "\n",
      "- Entrez ID ('entrez_id')\n",
      "- Ensembl gene name ('gene')\n",
      "- Entrez gene name ('hugo_gene')\n",
      "\n",
      "Entrez vs. Ensembl IDs and gene names are potentially many-to-many, e.g.\n",
      "CALM1/2/3. However, if we also require that the Ensembl and HUGO gene names\n",
      "match within a group, that (usually? always?) results in a unique row\n",
      "remaining.\n",
      "\n",
      "(Example: CALM1/2/3 IDs are many-to-many, but of the 3 Entrez IDs associated\n",
      "with Ensembl's CALM1, only 1 is called CALM1 in the Entrez/corr. table.)\n",
      "\n",
      "Failing that (no matches or multiple matches), prefer a lower Entrez ID,\n",
      "because well-characterized, protein-coding genes tend to have been\n",
      "discovered and accessioned first.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "45T\n",
      "Deduplicate table rows to select one transcript length per gene.\n",
      "\n",
      "Choose the lowest-number Entrez ID and the transcript with the greatest\n",
      "support (primarily) and length (secondarily).\n",
      "\n",
      "This is done at the end of Ensembl ID deduplication, after filtering on gene\n",
      "names and for single-row tables.\n",
      "\n",
      "Returns an integer row index corresponding to the original table.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "46T\n",
      "In case the same Entrez ID was assigned to multiple Ensembl IDs.\n",
      "\n",
      "Use HUGO vs. HGNC name again, similar to `dedupe_hugo`, but instead of\n",
      "emiting the indices of the rows to keep, emit the indices of the extra rows\n",
      "-- their correlation values will then be filled in with a default value\n",
      "(np.nan or 0.1).\n",
      "\n",
      "It will then be as if those genes hadn't appeared in the TCGA tables at all,\n",
      "i.e. CNV-expression correlation is unknown, but all entries are still\n",
      "retained in the BioMart table (gene_info).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "47T\n",
      "Align columns and sort.\n",
      "\n",
      "Also calculate weights and add to gene_info as 'weight', along with\n",
      "transcript lengths as 'tx_length'.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "48T\n",
      "Normalize read depths within each sample.\n",
      "\n",
      "Some samples have higher sequencing depth and therefore read depths need to\n",
      "be normalized within each sample. TCGA recommends an upper quartile\n",
      "normalization.\n",
      "\n",
      "After normalizing read depths within each sample, normalize (median-center)\n",
      "within each gene, across samples.\n",
      "\n",
      "Finally, convert to log2 ratios.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "49T\n",
      "Transform values to log2 scale, safely handling zeros.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "values : np.array\n",
      "    Absolute-scale values to transform. Should be non-negative.\n",
      "min_log2 : float\n",
      "    Assign input zeros this log2-scaled value instead of -inf. Rather than\n",
      "    hard-clipping, input values near 0 (especially below 2^min_log2) will be\n",
      "    squeezed a bit above `min_log2` in the log2-scale output.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "50T\n",
      "Join gene info to each sample's log2 expression ratios.\n",
      "\n",
      "Add the Ensembl gene info to the aggregated gene expected read counts,\n",
      "dropping genes that are not in the Ensembl table\n",
      "I.e., filter probes down to those genes that have names/IDs in the gene\n",
      "resource table.\n",
      "\n",
      "Split out samples to individual .cnr files, keeping (most) gene info.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "51T\n",
      "Apply bias corrections & smoothing.\n",
      "\n",
      "- Biases: 'gc', 'length'\n",
      "- Smoothing: rolling triangle window using weights.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "52T\n",
      "Ensure `a` and `w` are equal-length numpy arrays with no NaN values.\n",
      "\n",
      "For weighted descriptives -- `a` is the array of values, `w` is weights.\n",
      "\n",
      "1. Drop any cells in `a` that are NaN from both `a` and `w`\n",
      "2. Replace any remaining NaN cells in `w` with 0.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "53T\n",
      "Compute the biweight location for an array.\n",
      "\n",
      "The biweight is a robust statistic for estimating the central location of a\n",
      "distribution.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "58T\n",
      "Mean squared error (MSE).\n",
      "\n",
      "By default, assume the input array `a` is the residuals/deviations/error,\n",
      "so MSE is calculated from zero. Another reference point for calculating the\n",
      "error can be specified with `initial`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "59T\n",
      "Rousseeuw & Croux's (1993) Q_n, an alternative to MAD.\n",
      "\n",
      "``Qn := Cn first quartile of (|x_i - x_j|: i < j)``\n",
      "\n",
      "where Cn is a constant depending on n.\n",
      "\n",
      "Finite-sample correction factors must be used to calibrate the\n",
      "scale of Qn for small-to-medium-sized samples.\n",
      "\n",
      "    n   E[Qn]\n",
      "    --  -----\n",
      "    10  1.392\n",
      "    20  1.193\n",
      "    40  1.093\n",
      "    60  1.064\n",
      "    80  1.048\n",
      "    100 1.038\n",
      "    200 1.019\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "60T\n",
      "Fix the duplicated gene names Picard spits out.\n",
      "\n",
      "Return a string containing the single gene name, sans duplications and pipe\n",
      "characters.\n",
      "\n",
      "Picard CalculateHsMetrics combines the labels of overlapping intervals\n",
      "by joining all labels with '|', e.g. 'BRAF|BRAF' -- no two distinct\n",
      "targeted genes actually overlap, though, so these dupes are redundant.\n",
      "Meaningless target names are dropped, e.g. 'CGH|FOO|-' resolves as 'FOO'.\n",
      "In case of ambiguity, the longest name is taken, e.g. \"TERT|TERT Promoter\"\n",
      "resolves as \"TERT Promoter\".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "62T\n",
      "Safely run a command and get stdout; print stderr if there's an error.\n",
      "\n",
      "Like subprocess.check_output, but silent in the normal case where the\n",
      "command logs unimportant stuff to stderr. If there is an error, then the\n",
      "full error message(s) is shown in the exception message.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "64T\n",
      "Save text to a temporary file.\n",
      "\n",
      "NB: This won't work on Windows b/c the file stays open.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "65T\n",
      "Evaluate and compare two or more values for equality.\n",
      "\n",
      "Sugar for a common assertion pattern. Saves re-evaluating (and retyping)\n",
      "the same values for comparison and error reporting.\n",
      "\n",
      "Example:\n",
      "\n",
      ">>> assert_equal(\"Mismatch\", expected=1, saw=len(['xx', 'yy']))\n",
      "...\n",
      "ValueError: Mismatch: expected = 1, saw = 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "66T\n",
      "Convert absolute copy numbers to log2 ratios.\n",
      "\n",
      "Optionally round copy numbers to integers.\n",
      "\n",
      "Account for reference sex & ploidy of sex chromosomes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "67T\n",
      "Call integer copy number using hard thresholds for each level.\n",
      "\n",
      "Integer values are assigned for log2 ratio values less than each given\n",
      "threshold value in sequence, counting up from zero.\n",
      "Above the last threshold value, integer copy numbers are called assuming\n",
      "full purity, diploidy, and rounding up.\n",
      "\n",
      "Default thresholds follow this heuristic for calling CNAs in a tumor sample:\n",
      "For single-copy gains and losses, assume 50% tumor cell clonality (including\n",
      "normal cell contamination). Then::\n",
      "\n",
      "    R> log2(2:6 / 4)\n",
      "    -1.0  -0.4150375  0.0  0.3219281  0.5849625\n",
      "\n",
      "Allowing for random noise of +/- 0.1, the cutoffs are::\n",
      "\n",
      "    DEL(0)  <  -1.1\n",
      "    LOSS(1) <  -0.25\n",
      "    GAIN(3) >=  +0.2\n",
      "    AMP(4)  >=  +0.7\n",
      "\n",
      "For germline samples, better precision could be achieved with::\n",
      "\n",
      "    LOSS(1) <  -0.4\n",
      "    GAIN(3) >=  +0.3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "68T\n",
      "Absolute integer number of expected copies in each bin.\n",
      "\n",
      "I.e. the given ploidy for autosomes, and XY or XX sex chromosome counts\n",
      "according to the sample's specified chromosomal sex.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "69T\n",
      "Absolute integer number of reference copies in each bin.\n",
      "\n",
      "I.e. the given ploidy for autosomes, 1 or 2 X according to the reference\n",
      "sex, and always 1 copy of Y.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "70T\n",
      "Determine the number copies of a chromosome expected and in reference.\n",
      "\n",
      "For sex chromosomes, these values may not be the same ploidy as the\n",
      "autosomes. The \"reference\" number is the chromosome's ploidy in the\n",
      "CNVkit reference, while \"expect\" is the chromosome's neutral ploidy in the\n",
      "given sample, based on the specified sex of each. E.g., given a female\n",
      "sample and a male reference, on chromosome X the \"reference\" value is 1 but\n",
      "\"expect\" is 2.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "tuple\n",
      "    A pair of integers: number of copies in the reference, and expected in\n",
      "    the sample.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "71T\n",
      "Determine the reference number of chromosome copies (pure sample).\n",
      "\n",
      "Returns\n",
      "-------\n",
      "int\n",
      "    Number of copies in the reference.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "72T\n",
      "Transform a log2 ratio to absolute linear scale (for an impure sample).\n",
      "\n",
      "Does not round to an integer absolute value here.\n",
      "\n",
      "Math::\n",
      "\n",
      "    log2_ratio = log2(ncopies / ploidy)\n",
      "    2^log2_ratio = ncopies / ploidy\n",
      "    ncopies = ploidy * 2^log2_ratio\n",
      "\n",
      "With rescaling for purity::\n",
      "\n",
      "    let v = log2 ratio value, p = tumor purity,\n",
      "        r = reference ploidy, x = expected ploidy,\n",
      "        n = tumor ploidy (\"ncopies\" above);\n",
      "\n",
      "    v = log_2(p*n/r + (1-p)*x/r)\n",
      "    2^v = p*n/r + (1-p)*x/r\n",
      "    n*p/r = 2^v - (1-p)*x/r\n",
      "    n = (r*2^v - x*(1-p)) / p\n",
      "\n",
      "If purity adjustment is skipped (p=1; e.g. if germline or if scaling for\n",
      "heterogeneity was done beforehand)::\n",
      "\n",
      "    n = r*2^v\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "73T\n",
      "Transform a log2 ratio to absolute linear scale (for a pure sample).\n",
      "\n",
      "Purity adjustment is skipped. This is appropriate if the sample is germline\n",
      "or if scaling for tumor heterogeneity was done beforehand.\n",
      "\n",
      ".. math :: n = r*2^v\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "74T\n",
      "Adjust B-allele frequencies for sample purity.\n",
      "\n",
      "Math::\n",
      "\n",
      "    t_baf*purity + n_baf*(1-purity) = obs_baf\n",
      "    obs_baf - n_baf * (1-purity) = t_baf * purity\n",
      "    t_baf = (obs_baf - n_baf * (1-purity))/purity\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "75T\n",
      "Select the median-size file from several given filenames.\n",
      "\n",
      "If an even number of files is given, selects the file just below the median.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "76T\n",
      "Quickly calculate reasonable bin sizes from BAM read counts.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "bam_fname : string\n",
      "    BAM filename.\n",
      "method : string\n",
      "    One of: 'wgs' (whole-genome sequencing), 'amplicon' (targeted amplicon\n",
      "    capture), 'hybrid' (hybridization capture).\n",
      "targets : GenomicArray\n",
      "    Targeted genomic regions (for 'hybrid' and 'amplicon').\n",
      "access : GenomicArray\n",
      "    Sequencing-accessible regions of the reference genome (for 'hybrid' and\n",
      "    'wgs').\n",
      "bp_per_bin : int\n",
      "    Desired number of sequencing read nucleotide bases mapped to each bin.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "2-tuple of 2-tuples:\n",
      "    ((target depth, target avg. bin size),\n",
      "     (antitarget depth, antitarget avg. bin size))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "77T\n",
      "Estimate the average read depth across the genome.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "float\n",
      "    Median of the per-chromosome mean read depths, weighted by chromosome\n",
      "    size.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "78T\n",
      "Wrapper to coordinate the segment-filtering functions.\n",
      "\n",
      "Verify that the given columns are in the CopyNumArray the wrapped function\n",
      "takes. Also log the number of rows in the array before and after filtration.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "79T\n",
      "Assign a unique integer to each run of identical values.\n",
      "\n",
      "Repeated but non-consecutive values will be assigned different integers.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "80T\n",
      "Reduce a CopyNumArray to 1 row, keeping fields sensible.\n",
      "\n",
      "Most fields added by the `segmetrics` command will be dropped.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "81T\n",
      "Merge segments by amplified/deleted/neutral copy number status.\n",
      "\n",
      "Follow the clinical reporting convention:\n",
      "\n",
      "- 5+ copies (2.5-fold gain) is amplification\n",
      "- 0 copies is homozygous/deep deletion\n",
      "- CNAs of lesser degree are not reported\n",
      "\n",
      "This is recommended only for selecting segments corresponding to\n",
      "actionable, usually focal, CNAs. Any real and potentially informative but\n",
      "lower-level CNAs will be dropped.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "82T\n",
      "Merge segments by Bayesian Information Criterion.\n",
      "\n",
      "See: BIC-seq (Xi 2011), doi:10.1073/pnas.1110574108\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "83T\n",
      "Merge segments by confidence interval (overlapping 0).\n",
      "\n",
      "Segments with lower CI above 0 are kept as gains, upper CI below 0 as\n",
      "losses, and the rest with CI overlapping zero are collapsed as neutral.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "84T\n",
      "Merge segments by Standard Error of the Mean (SEM).\n",
      "\n",
      "Use each segment's SEM value to estimate a 95% confidence interval (via\n",
      "`zscore`). Segments with lower CI above 0 are kept as gains, upper CI below\n",
      "0 as losses, and the rest with CI overlapping zero are collapsed as neutral.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "92T\n",
      "Normalize tumor-sample allele frequencies.\n",
      "\n",
      "boosted = { 0.5 (t/n)           if t < n\n",
      "            1 - 0.5(1-t)/(1-n)  otherwise\n",
      "\n",
      "See: TumorBoost, Bengtsson et al. 2010\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "93T\n",
      "Split total copy number between alleles based on BAF.\n",
      "\n",
      "See: PSCBS, Bentsson et al. 2011\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "94T\n",
      "Get a probability for each bin based on its Z-score.\n",
      "\n",
      "Adds a column w/ p-values to the input .cnr. With `segments`, the Z-score is\n",
      "relative to the enclosing segment's mean, otherwise it is relative to 0.\n",
      "\n",
      "Bin p-values are corrected for multiple hypothesis testing by the\n",
      "Benjamini-Hochberg method.\n",
      "\n",
      "Returns: bins where the probability < `alpha`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "95T\n",
      "Given chromosome sizes, plot divider lines and labels.\n",
      "\n",
      "Draws black lines between each chromosome, with padding. Labels each chromosome range with the chromosome name,\n",
      "centered in the region, under a tick. Sets the axis limits to the covered range.\n",
      "\n",
      "By default, the dividers are vertical and the labels are on the X axis of the plot. If the `along` parameter is 'y',\n",
      "this is transposed to horizontal dividers and the labels on the Y axis.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "OrderedDict\n",
      "    A table of the position offsets of each chromosome along the specified axis.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "96T\n",
      "Map genomic coordinates to bin indices.\n",
      "\n",
      "Return a tuple of (chrom, start, end), just like unpack_range.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "97T\n",
      "Convert start/end positions from genomic to bin-wise coordinates.\n",
      "\n",
      "Instead of chromosomal basepairs, the positions indicate enumerated bins.\n",
      "\n",
      "Revise the start and end values for all GenomicArray instances at once,\n",
      "where the `cnarr` bins are mapped to corresponding `segments`, and\n",
      "`variants` are grouped into `cnarr` bins as well -- if multiple `variants`\n",
      "rows fall within a single bin, equally-spaced fractional positions are used.\n",
      "\n",
      "Returns copies of the 3 input objects with revised `start` and `end` arrays.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "98T\n",
      "Find the chromosomal position of each named gene in probes.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "dict\n",
      "    Of: {chromosome: [(start, end, gene name), ...]}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "99T\n",
      "Find the chromosomal position of all genes in a range.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "dict\n",
      "    Of: {chromosome: [(start, end, gene), ...]}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100T\n",
      "Plot a specified region on one chromosome.\n",
      "\n",
      "Possibilities::\n",
      "\n",
      "         Options | Shown\n",
      "    ------------ | --------\n",
      "    -c      | -g | Genes | Region\n",
      "    ------- | -- | ----- | ------\n",
      "    -       | +  | given | auto: gene(s) + margin\n",
      "    chr     | -  | none  | whole chrom\n",
      "    chr     | +  | given | whole chrom\n",
      "    chr:s-e | -  | all   | given\n",
      "    chr:s-e | +  | given | given\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "101T\n",
      "Determine which datapoints to show based on the given options.\n",
      "\n",
      "Behaviors::\n",
      "\n",
      "    start/end   show_gene\n",
      "       +           +       given region + genes; err if any gene outside it\n",
      "       -           +       window +/- around genes\n",
      "       +           -       given region, highlighting any genes within it\n",
      "       -           -       whole chromosome, no genes\n",
      "\n",
      "If `show_range` is a chromosome name only, no start/end positions, then the\n",
      "whole chromosome will be shown.\n",
      "\n",
      "If region start/end coordinates are given and `show_gene` is '' or ',' (or\n",
      "all commas, etc.), then instead of highlighting all genes in the selection,\n",
      "no genes will be highlighted.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "102T\n",
      "Draw a scatter plot of probe values with optional segments overlaid.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "genes : list\n",
      "    Of tuples: (start, end, gene name)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "103T\n",
      "Configure axes for plotting a single chromosome's data.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "probes : CopyNumArray\n",
      "segments : CopyNumArray\n",
      "variants : VariantArray\n",
      "    All should already be subsetted to the region that will be plotted.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "104T\n",
      "Choose a display color based on a segment's CNA status.\n",
      "\n",
      "Uses the fields added by the 'call' command. If these aren't present, use\n",
      "`highlight_color` for everything.\n",
      "\n",
      "For sex chromosomes, some single-copy deletions or gains might not be\n",
      "highlighted, since sample sex isn't used to infer the neutral ploidies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "106T\n",
      "Modified copy of Bio.Graphics.BasicChromosome.Organism.draw.\n",
      "\n",
      "Instead of stacking chromosomes horizontally (along the x-axis), stack rows\n",
      "vertically, then proceed with the chromosomes within each row.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "org :\n",
      "    The chromosome diagram object being modified.\n",
      "title : str\n",
      "    The output title of the produced document.\n",
      "wrap : int\n",
      "    Maximum number of chromosomes per row; the remainder will be wrapped to\n",
      "    the next row(s).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "107T\n",
      "Monkeypatch to Bio.Graphics.BasicChromosome.Chromosome._draw_label.\n",
      "\n",
      "Draw a label for the chromosome. Mod: above the chromosome, not below.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "108T\n",
      "Do segmentation for CNVkit.\n",
      "\n",
      "Calculate copy number segmentation by HaarSeg\n",
      "(http://haarseg.r-forge.r-project.org/)\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "cnarr : CopyNumArray\n",
      "    Binned, normalized copy ratios.\n",
      "fdr_q : float\n",
      "    False discovery rate q-value.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "CopyNumArray\n",
      "    The CBS data table as a CNVkit object.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "109T\n",
      "Perform segmentation according to the HaarSeg algorithm.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "I : array\n",
      "    A 1D array of log-ratio values, sorted according to their genomic\n",
      "    location.\n",
      "W : array\n",
      "    Weight matrix, corresponding to quality of measurement, with values\n",
      "    :math:`1/(\\sigma^2)`. Must have the same size as I.\n",
      "rawI : array\n",
      "    The minimum between the raw test-sample and control-sample coverages\n",
      "    (before applying log ratio, but after any background reduction and/or\n",
      "    normalization). These raw red / green measurments are used to detect\n",
      "    low-value probes, which are more sensitive to noise.\n",
      "    Used for the non-stationary variance compensation.\n",
      "    Must have the same size as I.\n",
      "breaksFdrQ : float\n",
      "    The FDR q parameter. This value should lie between 0 and 0.5. The\n",
      "    smaller this value is, the less sensitive the segmentation result will\n",
      "    be.\n",
      "    For example, we will detect fewer segmentation breaks when using Q =\n",
      "    1e-4, compared to when using Q = 1e-3.\n",
      "    Common used values are 1e-2, 1e-3, 1e-4.\n",
      "haarStartLevel : int\n",
      "    The detail subband from which we start to detect peaks. The higher this\n",
      "    value is, the less sensitive we are to short segments. The default is\n",
      "    value is 1, corresponding to segments of 2 probes.\n",
      "haarEndLevel : int\n",
      "    The detail subband until which we use to detect peaks. The higher this\n",
      "    value is, the more sensitive we are to large trends in the data. This\n",
      "    value DOES NOT indicate the largest possible segment that can be\n",
      "    detected.  The default is value is 5, corresponding to step of 32 probes\n",
      "    in each direction.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "dict\n",
      "\n",
      "Source: haarSeg.R\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "110T\n",
      "Average the values of the probes within each segment.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "data : array\n",
      "    the probe array values\n",
      "peaks : array\n",
      "    Positions of copy number breakpoints in the original array\n",
      "\n",
      "Source: SegmentByPeaks.R\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "111T\n",
      "Convolve haar wavelet function with a signal, applying circular padding.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "signal : const array of floats\n",
      "weight : const array of floats (optional)\n",
      "stepHalfSize : int\n",
      "\n",
      "Returns\n",
      "-------\n",
      "array\n",
      "    Of floats, representing the convolved signal.\n",
      "\n",
      "Source: HaarSeg.c\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "112T\n",
      "Find local maxima on positive values, local minima on negative values.\n",
      "\n",
      "First and last index are never considered extramum.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "signal : const array of floats\n",
      "\n",
      "Returns\n",
      "-------\n",
      "peakLoc : array of ints\n",
      "    Locations of extrema in `signal`\n",
      "\n",
      "Source: HaarSeg.c\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "113T\n",
      "Unify several decomposition levels.\n",
      "\n",
      "Merge the two lists of breakpoints, but drop addonLevel values that are too\n",
      "close to baseLevel values.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "baseLevel : const array of ints\n",
      "addonLevel : const array of ints\n",
      "windowSize : int\n",
      "\n",
      "Returns\n",
      "-------\n",
      "joinedLevel : array of ints\n",
      "\n",
      "Source: HaarSeg.c\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "114T\n",
      "Convolve a pulse function with a signal, applying circular padding to the\n",
      "signal.\n",
      "\n",
      "Used for non-stationary variance compensation.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "signal: const array of floats\n",
      "pulseSize: int\n",
      "\n",
      "Returns\n",
      "-------\n",
      "array of floats\n",
      "\n",
      "Source: HaarSeg.c\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "115T\n",
      "Improve localization of breaks. Suboptimal, but linear-complexity.\n",
      "\n",
      "We try to move each break 1 sample left/right, choosing the offset which\n",
      "leads to minimum data error.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "signal: const array of floats\n",
      "peakLoc: const array of ints\n",
      "\n",
      "Source: HaarSeg.c\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "116T\n",
      "Segment bins by Hidden Markov Model.\n",
      "\n",
      "Use Viterbi method to infer copy number segments from sequential data.\n",
      "\n",
      "With b-allele frequencies ('baf' column in `cnarr`), jointly segment\n",
      "log-ratios and b-allele frequencies across a chromosome.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "cnarr : CopyNumArray\n",
      "    The bin-level data to segment.\n",
      "method : string\n",
      "    One of 'hmm' (3 states, flexible means), 'hmm-tumor' (5 states, flexible\n",
      "    means), 'hmm-germline' (3 states, fixed means).\n",
      "\n",
      "Results\n",
      "-------\n",
      "segarr : CopyNumArray\n",
      "    The segmented data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "117T\n",
      "Parameters\n",
      "----------\n",
      "cnarr : CopyNumArray\n",
      "    The normalized bin-level values to be segmented.\n",
      "method : string\n",
      "    One of 'hmm', 'hmm-tumor', 'hmm-germline'.\n",
      "processes : int\n",
      "    Number of parallel jobs to run.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "model :\n",
      "    A pomegranate HiddenMarkovModel trained on the given dataset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "118T\n",
      "Extract HMM fitting values from `cnarr`.\n",
      "\n",
      "For each chromosome arm, extract log2 ratios as a numpy array.\n",
      "\n",
      "Future: If VCF of variants is given, or 'baf' column has already been\n",
      "added to `cnarr` from the same, then the BAF values are a second row/column\n",
      "in each numpy array.\n",
      "\n",
      "Returns: List of numpy.ndarray, one per chromosome arm.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "119T\n",
      "Merge probe values from multiple samples into a 2D table (of sorts).\n",
      "\n",
      "Input:\n",
      "    dict of {sample ID: (probes, values)}\n",
      "Output:\n",
      "    list-of-tuples: (probe, log2 coverages...)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "120T\n",
      "Format as CDT.\n",
      "\n",
      "See:\n",
      "\n",
      "- http://jtreeview.sourceforge.net/docs/JTVUserManual/ch02s11.html\n",
      "- http://www.eisenlab.org/FuzzyK/cdt.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "121T\n",
      "Biodiscovery Nexus Copy Number \"basic\" format.\n",
      "\n",
      "Only represents one sample per file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "122T\n",
      "Biodiscovery Nexus Copy Number \"Custom-OGT\" format.\n",
      "\n",
      "To create the b-allele frequencies column, alterate allele frequencies from\n",
      "the VCF are aligned to the .cnr file bins.  Bins that contain no variants\n",
      "are left blank; if a bin contains multiple variants, then the frequencies\n",
      "are all \"mirrored\" to be above or below .5 (majority rules), then the median\n",
      "of those values is taken.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "123T\n",
      "SEG format for copy number segments.\n",
      "\n",
      "Segment breakpoints are not the same across samples, so samples are listed\n",
      "in serial with the sample ID as the left column.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "124T\n",
      "Convert a copy number array to a BED-like DataFrame.\n",
      "\n",
      "For each region in each sample (possibly filtered according to `show`),\n",
      "the columns are:\n",
      "\n",
      "    - reference sequence name\n",
      "    - start (0-indexed)\n",
      "    - end\n",
      "    - sample name or given label\n",
      "    - integer copy number\n",
      "\n",
      "By default (show=\"ploidy\"), skip regions where copy number is the default\n",
      "ploidy, i.e. equal to 2 or the value set by --ploidy.\n",
      "If show=\"variant\", skip regions where copy number is neutral, i.e. equal to\n",
      "the reference ploidy on autosomes, or half that on sex chromosomes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "125T\n",
      "Convert segments to Variant Call Format.\n",
      "\n",
      "For now, only 1 sample per VCF. (Overlapping CNVs seem tricky.)\n",
      "\n",
      "Spec: https://samtools.github.io/hts-specs/VCFv4.2.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "126T\n",
      "Assign ci_start and ci_end fields to segments.\n",
      "\n",
      "Values for each segment indicate the CI boundary points within that segment,\n",
      "i.e. the right CI boundary for the left-side breakpoint (segment start), and\n",
      "left CI boundary for the right-side breakpoint (segment end).\n",
      "\n",
      "This is a little unintuitive because the CI refers to the breakpoint, not\n",
      "the segment, but we're storing the info in an array of segments.\n",
      "\n",
      "Calculation: Just use the boundaries of the bins left- and right-adjacent to\n",
      "each segment breakpoint.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "127T\n",
      "Generate a GISTIC 2.0 \"markers\" file from a set of .cnr files.\n",
      "\n",
      "GISTIC documentation:\n",
      "\n",
      "ftp://ftp.broadinstitute.org/pub/GISTIC2.0/GISTICDocumentation_standalone.htm\n",
      "http://genepattern.broadinstitute.org/ftp/distribution/genepattern/modules_public_server_doc/GISTIC2.pdf\n",
      "http://gdac.broadinstitute.org/runs/analyses__2013_05_23/reports/cancer/KICH-TP/CopyNumber_Gistic2/nozzle.html\n",
      "\n",
      "The markers file identifies the marker names and positions of the markers in\n",
      "the original dataset (before segmentation). It is a three column,\n",
      "tab-delimited file with an optional header. The column headers are:\n",
      "\n",
      "(1) Marker Name\n",
      "(2) Chromosome\n",
      "(3) Marker Position (in bases)\n",
      "\n",
      "GISTIC also needs an accompanying SEG file generated from corresponding .cns\n",
      "files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "128T\n",
      "Convert tumor segments and normal .cnr or reference .cnn to THetA input.\n",
      "\n",
      "Follows the THetA segmentation import script but avoid repeating the\n",
      "pileups, since we already have the mean depth of coverage in each target\n",
      "bin.\n",
      "\n",
      "The options for average depth of coverage and read length do not matter\n",
      "crucially for proper operation of THetA; increased read counts per bin\n",
      "simply increase the confidence of THetA's results.\n",
      "\n",
      "THetA2 input format is tabular, with columns:\n",
      "    ID, chrm, start, end, tumorCount, normalCount\n",
      "\n",
      "where chromosome IDs (\"chrm\") are integers 1 through 24.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "129T\n",
      "Extract segments' reference mean log2 values and probe counts.\n",
      "\n",
      "Code paths::\n",
      "\n",
      "    wt_mdn  wt_old  probes  norm -> norm, nbins\n",
      "    +       *       *       -       0,  wt_mdn\n",
      "    -       +       +       -       0,  wt_old * probes\n",
      "    -       +       -       -       0,  wt_old * size?\n",
      "    -       -       +       -       0,  probes\n",
      "    -       -       -       -       0,  size?\n",
      "\n",
      "    +       -       +       +       norm, probes\n",
      "    +       -       -       +       norm, bin counts\n",
      "    -       +       +       +       norm, probes\n",
      "    -       +       -       +       norm, bin counts\n",
      "    -       -       +       +       norm, probes\n",
      "    -       -       -       +       norm, bin counts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "130T\n",
      "Calculate segments' read counts from log2-ratios.\n",
      "\n",
      "Math:\n",
      "    nbases = read_length * read_count\n",
      "and\n",
      "    nbases = bin_width * read_depth\n",
      "where\n",
      "    read_depth = read_depth_ratio * avg_depth\n",
      "\n",
      "So:\n",
      "    read_length * read_count = bin_width * read_depth\n",
      "    read_count = bin_width * read_depth / read_length\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "131T\n",
      "Convert a cohort of per-gene read counts to CNVkit .cnr format.\n",
      "\n",
      "The expected data source is TCGA gene-level expression counts for individual\n",
      "samples, but other sources should be fine, too.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "132T\n",
      "Pull out the expected read counts from each RSEM file.\n",
      "\n",
      "The format of RSEM's ``*_rsem.genes.results`` output files is tab-delimited\n",
      "with a header row. We extract the Ensembl gene ID, expected read counts, and\n",
      "transcript lengths from each file.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "sample_counts : DataFrame\n",
      "    Row index is Ensembl gene ID, column index is filename.\n",
      "tx_lengths : Series\n",
      "    Gene lengths.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "133T\n",
      "Add Gaussian noise to each bootstrap replicate.\n",
      "\n",
      "The result is used to compute a \"smoothed bootstrap,\" where the added noise\n",
      "ensures that for small samples (e.g. number of bins in the segment) the\n",
      "bootstrapped CI is close to the standard error of the mean, as it should be.\n",
      "Conceptually, sample from a KDE instead of the values themselves.\n",
      "\n",
      "This addresses the issue that small segments (#bins < #replicates) don't\n",
      "fully represent the underlying distribution, in particular the extreme\n",
      "values, so the CI is too narrow. For single-bin segments in particular,\n",
      "the confidence interval will always have zero width unless the samples are\n",
      "smoothed.\n",
      "\n",
      "Standard deviation of the noise added to each bin comes from each bin's\n",
      "weight, which is an estimate of (1-variance).\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "values : np.ndarray\n",
      "    Original log2 values within the segment.\n",
      "samples : list of np.ndarray\n",
      "    Bootstrap replicates as (value_sample, weight_sample).\n",
      "\n",
      "Returns\n",
      "-------\n",
      "`samples` with random N(0, pop_sd) added to each value, and\n",
      "weights unchanged.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "134T\n",
      "Bias Corrected & Accellerated (BCa) bootstrap adjustment.\n",
      "\n",
      "See: Efron 1987, \"Better Bootstrap Confidence Intervals\"\n",
      "http://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478410\n",
      "\n",
      "Ported from R package \"bootstrap\" function \"bcanon\".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "135T\n",
      "Drop contigs with noncanonical names.\n",
      "\n",
      "`region_tups` is an iterable of (chrom, start, end) tuples.\n",
      "\n",
      "Yield the same, but dropping noncanonical chrom.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "136T\n",
      "Compile a neutral-coverage reference from the given intervals.\n",
      "\n",
      "Combines the intervals, shifts chrX values if requested, and calculates GC\n",
      "and RepeatMasker content from the genome FASTA sequence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "137T\n",
      "Map sample IDs to inferred chromosomal sex, where possible.\n",
      "\n",
      "For samples where the source file is empty or does not include either sex\n",
      "chromosome, that sample ID will not be in the returned dictionary.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "138T\n",
      "Calculate the median coverage of each bin across multiple samples.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "filenames : list\n",
      "    List of string filenames, corresponding to targetcoverage.cnn and\n",
      "    antitargetcoverage.cnn files, as generated by 'coverage' or\n",
      "    'import-picard'.\n",
      "fa_fname : str\n",
      "    Reference genome sequence in FASTA format, used to extract GC and\n",
      "    RepeatMasker content of each genomic bin.\n",
      "is_haploid_x : bool\n",
      "do_cluster : bool\n",
      "fix_gc : bool\n",
      "fix_edge : bool\n",
      "fix_rmask : bool\n",
      "\n",
      "Returns\n",
      "-------\n",
      "CopyNumArray\n",
      "    One object summarizing the coverages of the input samples, including\n",
      "    each bin's \"average\" coverage, \"spread\" of coverages, and GC content.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "139T\n",
      "Load and summarize a pool of \\*coverage.cnn files.\n",
      "\n",
      "Run separately for the on-target and (optional) antitarget bins.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "ref_df : pandas.DataFrame\n",
      "    All columns needed for the reference CNA object, including\n",
      "    aggregate log2 and spread.\n",
      "all_logr : numpy.ndarray\n",
      "    All sample log2 ratios, as a 2D matrix (rows=bins, columns=samples),\n",
      "    to be used with do_cluster.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "140T\n",
      "Shift sample X and Y chromosomes to match the reference sex.\n",
      "\n",
      "Reference values::\n",
      "\n",
      "    XY: chrX -1, chrY -1\n",
      "    XX: chrX 0, chrY -1\n",
      "\n",
      "Plan::\n",
      "\n",
      "    chrX:\n",
      "    xx sample, xx ref: 0    (from 0)\n",
      "    xx sample, xy ref: -= 1 (from -1)\n",
      "    xy sample, xx ref: += 1 (from 0)    +1\n",
      "    xy sample, xy ref: 0    (from -1)   +1\n",
      "    chrY:\n",
      "    xx sample, xx ref: = -1 (from -1)\n",
      "    xx sample, xy ref: = -1 (from -1)\n",
      "    xy sample, xx ref: 0    (from -1)   +1\n",
      "    xy sample, xy ref: 0    (from -1)   +1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "141T\n",
      "Average & spread of log2ratios and depths for a group of samples.\n",
      "\n",
      "Can apply to all samples, or a given cluster of samples.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "142T\n",
      "Extract and summarize clusters of samples in logr_matrix.\n",
      "\n",
      "1. Calculate correlation coefficients between all samples (columns).\n",
      "2. Cluster the correlation matrix.\n",
      "3. For each resulting sample cluster (down to a minimum size threshold),\n",
      "   calculate the central log2 value for each bin, similar to the full pool.\n",
      "   Also print the sample IDs in each cluster, if feasible.\n",
      "\n",
      "Also recalculate and store the 'spread' of each cluster, though this might\n",
      "not be necessary/good.\n",
      "\n",
      "Return a DataFrame of just the log2 values. Column names are ``log2_i``\n",
      "where i=1,2,... .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "143T\n",
      "Warn about target bins where coverage is poor.\n",
      "\n",
      "Prints a formatted table to stderr.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "144T\n",
      "Extract an iterable of regions from an indexed FASTA file.\n",
      "\n",
      "Input: FASTA file name; iterable of (seq_id, start, end) (1-based)\n",
      "Output: iterable of string sequences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "145T\n",
      "Generate antitarget intervals between/around target intervals.\n",
      "\n",
      "Procedure:\n",
      "\n",
      "- Invert target intervals\n",
      "- Subtract the inverted targets from accessible regions\n",
      "- For each of the resulting regions:\n",
      "\n",
      "    - Shrink by a fixed margin on each end\n",
      "    - If it's smaller than min_bin_size, skip\n",
      "    - Divide into equal-size (region_size/avg_bin_size) portions\n",
      "    - Emit the (chrom, start, end) coords of each portion\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "146T\n",
      "Drop contigs that are not targeted or canonical chromosomes.\n",
      "\n",
      "Antitargets will be binned over chromosomes that:\n",
      "\n",
      "- Appear in the sequencing-accessible regions of the reference genome\n",
      "  sequence, and\n",
      "- Contain at least one targeted region, or\n",
      "- Are named like a canonical chromosome (1-22,X,Y for human)\n",
      "\n",
      "This allows antitarget binning to pick up canonical chromosomes that do not\n",
      "contain any targets, as well as non-canonical or oddly named chromosomes\n",
      "that were targeted.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "147T\n",
      "Check if chromosome names overlap, and preview each if not.\n",
      "\n",
      "This summary message will help triage cases of e.g. \"chr1\" vs. \"1\" in the\n",
      "two genomic datasets being compared.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "148T\n",
      "Drop contigs that are much shorter than the others.\n",
      "\n",
      "Cutoff is where a contig is less than half the size of the next-shortest\n",
      "contig.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "149T\n",
      "Executes commands.\n",
      "\n",
      "Args:\n",
      "    to_process (list): a list of tasks to process\n",
      "    nb_threads (int): the number of processes that is required\n",
      "    check_rc (bool): whether or not to check the return code of the task\n",
      "    hpc (bool): whether or not to execute the tasks on a cluster (DRMAA)\n",
      "    hpc_options (dict): the DRMAA options\n",
      "    out_dir (str): the output directory\n",
      "    preamble (str): the script preamble (for DRMAA)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "150T\n",
      "Check that the files exist.\n",
      "\n",
      "Args:\n",
      "    o_files (list): the list of files got check\n",
      "    task (str): the name of the task\n",
      "\n",
      "Returns:\n",
      "    bool: ``True`` if all files exist, ``False`` otherwise\n",
      "\n",
      "If the file to check is an impute2 file, and that this file is missing, we\n",
      "check for further statistics using the :py:func:`_check_impute2_file`.\n",
      "\n",
      "Note\n",
      "----\n",
      "    If the file name ends with ``.impute2`` and the file doesn't exist, we\n",
      "    look for the compressed file (``.impute2.gz``) instead.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "151T\n",
      "Checks the log to explain a failure return code.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file to check\n",
      "    task (str): the name of the task\n",
      "\n",
      "Returns:\n",
      "    bool: ``True`` if everything is norma, ``False`` otherwise\n",
      "\n",
      "This function looks for a known error message in the log file. If the\n",
      "message ``ERROR: Reference and Main panels are not well aligned:`` appears\n",
      "in the log file, then it's normal that the job failed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "152T\n",
      "Checks the log to explain the absence of an .snp.strand file.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file to check\n",
      "    task (str): the name of the task\n",
      "\n",
      "Returns:\n",
      "    bool: ``True`` if everything is normal, ``False`` otherwise.\n",
      "\n",
      "This function looks for known message in the log file. If the SNPs were\n",
      "read from the legend file and the haplotypes were read from the hap file,\n",
      "then there were no SNPs flip issue.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "153T\n",
      "Checks the summary to explain the absence of an .impute2 file.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file to check\n",
      "    task (str): the name of the task\n",
      "\n",
      "Returns:\n",
      "    bool: ``True`` if everything is normal, ``False`` otherwise.\n",
      "\n",
      "This function looks for known message in the summary file. Three possible\n",
      "ways that an impute2 file is missing:\n",
      "\n",
      "1. there are no SNPs in the imputation interval;\n",
      "2. there are no type 2 SNPs after applying the settings;\n",
      "3. there are no SNPs for output.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "154T\n",
      "Executes a single command.\n",
      "\n",
      "Args:\n",
      "    command_info (dict): information about the command\n",
      "\n",
      "Returns:\n",
      "    tuple: a tuple containing 4 entries: whether the task completed (bool),\n",
      "           the name of the task (str), the status of the run (str) and the\n",
      "           execution time in seconds (int)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "155T\n",
      "Executes a command using DRMAA (usually on a HPC).\n",
      "\n",
      "Args:\n",
      "    command_info (dict): information about the command\n",
      "\n",
      "Returns:\n",
      "    tuple: a tuple containing 4 entries: whether the task completed (bool),\n",
      "           the name of the task (str), the status of the run (str) and the\n",
      "           execution time in seconds (int)\n",
      "\n",
      "Note\n",
      "----\n",
      "    The preamble (if required) is inserted between the shebang line and the\n",
      "    actual command.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "156T\n",
      "Generates the probability matrix from an IMPUTE2 line.\n",
      "\n",
      "Args:\n",
      "    impute2_line (list): a single line from IMPUTE2's result (split by\n",
      "                         space)\n",
      "\n",
      "Returns:\n",
      "    tuple: a tuple containing the marker's information (first five values\n",
      "           of the line) and the matrix probability (numpy array, float)\n",
      "\n",
      "The shape of the matrix is n x 3 where n is the number of samples.\n",
      "The columns represent the probability for AA, AB and BB.\n",
      "\n",
      "Note\n",
      "----\n",
      "    The ``impute2_line`` variable is a list of str, corresponding to a line\n",
      "    from the IMPUTE2's result, split by space.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "157T\n",
      "Gathers good imputed genotypes (>= probability threshold).\n",
      "\n",
      "Args:\n",
      "    prob_matrix (numpy.array): the probability matrix\n",
      "    min_prob (float): the probability threshold\n",
      "\n",
      "Returns:\n",
      "    numpy.array: a mask array containing the positions where the\n",
      "                 probabilities are equal or higher to the threshold\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "158T\n",
      "Computes MAF from a probability matrix (and gender if chromosome X).\n",
      "\n",
      "Args:\n",
      "    prob_matrix (numpy.array): the probability matrix\n",
      "    a1 (str): the first allele\n",
      "    a2 (str): the second allele\n",
      "    gender (numpy.array): the gender of the samples\n",
      "    site_name (str): the name for this site\n",
      "\n",
      "Returns:\n",
      "    tuple: a tuple containing three values: the minor allele frequency, the\n",
      "           minor and the major allele.\n",
      "\n",
      "When 'gender' is not None, we assume that the MAF on chromosome X is\n",
      "required (hence, males count as 1, and females as 2 alleles). There is also\n",
      "an Exception raised if there are any heterozygous males.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "159T\n",
      "Computes MAF and dosage vector from probs matrix.\n",
      "\n",
      "Args:\n",
      "    prob_matrix (numpy.array): the probability matrix\n",
      "    a1 (str): the first allele\n",
      "    a2 (str): the second allele\n",
      "    scale (int): the scale value\n",
      "    gender (numpy.array): the gender of the samples\n",
      "    site_name (str): the name for this site\n",
      "\n",
      "Returns:\n",
      "    tuple: a tuple containing four values: the dosage vector, the minor\n",
      "           allele frequency, the minor and the major allele.\n",
      "\n",
      "When 'gender' is not None, we assume that the MAF on chromosome X is\n",
      "required (hence, males count as 1, and females as 2 alleles). There is also\n",
      "an Exception raised if there are any heterozygous males.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "160T\n",
      "Computes dosage from probability matrix (for the minor allele).\n",
      "\n",
      "Args:\n",
      "    homo_probs (numpy.array): the probabilities for the homozygous genotype\n",
      "    hetero_probs (numpy.array): the probabilities for the heterozygous\n",
      "                                genotype\n",
      "    scale (int): the scale value\n",
      "\n",
      "Returns:\n",
      "    numpy.array: the dosage computed from the probabilities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "161T\n",
      "Computes hard calls from probability matrix.\n",
      "\n",
      "Args:\n",
      "    a1 (str): the first allele\n",
      "    a2 (str): the second allele\n",
      "    probs (numpy.array): the probability matrix\n",
      "\n",
      "Returns:\n",
      "    numpy.array: the hard calls computed from the probabilities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "162T\n",
      "Compute additive format from probability matrix.\n",
      "\n",
      "Args:\n",
      "    a1 (str): the a1 allele\n",
      "    a2 (str): the a2 allele\n",
      "    probs (numpy.array): the probability matrix\n",
      "\n",
      "Returns:\n",
      "    tuple: the additive format computed from the probabilities, the minor\n",
      "           and major allele.\n",
      "\n",
      "The encoding is as follow: 0 when homozygous major allele, 1 when\n",
      "heterozygous and 2 when homozygous minor allele.\n",
      "\n",
      "The minor and major alleles are inferred by looking at the MAF. By default,\n",
      "we think a2 is the minor allele, but flip if required.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "163T\n",
      "Yields seek position for each line.\n",
      "\n",
      "Args:\n",
      "    f (file): the file object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "164T\n",
      "Build a index for the given file.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file\n",
      "    cols (list): a list containing column to keep (as int)\n",
      "    names (list): the name corresponding to the column to keep (as str)\n",
      "    sep (str): the field separator\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the index\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "165T\n",
      "Get the opening function.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file\n",
      "    return_fmt (bool): if the file format needs to be returned\n",
      "\n",
      "Returns:\n",
      "    tuple: either a tuple containing two elements: a boolean telling if the\n",
      "           format is bgzip, and the opening function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "166T\n",
      "Restores the index for a given file.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file\n",
      "    cols (list): a list containing column to keep (as int)\n",
      "    names (list): the name corresponding to the column to keep (as str)\n",
      "    sep (str): the field separator\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the index\n",
      "\n",
      "If the index doesn't exist for the file, it is first created.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "168T\n",
      "Reads index from file.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file containing the index\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the index of the file\n",
      "\n",
      "Before reading the index, we check the first couple of bytes to see if it\n",
      "is a valid index file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "169T\n",
      "Generates the index filename from the path to the indexed file.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file for which we want an index\n",
      "\n",
      "Returns:\n",
      "    str: the name of the file containing the index\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "170T\n",
      "Checks if the index exists, if not, create it.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file for which we want the index\n",
      "\n",
      "Returns:\n",
      "    bool: ``True`` if the file contains an index, ``False`` otherwise\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "171T\n",
      "Creates a task DB.\n",
      "\n",
      "Args:\n",
      "    out_dir (str): the directory where the DB will be saved\n",
      "\n",
      "Returns:\n",
      "    str: the name of the file containing the DB\n",
      "\n",
      "A SQLITE database will be created in the ``out_dir`` directory (with the\n",
      "name ``tasks.db``. The ``genipe_task`` table is automatically created.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "172T\n",
      "Creates a DB connection.\n",
      "\n",
      "Args:\n",
      "    db_name (str): the name of the database (usually a file)\n",
      "\n",
      "Returns:\n",
      "    tuple: a tuple containing the connection object and a cursor to that\n",
      "           object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "173T\n",
      "Checks if the task exists and if it's completed.\n",
      "\n",
      "Args:\n",
      "    task_id (str): the ID of the task\n",
      "    db_name (str): the name of the database (usually a file)\n",
      "\n",
      "Returns:\n",
      "    bool: ``True`` if the task exists **and** is completed, ``False``\n",
      "          otherwise\n",
      "\n",
      "Note\n",
      "----\n",
      "    A task is completed if the column ``completed`` equals 1. It is not\n",
      "    completed otherwise.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "174T\n",
      "Creates (or updates) a task.\n",
      "\n",
      "Args:\n",
      "    task_id (str): the ID of the task\n",
      "    db_name (str): the name of the database (usually a file)\n",
      "\n",
      "If the task ID doesn't exist in the DB, a new one will be created with the\n",
      "current time as launch and start time.\n",
      "\n",
      "If the task ID already exist, it is presumed that the task will be\n",
      "relaunched, hence the database entry is updated to the current time (for\n",
      "launch and start time) and ``completed`` is set to ``0``.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "175T\n",
      "Marks the task as completed.\n",
      "\n",
      "Args:\n",
      "    task_id (str): the ID of the task\n",
      "    db_name (str): the name of the DB (usually a file)\n",
      "\n",
      "The task entry is modified so that ``completed=1`` and the end time is\n",
      "updated to the current time.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "176T\n",
      "Marks a task as incomplete.\n",
      "\n",
      "Args:\n",
      "    task_id (str): the ID of the task\n",
      "    db_name (str): the name of the DB (usually a file)\n",
      "\n",
      "The task entry is set as incomplete by updating the ``completed`` value to\n",
      "``0``.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "177T\n",
      "Marks a task run by DRMAA as completed (while updating times).\n",
      "\n",
      "Args:\n",
      "    task_id (str): the ID of the task\n",
      "    launch_time (float): the launch time (according to DRMAA)\n",
      "    start_time (float): the start time (according to DRMAA)\n",
      "    end_time (float): the end time (according to DRMAA)\n",
      "    db_name (str): the name of the DB (usually a file)\n",
      "\n",
      "The task entry is updated with the launch, start and end time. Those times\n",
      "come from the DRMAA library. The launch time is the time at which the task\n",
      "was launched to the scheduler. The start time correspond to the time that\n",
      "the scheduler started the job on the cluster. Finally, the end time is the\n",
      "time that the job was completed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "178T\n",
      "Gets the task run time.\n",
      "\n",
      "Args:\n",
      "    task_id (str): the ID of the task\n",
      "    db_name (str): the name of the DB (usually a file)\n",
      "\n",
      "Returns:\n",
      "    int: the execution time of the task (in seconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "179T\n",
      "Gets all tasks execution time.\n",
      "\n",
      "Args:\n",
      "    db_name (str): the name of the DB (usually a file)\n",
      "\n",
      "Returns:\n",
      "    dict: the execution time (seconds) of all the tasks in the database\n",
      "\n",
      "This function returns a dictionary of task ID (keys) pointing to execution\n",
      "time (in second) (int).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "180T\n",
      "Phase markers using shapeit.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosome to phase\n",
      "    prefix (str): the prefix template of the input files\n",
      "    o_prefix (str): the prefix template of the output files\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "Returns:\n",
      "    list: the list of all samples that were phased\n",
      "\n",
      "A template contains the string ``{chrom}``, which will be replaced by the\n",
      "chromosome number (e.g. ``genipe/chr{chrom}/chr{chrom}.final`` will be\n",
      "replaced by ``genipe/chr1/chr1.final``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "181T\n",
      "Imputes the markers using IMPUTE2.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosome to phase\n",
      "    phased_haplotypes (str): the template for the haplotype files\n",
      "    out_prefix (str): the prefix template of the output files\n",
      "    chrom_length (dict): the length of each chromosome\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "A template contains the string ``{chrom}``, which will be replaced by the\n",
      "chromosome number (e.g. ``genipe/chr{chrom}/chr{chrom}.final`` will be\n",
      "replaced by ``genipe/chr1/chr1.final``).\n",
      "\n",
      "Note\n",
      "----\n",
      "    When imputing the pseudo-autosomal regions of chromosome 23, the\n",
      "    '-chrX' and '-Xpar' options are used. This combination of options\n",
      "    reduces the ``-Ne`` value by 25%.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "182T\n",
      "Merges impute2 files.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of required chromosomes to merge\n",
      "    in_glob (str): the template that will be used to find files with the\n",
      "                   :py:mod:`glob` module\n",
      "    o_prefix (str): the prefix template of the output files\n",
      "    probability_t (float): the probability threshold to use\n",
      "    completion_t (float): the completion threshold to use\n",
      "    info_t (float): the info threshold to use\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "Returns:\n",
      "    set: a set containing the chromosome to skip.\n",
      "\n",
      "A template contains the string ``{chrom}``, which will be replaced by the\n",
      "chromosome number (e.g. ``genipe/chr{chrom}/chr{chrom}.final`` will be\n",
      "replaced by ``genipe/chr1/chr1.final``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "183T\n",
      "Merges impute2 files.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosome to compress\n",
      "    filename_template (str): the template for the final IMPUTE2 file\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "A template contains the string ``{chrom}``, which will be replaced by the\n",
      "chromosome number (e.g. ``genipe/chr{chrom}/chr{chrom}.final`` will be\n",
      "replaced by ``genipe/chr1/chr1.final``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "184T\n",
      "Helps in filename sorting.\n",
      "\n",
      "Args:\n",
      "    filename (str): the name of the file to compare while sorting\n",
      "\n",
      "Returns:\n",
      "    tuple: a tuple containing three elements: chromosome (int), start (int)\n",
      "           and end (int)\n",
      "\n",
      "Using a regular expression, finds the chromosome along with the starting\n",
      "and ending position of an imputed segment. The file\n",
      "``chr22.1_50000.impute2`` should return ``(22, 1, 50000)``.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "185T\n",
      "Gets the chromosome length from Ensembl REST API.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of required chromosomes\n",
      "    legend (str): the legend file template for the autosomal chromosomes\n",
      "    legend_chr23 (str): the legend file for the non pseudo-autosomal region\n",
      "                        of chromosome 23\n",
      "    legend_par1 (stR): the legend file for the first pseudo-autosomal\n",
      "                       region of chromosome 23\n",
      "    legend_par2 (stR): the legend file for the second pseudo-autosomal\n",
      "                       region of chromosome 23\n",
      "    out_dir (str): the output directory\n",
      "\n",
      "Returns:\n",
      "    dict: the chromosome length (chr -> length)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "186T\n",
      "Checks the strand using SHAPEIT2.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosome to check\n",
      "    prefix (str): the prefix template of the input files\n",
      "    id_suffix (str): the suffix of the task\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "    exclude (bool): should markers be excluded or flipped (default is\n",
      "                    flipped)\n",
      "\n",
      "Returns:\n",
      "    dict: statistics about the task (number of markers that will be flipped\n",
      "          or excluded)\n",
      "\n",
      "A template contains the string ``{chrom}``, which will be replaced by the\n",
      "chromosome number (e.g. ``genipe/chr{chrom}/chr{chrom}.final`` will be\n",
      "replaced by ``genipe/chr1/chr1.final``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "187T\n",
      "Flip markers.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosomes to flip\n",
      "    prefix (str): the prefix template of the input files\n",
      "    to_flip (str): the name of the file containing markers to flip\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "A template contains the string ``{chrom}``, which will be replaced by the\n",
      "chromosome number (e.g. ``genipe/chr{chrom}/chr{chrom}.final`` will be\n",
      "replaced by ``genipe/chr1/chr1.final``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "188T\n",
      "Flip markers.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosome to extract\n",
      "    prefix (str): the prefix template of the input files\n",
      "    to_exclude (str): the name of the file containing the markers to\n",
      "                      exclude\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "Returns:\n",
      "    dict: the number of remaining markers for phasing\n",
      "\n",
      "A template contains the string ``{chrom}``, which will be replaced by the\n",
      "chromosome number (e.g. ``genipe/chr{chrom}/chr{chrom}.final`` will be\n",
      "replaced by ``genipe/chr1/chr1.final``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "189T\n",
      "Compute (using Plink) marker missing rate.\n",
      "\n",
      "Args:\n",
      "    prefix (str): the prefix of the input file\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the missing rate for each site (results from Plink)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "190T\n",
      "Finds ambiguous markers (A/T and G/C) or duplicated.\n",
      "\n",
      "Args:\n",
      "    prefix (str): the prefix of the input files\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "Returns:\n",
      "    dict: information about the data set\n",
      "\n",
      "If required, the function uses :py:func:`is_reversed` to check if a marker\n",
      "is on the reverse strand and needs flipping.\n",
      "\n",
      "The information returned includes the initial number of markers and\n",
      "samples, the number of ambiguous, duplicated and non-autosomal markers,\n",
      "along with the number of markers to flip if the reference was checked.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "191T\n",
      "Finds and excludes ambiguous markers (A/T and G/C) or duplicated.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of required chromosomes\n",
      "    prefix (str): the prefix of the input files\n",
      "    db_name (str): the name of the DB saving tasks' information\n",
      "    chrom_length (dict): the length of each chromosomes\n",
      "    options (argparse.Namespace): the pipeline options\n",
      "\n",
      "Returns:\n",
      "    set: a set of chromosome 23 regions to skip (because there were no\n",
      "         markers left).\n",
      "\n",
      "If required, the function uses :py:func:`is_reversed` to check if a marker\n",
      "is on the reverse strand and needs flipping.\n",
      "\n",
      "The information returned includes the initial number of markers and\n",
      "samples, the number of ambiguous, duplicated and non-autosomal markers,\n",
      "along with the number of markers to flip if the reference was checked.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "192T\n",
      "Reorders chromosome 23 markers.\n",
      "\n",
      "Args:\n",
      "    chrom (int): the chromosome to reorder\n",
      "    to_skip (set): the set of regions to skip (if necessary)\n",
      "    prefix (str): the prefix of the output files\n",
      "    base_command (list): the base command\n",
      "\n",
      "Returns:\n",
      "    list: a list of command information to run for chromosome 23\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "193T\n",
      "Creates the command to extract the chromosome 23 regions.\n",
      "\n",
      "Args:\n",
      "    chrom (int): the chromosome to extract\n",
      "    prefix (str): the prefix of the output files\n",
      "    bim (pandas.DataFrame): the BIM file\n",
      "    chrom_length (dict): the length of each of the chromosomes\n",
      "    base_command (list): the base command\n",
      "\n",
      "Returns:\n",
      "    list: a list of command information to run for chromosome 23\n",
      "\n",
      "Note\n",
      "----\n",
      "    Chromosome 23 represents the non pseudo-autosomal region. Chromosome 25\n",
      "    represents the two pseudo-autosomal region of chromosome 23.\n",
      "\n",
      "Warning\n",
      "-------\n",
      "    It is assume that chromosome 25 positions (pseudo-autosomal regions)\n",
      "    are relative to chromosome 23 positions.\n",
      "\n",
      "Note\n",
      "----\n",
      "    Markers are dispatched to correct chromosome (23 or 25, for non- and\n",
      "    pseudo-autosomal region) according to their genomic location. If a\n",
      "    marker should be located on chromosome 23, but is located on chromosome\n",
      "    25, its mapping information is changed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "194T\n",
      "Reads a BIM file and extracts chromosomes.\n",
      "\n",
      "Args:\n",
      "    bim_fn (str): the name of the BIM file\n",
      "    chromosomes (list): the list of chromosome to extract\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the BIM file content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "195T\n",
      "Gets the chromosome's encoding (e.g. 1 vs chr1, X vs 23, etc).\n",
      "\n",
      "Args:\n",
      "    reference (pyfaidx.Fasta): the reference\n",
      "\n",
      "Returns:\n",
      "    dict: the chromosome encoding\n",
      "\n",
      "The encoding is a dictionary where numerical autosomes from 1 to 22 are\n",
      "the keys, and the encoded autosomes (the one present in the reference)\n",
      "are the values. An example would be ``1 -> chr1``.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "196T\n",
      "Checks the strand using a reference, returns False if problem.\n",
      "\n",
      "Args:\n",
      "    chrom (str): the chromosome\n",
      "    pos (int): the position\n",
      "    a1 (str): the first allele\n",
      "    a2 (str): the second allele\n",
      "    reference (pyfaidx.Fasta): the reference\n",
      "    encoding (dict): the chromosome encoding in the reference\n",
      "\n",
      "Returns:\n",
      "    bool: ``True`` if it's the complement, ``False`` otherwise (also\n",
      "          returns ``False`` if there was a problem)\n",
      "\n",
      "The encoding (used to encode chromosome to search in the reference) is the\n",
      "dictionary returned by the :py:func:`get_chrom_encoding` function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "197T\n",
      "Creates a weighted mean for each chromosome for cross-validation.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosome to gather cross\n",
      "                            validation statistics\n",
      "    glob_pattern (str): the pattern used to find files using :py:mod:`glob`\n",
      "\n",
      "Returns:\n",
      "    dict: weighted mean by chromosome (cross-validation)\n",
      "\n",
      "The returned information includes the total number of genotyped tested, the\n",
      "total number of genotyped by chromosome, the two summary tables produced by\n",
      "IMPUTE2 (but using weighted means) for all autosomes, and the two summary\n",
      "tables for each chromosome.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "198T\n",
      "Gathers imputation statistics from the merged dataset.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the chromosome to gather statistics from\n",
      "    prob_t (float): the probability threshold (>= t)\n",
      "    completion_t (float): the completion threshold (>= t)\n",
      "    info_t (float): the information threshold (>= t)\n",
      "    nb_samples (int): the number of samples\n",
      "    missing (pandas.DataFrame): the missing rate\n",
      "    o_dir (str): the output directory\n",
      "\n",
      "Returns:\n",
      "    dict: imputation statistics\n",
      "\n",
      "The information returned includes the following (all values are formated in\n",
      "strings):\n",
      "\n",
      "+--------------------------------+----------------------------------------+\n",
      "| Information                    | Description                            |\n",
      "+================================+========================================+\n",
      "| ``prob_threshold``             | the probability threshold used (>= t)  |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``nb_imputed``                 | the number of imputed sites            |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``average_comp_rate``          | the average completion rate            |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``rate_threshold``             | the completion rate threshold (>= t)   |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``info_threshold``             | the information threshold (>= t)       |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``nb_good_sites``              | the number of \"good\" sites (that pass  |\n",
      "|                                | the different probability and          |\n",
      "|                                | completion thresholds)                 |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``pct_good_sites``             | the percentage of \"good\" sites (that   |\n",
      "|                                | pass the different probability and     |\n",
      "|                                | completion thresholds)                 |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``average_comp_rate_cleaned``  | the average completion rate of \"good\"  |\n",
      "|                                | sites                                  |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``mean_missing``               | the mean number of missing genotypes   |\n",
      "|                                | per sample (according to the           |\n",
      "|                                | probability and completion thresholds) |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``nb_samples``                 | the total number of samples            |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``nb_genotyped``               | the number of genotyped sites in the   |\n",
      "|                                | original dataset that are included in  |\n",
      "|                                | the imputed dataset                    |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``nb_genotyped_not_complete``  | The number of original markers with a  |\n",
      "|                                | completion rate lower than 100%        |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``pct_genotyped_not_complete`` | The percentage of original markers     |\n",
      "|                                | with a completion rate lower than 100% |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``nb_geno_now_complete``       | The number of original (genotyped)     |\n",
      "|                                | markers which are now 100% complete    |\n",
      "|                                | after imputation                       |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``nb_missing_geno``            | The number of missing genotypes        |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``pct_geno_now_complete``      | The percentage of now complete         |\n",
      "|                                | genotypes (according to the number of  |\n",
      "|                                | previously missing ones)               |\n",
      "+--------------------------------+----------------------------------------+\n",
      "| ``nb_site_now_complete``       | The number of previously genotyped     |\n",
      "|                                | markers that are now 100% complete     |\n",
      "|                                | after imputation                       |\n",
      "+--------------------------------+----------------------------------------+\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "199T\n",
      "Gather minor allele frequencies from imputation.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosome to gather statistics\n",
      "    o_dir (str): the output directory\n",
      "\n",
      "Returns:\n",
      "    dict: frequency information\n",
      "\n",
      "The following information about minor allele frequencies (MAF) are\n",
      "returned (all values are formatted in strings):\n",
      "\n",
      "+--------------------------+----------------------------------------------+\n",
      "| Information              | Description                                  |\n",
      "+==========================+==============================================+\n",
      "| ``nb_maf_nan``           | the number of markers with invalid MAF       |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``nb_marker_with_maf``   | the number of markers with valid MAF         |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``nb_maf_geq_01``        | the number of markers with MAF >= 1%         |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``nb_maf_geq_05``        | the number of markers with MAF >= 5%         |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``nb_maf_lt_05``         | the number of markers with MAF < 5%          |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``nb_maf_lt_01``         | the number of markers with MAF < 1%          |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``nb_maf_geq_01_lt_05``  | the number of markers with 1% >= MAF < 5%    |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``pct_maf_geq_01``       | the percentage of markers with MAF >= 1%     |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``pct_maf_geq_05``       | the percentage of markers with MAF >= 5%     |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``pct_maf_lt_05``        | the percentage of markers with MAF < 5%      |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``pct_maf_lt_01``        | the percentage of markers with MAF < 1%      |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``pct_maf_geq_01_lt_05`` | the percentage of markers with               |\n",
      "|                          | 1% >= MAF < 5%                               |\n",
      "+--------------------------+----------------------------------------------+\n",
      "| ``frequency_barh``       | the name of the file containing the bar plot |\n",
      "|                          | (if it was created)                          |\n",
      "+--------------------------+----------------------------------------------+\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "200T\n",
      "Gather all the execution times.\n",
      "\n",
      "Args:\n",
      "    required_chrom (tuple): the list of chromosomes to gather statistics\n",
      "    db_name (str): the name of the DB\n",
      "\n",
      "Returns:\n",
      "    dict: the execution time for all tasks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "201T\n",
      "Reads the preamble file.\n",
      "\n",
      "Args:\n",
      "    filename (str): the name of the preamble file\n",
      "\n",
      "Returns:\n",
      "    str: the preamble\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "204T\n",
      "Gets the Plink version from the binary.\n",
      "\n",
      "Args:\n",
      "    binary (str): the name of the Plink binary\n",
      "\n",
      "Returns:\n",
      "    str: the version of the Plink software\n",
      "\n",
      "This function uses :py:class:`subprocess.Popen` to gather the version of\n",
      "the Plink binary. Since executing the software to gather the version\n",
      "creates an output file, it is deleted.\n",
      "\n",
      "Warning\n",
      "-------\n",
      "    This function only works as long as the version is returned as\n",
      "    ``| PLINK! | NNN |`` (where, ``NNN`` is the version), since we use\n",
      "    regular expresion to extract the version number from the standard\n",
      "    output of the software.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "206T\n",
      "Checks the arguments and options.\n",
      "\n",
      "Args:\n",
      "    args (argparse.Namespace): the arguments and options\n",
      "\n",
      "Returns:\n",
      "    bool: `True` if everything is OK\n",
      "\n",
      "If an option is invalid, a :py:class:`genipe.error.GenipeError` is raised.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "207T\n",
      "Configure the jinja2 environment for LaTeX.\n",
      "\n",
      "Note\n",
      "----\n",
      "    The configuration used is for LaTeX documents. Hence, a block command\n",
      "    is done using ``\\BLOCK{}`` and variables using ``\\VAR{}`` in the Jinja2\n",
      "    template.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "208T\n",
      "Sanitize TeX text.\n",
      "\n",
      "Args:\n",
      "    original_text (str): the text to sanitize for LaTeX\n",
      "\n",
      "Text is sanitized by following these steps:\n",
      "\n",
      "1. Replaces ``\\\\`` by ``\\textbackslash``\n",
      "2. Escapes certain characters (such as ``$``, ``%``, ``_``, ``}``, ``{``,\n",
      "   ``&`` and ``#``) by adding a backslash (*e.g.* from ``&`` to ``\\&``).\n",
      "3. Replaces special characters such as ``~`` by the LaTeX equivalent\n",
      "   (*e.g.* from ``~`` to ``$\\sim$``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "209T\n",
      "Wraps the text.\n",
      "\n",
      "Args:\n",
      "    original_text (str): the text to wrap\n",
      "\n",
      "Returns:\n",
      "    str: a string where the original text was wrapped\n",
      "\n",
      "Wraps the text so that lines are no longer than 80 characters. Uses the\n",
      ":py:func:`str.join` function on the results of the :py:func:`wrap`\n",
      "function, so that a single string is returned.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "210T\n",
      "Change the TeX text format.\n",
      "\n",
      "Args:\n",
      "    text (str): the text for which the format needs to be specified\n",
      "    tex_format (str): the format of the text to return\n",
      "\n",
      "Returns:\n",
      "    str: the formatted text\n",
      "\n",
      "This will change the format by adding the LaTeX format command (*e.g.* from\n",
      "``text`` to ``\\texttt{text}``).\n",
      "\n",
      "Note\n",
      "----\n",
      "    Only the following format are available:\n",
      "\n",
      "    * ``texttt``\n",
      "    * ``emph``\n",
      "    * ``textbf``\n",
      "    * ``textit``\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "211T\n",
      "Creates an inline mathematical formula in TeX.\n",
      "\n",
      "Args:\n",
      "    content (str): the content of the mathematical formula\n",
      "\n",
      "Returns:\n",
      "    str: the formatted mathematical formula\n",
      "\n",
      "The function only adds ``$`` symbols before and after the content (*e.g.*\n",
      "from ``\\pi`` to ``$\\pi$``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "212T\n",
      "Check if text is sanitized.\n",
      "\n",
      "Args:\n",
      "    text (str): the text to check\n",
      "\n",
      "Returns:\n",
      "    bool: ``True`` if the text is sanitized, ``False`` otherwise\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "213T\n",
      "Creates a TeX tabular.\n",
      "\n",
      "Args:\n",
      "    template (jinja2.Template): the tabular template\n",
      "    header (list): the header of the tabular\n",
      "    data (list): the tabular data\n",
      "    header_multicol (list): the number of columns for the header\n",
      "    col_align (list): the column alignement\n",
      "\n",
      "Returns:\n",
      "    str: a string representation of a LaTeX tabular\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "214T\n",
      "Creates a TeX float.\n",
      "\n",
      "Args:\n",
      "    template (jinja2.Template): the float template\n",
      "    float_type (str): the type of float (``figure`` or  ``table``)\n",
      "    caption (str): the caption of the float\n",
      "    label (str): the label of the float\n",
      "    content (str): the content of the float\n",
      "    placement (str): the float placement (*e.g.* ``H``)\n",
      "\n",
      "Returns:\n",
      "    str: a string representation of a LaTeX float\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "215T\n",
      "Format time (either \"HH:MM:SS\" or \"H hours, M minutes and S seconds\".\n",
      "\n",
      "Args:\n",
      "    total_seconds (int): the total number of seconds\n",
      "    written_time (bool): whether to write time in written language\n",
      "\n",
      "Returns:\n",
      "    str: a string representation of the total time\n",
      "\n",
      "If ``written_time`` is ``True``, time will be displayed as \"H hours, M\n",
      "minutes and S seconds\". Otherwise, the time will be represented as\n",
      "HH:MM:SS.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "216T\n",
      "Colorize the time.\n",
      "\n",
      "Args:\n",
      "    total_seconds (int): the total number of seconds\n",
      "\n",
      "Returns:\n",
      "    str: a colorized LaTeX string representation of time\n",
      "\n",
      "The time is displayed as ``HH:MM:SS``, but insignificant zeros are\n",
      "grayed-out.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "217T\n",
      "Generate the report.\n",
      "\n",
      "Args:\n",
      "    out_dir (str): the output directory for the report\n",
      "    run_opts (dict): the run options\n",
      "    run_info (dict): the run information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "218T\n",
      "Generates the background section of the report.\n",
      "\n",
      "Args:\n",
      "    templates (jinja2.Environment): the jinja2 template environment\n",
      "    run_options (dict): the run options\n",
      "    run_information (dict): the run information\n",
      "\n",
      "Returns:\n",
      "    str: a string representation of the \"background\" section\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "219T\n",
      "Generate the method section of the report.\n",
      "\n",
      "Args:\n",
      "    templates (jinja2.Environment): the jinja2 template environment\n",
      "    run_options (dict): the run options\n",
      "    run_information (dict): the run information\n",
      "\n",
      "Returns:\n",
      "    str: a string representation of the \"methods\" section\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "220T\n",
      "Generates the results section of the report.\n",
      "\n",
      "Args:\n",
      "    templates (jinja2.Environment): the jinja2 template environment\n",
      "    run_options (dict): the run options\n",
      "    run_information (dict): the run information\n",
      "\n",
      "Returns:\n",
      "    str: a string representation of the \"results\" section\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "221T\n",
      "Generates the background section of the report.\n",
      "\n",
      "Args:\n",
      "    templates (jinja2.Environment): the jinja2 template environment\n",
      "    run_options (dict): the run options\n",
      "    run_information (dict): the run information\n",
      "\n",
      "Returns:\n",
      "    str: a string representation of the \"conclusions\" section\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "222T\n",
      "Generates the annex section of the report (execution times).\n",
      "\n",
      "Args:\n",
      "    templates (jinja2.Environment): the jinja2 template environment\n",
      "    run_options (dict): the run options\n",
      "    run_information (dict): the run information\n",
      "\n",
      "Returns:\n",
      "    str: a string representation of the \"Annex\" section\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "223T\n",
      "Generates time tables (split one long table in two).\n",
      "\n",
      "Args:\n",
      "    task_name (str): the name of the task\n",
      "    label (str): the label for the float\n",
      "    table (list): the data for the float\n",
      "    header (str): the header for the tables\n",
      "    tabular_t (jinja2.Template): the template for the tabular\n",
      "    float_t (jinja2.Template): the template for the float\n",
      "    first_time_col (int): the first column containing time (base 0)\n",
      "\n",
      "Returns:\n",
      "    str: a LaTeX float\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "224T\n",
      "Colorize the time in the table (columns 2 and up).\n",
      "\n",
      "Args:\n",
      "    table (list): the data for the tabular\n",
      "    first_col (int): the first column containing time\n",
      "\n",
      "Returns:\n",
      "    list: the same data, but with time column colorized\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "225T\n",
      "Parses the tasks' configuration file for DRMAA.\n",
      "\n",
      "Args:\n",
      "    configfile (str): the name of the configuration file\n",
      "\n",
      "Returns:\n",
      "    dict: the DRMAA configuration for each task\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "226T\n",
      "Generates default values for missing DRMAA configuration.\n",
      "\n",
      "Args:\n",
      "    task_name (str): the name of the task\n",
      "    config (dict): the configuration\n",
      "    walltime (str): the default execution time\n",
      "    nodes (str): the default number of nodes\n",
      "    ppn (str): the default number of processes\n",
      "    only_one (bool): if there is only on task (and not one per chromosome)\n",
      "    template (str): task name template (for each chromosome)\n",
      "\n",
      "Returns:\n",
      "    dict: the final configuration for this task\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "227T\n",
      "Concatenates and extracts information from IMPUTE2 GEN file(s).\n",
      "\n",
      "Args:\n",
      "    i_filenames (list): the list of input filenames (to concatenate)\n",
      "    out_prefix (str): the output prefix for the output files\n",
      "    real_chrom (str): the chromosome contained in all the input files\n",
      "    options (argparse.Namespace): the options\n",
      "\n",
      "This function will create the following seven files:\n",
      "\n",
      "+-----------------------+-------------------------------------------------+\n",
      "| File name             | Description                                     |\n",
      "+=======================+=================================================+\n",
      "| ``.impute2``          | Imputation results (merged from all the input   |\n",
      "|                       | files).                                         |\n",
      "+-----------------------+-------------------------------------------------+\n",
      "| ``.alleles``          | Description of the reference and alternative    |\n",
      "|                       | allele at each sites.                           |\n",
      "+-----------------------+-------------------------------------------------+\n",
      "| ``.imputed_sites``    | List of imputed sites (excluding sites that     |\n",
      "|                       | were previously genotyped in the study cohort). |\n",
      "+-----------------------+-------------------------------------------------+\n",
      "| ``.impute2_info``     | SNP-wise information file with one line per SNP |\n",
      "|                       | and a single header line at the beginning.      |\n",
      "+-----------------------+-------------------------------------------------+\n",
      "| ``.completion_rates`` | Number of missing values and completion rate    |\n",
      "|                       | for all sites (using the probability threshold  |\n",
      "|                       | set by the user, where the default is higher    |\n",
      "|                       | and equal to 0.9).                              |\n",
      "+-----------------------+-------------------------------------------------+\n",
      "| ``.good_sites``       | List of sites which pass the completion rate    |\n",
      "|                       | threshold (set by the user, where the default   |\n",
      "|                       | is higher and equal to 0.98) using the          |\n",
      "|                       | probability threshold (set by the user, where   |\n",
      "|                       | the default is higher and equal to 0.9).        |\n",
      "+-----------------------+-------------------------------------------------+\n",
      "| ``.map``              | A map file describing the genomic location of   |\n",
      "|                       | all sites.                                      |\n",
      "+-----------------------+-------------------------------------------------+\n",
      "| ``.maf``              | File containing the minor allele frequency      |\n",
      "|                       | (along with minor allele identification) for    |\n",
      "|                       | all sites using the probabilitty threshold of   |\n",
      "|                       | 0.9. When no genotypes are available (because   |\n",
      "|                       | they are all below the threshold), the MAF is   |\n",
      "|                       | ``NA``.                                         |\n",
      "+-----------------------+-------------------------------------------------+\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "228T\n",
      "Checks the arguments and options.\n",
      "\n",
      "Args:\n",
      "    args (argparse.Namespace): the options to verify\n",
      "\n",
      "Note\n",
      "----\n",
      "    If there is a problem, a :py:class:`genipe.error.GenipeError` is\n",
      "    raised.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "229T\n",
      "Parses the command line options and arguments.\n",
      "\n",
      "Args:\n",
      "    parser (argparse.ArgumentParser): the argument parser\n",
      "    args (list): the list of arguments (if not taken from ``sys.argv``)\n",
      "\n",
      "Returns:\n",
      "    argparse.Namespace: the list of options and arguments\n",
      "\n",
      "Note\n",
      "----\n",
      "    The only check that is done here is by the parser itself. Values are\n",
      "    verified later by the :py:func:`check_args` function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "230T\n",
      "Checks if the SKAT R library is installed.\n",
      "\n",
      "Returns:\n",
      "    bool: True if SKAT is installed, False otherwise.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "231T\n",
      "Reads the phenotype file.\n",
      "\n",
      "Args:\n",
      "    i_filename (str): the name of the input file\n",
      "    opts (argparse.Namespace): the options\n",
      "    check_duplicated (bool): whether or not to check for duplicated samples\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the phenotypes\n",
      "\n",
      "This file is expected to be a tab separated file of phenotypes and\n",
      "covariates. The columns to use will be determined by the\n",
      "``--sample-column`` and the ``--covar`` options.\n",
      "\n",
      "For analysis including the X chromosome, the gender is automatically\n",
      "added as a covariate. The results are not shown to the user unless asked\n",
      "for.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "232T\n",
      "Reads the sample file (produced by SHAPEIT).\n",
      "\n",
      "Args:\n",
      "    i_filename (str): the name of the input file\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the list of samples\n",
      "\n",
      "This file contains the list of samples that are contained in the\n",
      "``impute2`` file (with same order). The expected format for this file is a\n",
      "tab separated file with a first row containing the following columns: ::\n",
      "\n",
      "    ID_1    ID_2    missing father  mother  sex     plink_pheno\n",
      "\n",
      "The subsequent row will be discarded and should contain: ::\n",
      "\n",
      "    0       0       0 D     D       D       B\n",
      "\n",
      "Notes\n",
      "-----\n",
      "    We are mostly interested in the sample IDs corresponding to the\n",
      "    ``ID_2`` column. Their uniqueness is verified by pandas.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "233T\n",
      "Reads the SKAT SNP set file.\n",
      "\n",
      "Args:\n",
      "    i_filename (str): the name of the input file\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the SNP set for the SKAT analysis\n",
      "\n",
      "This file has to be supplied by the user. The recognized columns are:\n",
      "``variant``, ``snp_set`` and ``weight``. The ``weight`` column is optional\n",
      "and can be used to specify a custom weighting scheme for SKAT. If nothing\n",
      "is specified, the default Beta weights are used.\n",
      "\n",
      "The file has to be tab delimited.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "234T\n",
      "Reads the list of sites to extract.\n",
      "\n",
      "Args:\n",
      "    i_filename (str): The input filename containing the IDs of the variants\n",
      "                      to consider for the analysis.\n",
      "\n",
      "Returns:\n",
      "    set: A set containing the variants.\n",
      "\n",
      "The expected file format is simply a list of variants. Every row should\n",
      "correspond to a single variant identifier. ::\n",
      "\n",
      "    3:60069:t\n",
      "    rs123456:A\n",
      "    3:60322:A\n",
      "\n",
      "Typically, this is used to analyze only variants that passed some QC\n",
      "threshold. The :py:mod:`genipe` pipeline generates this file at the\n",
      "'merge_impute2' step.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "235T\n",
      "Read the impute2 file and run the SKAT analysis.\n",
      "\n",
      "Args:\n",
      "    impute2_filename (str): the name of the input file\n",
      "    samples (pandas.DataFrame): the samples\n",
      "    markers_to_extract (set): the set of markers to analyse\n",
      "    phenotypes (pandas.DataFrame): the phenotypes\n",
      "    remove_gender (bool): whether or not to remove the gender column\n",
      "    out_prefix (str): the output prefix\n",
      "    args (argparse.Namespace): the options\n",
      "\n",
      "\n",
      "This function does most of the \"dispatching\" to run SKAT. It writes the\n",
      "input files to the disk, runs the generated R scripts to do the actual\n",
      "analysis and then writes the results to disk.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "236T\n",
      "Calls Rscript with the generated script and parses the results.\n",
      "\n",
      "Args:\n",
      "    script_filename (str): the name of the script\n",
      "\n",
      "Returns:\n",
      "    tuple: two values: the *p-value* and the *q-value* (for SKAT-O, the\n",
      "           *q-value* is set to None)\n",
      "\n",
      "The results should be somewhere in the standard output. The expected\n",
      "format is: ::\n",
      "\n",
      "    _PYTHON_HOOK_QVAL:[0.123]\n",
      "    _PYTHON_HOOK_PVAL:[0.123]\n",
      "\n",
      "If the template script is modified, this format should still be respected.\n",
      "\n",
      "It is also noteworthy that this function uses ``Rscript`` to run the\n",
      "analysis. Hence, it should be in the path when using the imputed_stats\n",
      "skat mode.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "238T\n",
      "Parses a single line of the Impute2 file.\n",
      "\n",
      "Args:\n",
      "    line (str): a line from the Impute2 file\n",
      "    markers_of_interest (set): a set of markers that are required for the\n",
      "                               analysis\n",
      "    samples (pandas.DataFrame): contains the samples IDs (this is useful to\n",
      "                                make sure we return a dosage vector with\n",
      "                                the appropriate data)\n",
      "\n",
      "Returns:\n",
      "    tuple: Either None if the marker is not of interest or a tuple of\n",
      "           ``(name , dosage_vector)`` where ``name`` is a ``str``\n",
      "           representing the variant ID and ``dosage_vector`` is a numpy\n",
      "           array containing the dosage values for every sample in the\n",
      "           ``samples`` dataframe.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "239T\n",
      "Write the dosage information to the appropriate genotype file.\n",
      "\n",
      "Args:\n",
      "    name (str): the name of the marker\n",
      "    dosage (numpy.array): the dosage vector\n",
      "    snp_set (pandas.DataFrame: the dataframe that allows us to identify the\n",
      "                               correct SNP set for the specified variant\n",
      "    genotype_files (dict): a dictionary containing the opened CSV files for\n",
      "                           the genotypes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "240T\n",
      "Extracts the random effects from a MixedLM fit object.\n",
      "\n",
      "Args:\n",
      "    fitted (MixedLMResultsWrapper): The fitted object.\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: The random effects as a DataFrame (with a column\n",
      "                      named \"RE\").\n",
      "\n",
      "Note\n",
      "====\n",
      "    Depending of the version of StatsModels, the object might be a pandas\n",
      "    DataFrame or a dictionary...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "241T\n",
      "Parses IMPUTE2 file while computing statistics.\n",
      "\n",
      "Args:\n",
      "    impute2_filename (str): the name of the input file\n",
      "    samples (pandas.DataFrame): the list of samples\n",
      "    markers_to_extract (set): the set of markers to extract\n",
      "    phenotypes (pandas.DataFrame): the phenotypes\n",
      "    remove_gender (bool): whether or not to remove the gender column\n",
      "    out_prefix (str): the output prefix\n",
      "    options (argparse.Namespace): the options\n",
      "\n",
      "This function takes care of parallelism. It reads the Impute2 file and\n",
      "fills a queue that will trigger the analysis when full.\n",
      "\n",
      "If the number of process to launch is 1, the rows are analyzed as they\n",
      "come.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "242T\n",
      "Process an IMPUTE2 site (a line in an IMPUTE2 file).\n",
      "\n",
      "Args:\n",
      "    site_info (list): the impute2 line (split by space)\n",
      "\n",
      "Returns:\n",
      "    list: the results of the analysis\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "243T\n",
      "Gets male and heterozygous calls.\n",
      "\n",
      "Args:\n",
      "    data (pandas.DataFrame): the probability matrix\n",
      "    hetero_c (str): the name of the heterozygous column\n",
      "\n",
      "Returns:\n",
      "    pandas.Index: samples where call is heterozygous\n",
      "\n",
      "Note\n",
      "----\n",
      "    If there are no data (i.e. no males), an empty list is returned.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "244T\n",
      "Creates the linear/logistic regression formula (for statsmodel).\n",
      "\n",
      "Args:\n",
      "    phenotype (str): the phenotype column\n",
      "    covars (list): the list of co variable columns\n",
      "    interaction (str): the interaction column\n",
      "\n",
      "Returns:\n",
      "    str: the formula for the statistical analysis\n",
      "\n",
      "Note\n",
      "----\n",
      "    The phenotype column needs to be specified. The list of co variables\n",
      "    might be empty (if no co variables are necessary). The interaction\n",
      "    column can be set to ``None`` if there is no interaction.\n",
      "\n",
      "Note\n",
      "----\n",
      "    The gender column should be categorical (hence, the formula requires\n",
      "    the gender to be included into ``C()``, *e.g.* ``C(Gender)``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "245T\n",
      "Fit a Cox' proportional hazard to the data.\n",
      "\n",
      "Args:\n",
      "    data (pandas.DataFrame): the data to analyse\n",
      "    time_to_event (str): the time to event column for the survival analysis\n",
      "    event (str): the event column for the survival analysis\n",
      "    formula (str): the formula for the data preparation\n",
      "    result_col (str): the column that will contain the results\n",
      "\n",
      "Returns:\n",
      "    numpy.array: the results from the survival analysis\n",
      "\n",
      "Note\n",
      "----\n",
      "    Using alpha of 0.95, and default parameters.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "246T\n",
      "Fit a linear regression to the data.\n",
      "\n",
      "Args:\n",
      "    data (pandas.DataFrame): the data to analyse\n",
      "    formula (str): the formula for the linear regression\n",
      "    result_col (str): the column that will contain the results\n",
      "\n",
      "Returns:\n",
      "    list: the results from the linear regression\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "248T\n",
      "Fit a linear mixed effects model to the data.\n",
      "\n",
      "Args:\n",
      "    data (pandas.DataFrame): the data to analyse\n",
      "    formula (str): the formula for the linear mixed effects model\n",
      "    use_ml (bool): whether to use ML instead of REML\n",
      "    groups (str): the column containing the groups\n",
      "    result_col (str): the column that will contain the results\n",
      "    random_effects (pandas.Series): the random effects\n",
      "    mixedlm_p (float): the p-value threshold for which loci will be\n",
      "                       computed with the real MixedLM analysis\n",
      "    interaction (bool): Whether there is an interaction or not\n",
      "\n",
      "Returns:\n",
      "    list: the results from the linear mixed effects model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "249T\n",
      "Gets results from either a linear, a logistic or a mixedlm regression.\n",
      "\n",
      "Args:\n",
      "    fit_result (RegressionResults): the results from the regression\n",
      "    result_col (str): the name of the result column\n",
      "\n",
      "Returns:\n",
      "    list: the regression results\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "250T\n",
      "Gets results from either a linear, a logistic or a mixedlm regression.\n",
      "\n",
      "Args:\n",
      "    fit_result (RegressionResults): the results from the regression\n",
      "    result_col (str): the name of the result column\n",
      "\n",
      "Returns:\n",
      "    list: the regression results\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "251T\n",
      "Checks if the path is like a file (it might be a named pipe).\n",
      "\n",
      "Args:\n",
      "    fn (str): the path to check\n",
      "\n",
      "Returns:\n",
      "    bool: True if path is like a file, False otherwise.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "252T\n",
      "Checks the arguments and options.\n",
      "\n",
      "Args:\n",
      "    args (argparse.Namespace): the options to verify\n",
      "\n",
      "Note\n",
      "----\n",
      "    If there is a problem, a :py:class:`genipe.error.GenipeError` is\n",
      "    raised.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "253T\n",
      "Parses the command line options and arguments.\n",
      "\n",
      "Args:\n",
      "    parser (argparse.ArgumentParser): the argument parser\n",
      "    args (list): the list of arguments (if not taken from ``sys.argv``)\n",
      "\n",
      "Returns:\n",
      "    argparse.Namespace: the list of options and arguments\n",
      "\n",
      "Note\n",
      "----\n",
      "    The only check that is done here is by the parser itself. Values are\n",
      "    verified later by the :py:func:`check_args` function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "254T\n",
      "Generates a bash script to launch the imputation pipeline.\n",
      "\n",
      "Args:\n",
      "    path (str): the path to write the bash script\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "256T\n",
      "Getting the OS information.\n",
      "\n",
      "Returns:\n",
      "    tuple: first element is the name of the os, and the second is the\n",
      "           system's architecture\n",
      "\n",
      "Note\n",
      "----\n",
      "    The tutorial does not work on the Windows operating system. The script\n",
      "    will quit unless the operating system is Linux or Darwin (MacOSX).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "257T\n",
      "Gets the impute2's reference files.\n",
      "\n",
      "Args:\n",
      "    path (str): the path where to put the reference files\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "258T\n",
      "Gets the genotypes files.\n",
      "\n",
      "Args:\n",
      "    path (str): the path where to put the genotypes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "259T\n",
      "Gets the hg19 reference file.\n",
      "\n",
      "Args:\n",
      "    path (str): the path where to put the reference\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "260T\n",
      "Gets Plink depending of the system, and puts it in 'path'.\n",
      "\n",
      "Args:\n",
      "    os_name (str): the name of the OS\n",
      "    arch (str): the architecture of the system\n",
      "    path (str): the path where to put Plink\n",
      "\n",
      "Note\n",
      "====\n",
      "    If the binary is in the system path, it is copied to the destination\n",
      "    path. Otherwise, we download it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "261T\n",
      "Gets impute2 depending of the system, and puts it in 'path'.\n",
      "\n",
      "Args:\n",
      "    os_name (str): the name of the OS\n",
      "    arch (str): the architecture of the system\n",
      "    path (str): the path where to put impute2\n",
      "\n",
      "Note\n",
      "====\n",
      "    If the binary is in the system path, it is copied to the destination\n",
      "    path. Otherwise, we download it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "262T\n",
      "Gets shapeit depending of the system, and puts it in 'path'.\n",
      "\n",
      "Args:\n",
      "    os_name (str): the name of the OS\n",
      "    arch (str): the architecture of the system\n",
      "    path (str): the path where to put shapeit\n",
      "\n",
      "Note\n",
      "====\n",
      "    If the binary is in the system path, it is copied to the destination\n",
      "    path. Otherwise, we download it.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "263T\n",
      "Downloads a file from a URL to a path.\n",
      "\n",
      "Args:\n",
      "    url (str): the url to download\n",
      "    path (str): the path where to save the file\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "264T\n",
      "Extracts a tar archive.\n",
      "\n",
      "Args:\n",
      "    path (str): the path to where the file will be extracted\n",
      "    fn (str): the name of the tar archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "265T\n",
      "Parses the command line options and arguments.\n",
      "\n",
      "Args:\n",
      "    parser (argparse.ArgumentParser): the argument parser\n",
      "    args (list): the list of arguments (if not taken from ``sys.argv``)\n",
      "\n",
      "Returns:\n",
      "    argparse.Namespace: the list of options and arguments\n",
      "\n",
      "Note\n",
      "----\n",
      "    The only check that is done here is by the parser itself. Values are\n",
      "    verified later by the :py:func:`check_args` function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "266T\n",
      "Indexes the impute2 file.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the impute2 file\n",
      "\n",
      "This function uses the :py:func:`genipe.formats.index.get_index` to create\n",
      "the index file if it's missing.\n",
      "\n",
      "Note\n",
      "----\n",
      "    We won't catch the :py:class:`genipe.error.GenipeError` exception if\n",
      "    it's raised, since the message will be relevant to the user.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "267T\n",
      "Extracts according to names.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the input file\n",
      "    to_extract (set): the list of markers to extract for each input file\n",
      "    out_prefix (str): the output prefix\n",
      "    out_format (list): the output format(s)\n",
      "    prob_t (float): the probability threshold\n",
      "    is_long (bool): True if format needs to be long\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "268T\n",
      "Reads the sample files, and extract the information.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the sample file\n",
      "\n",
      "Returns:\n",
      "    pandas.DataFrame: the sample information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "269T\n",
      "Extract markers from companion files (if they exists).\n",
      "\n",
      "Args:\n",
      "    i_prefix (str): the prefix of the input file\n",
      "    o_prefix (str): the prefix of the output file\n",
      "    to_extract (set): the set of markers to extract\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "270T\n",
      "Prints an impute2 line.\n",
      "\n",
      "Args:\n",
      "    o_files (dict): the output files\n",
      "    prob_t (float): the probability threshold\n",
      "    fid (list): the list of family IDs\n",
      "    iid (list): the list of sample IDs\n",
      "    is_long (bool): True if the format is long (dosage, calls)\n",
      "    line (str): the impute2 line\n",
      "    row (list): the impute2 line, split by spaces\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "271T\n",
      "Gather positions that are required.\n",
      "\n",
      "Args:\n",
      "    fn (str): the impute2 filename\n",
      "    maf (float): the minor allele frequency threshold (might be ``None``)\n",
      "    rate (float): the call rate threshold (might be ``None``)\n",
      "    info (float): the marker information value threshold (might be\n",
      "                  ``None``)\n",
      "    extract_filename (str): the name of the file containing marker names to\n",
      "                            extract (might be ``None``)\n",
      "    genomic_range (str): the genomic range for extraction\n",
      "\n",
      "Returns:\n",
      "    set: the set of markers to extract\n",
      "\n",
      "If extraction by marker name is required, only those markers will be\n",
      "extracted. Otherwise, ``maf``, ``rate``, ``info`` or ``genomic_range`` can\n",
      "be specified (alone or together) to extract markers according to minor\n",
      "allele frequency, call rate and genomic location.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "272T\n",
      "Gets the filename prefix.\n",
      "\n",
      "Args:\n",
      "    fn (str): the name of the file from which the prefix is required\n",
      "\n",
      "Returns:\n",
      "    str: the prefix of the file\n",
      "\n",
      "This function removes the extension from the file name, and return its\n",
      "prefix (*e.g.* ``test.impute2`` returns ``test``, and\n",
      "``../test.impute2.gz`` returns ``../test``).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "273T\n",
      "Checks the arguments and options.\n",
      "\n",
      "Args:\n",
      "    args (argparse.Namespace): the options to verify\n",
      "\n",
      "Note\n",
      "----\n",
      "    If there is a problem, a :py:class:`genipe.error.GenipeError` is\n",
      "    raised.\n",
      "\n",
      "Note\n",
      "----\n",
      "    Noting is checked (apart from the impute2 files) if indexation is asked\n",
      "    (``--index`` option).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "274T\n",
      "Parses the command line options and arguments.\n",
      "\n",
      "Args:\n",
      "    parser (argparse.ArgumentParser): the argument parser\n",
      "    args (list): the list of arguments (if not taken from ``sys.argv``)\n",
      "\n",
      "Returns:\n",
      "    argparse.Namespace: the list of options and arguments\n",
      "\n",
      "Note\n",
      "----\n",
      "    The only check that is done here is by the parser itself. Values are\n",
      "    verified later by the :py:func:`check_args` function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "275T\n",
      "HTML request with rate-limiting base on response code\n",
      "\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "url : str\n",
      "    The url for the request\n",
      "rtype : str\n",
      "    The request type (oneof [\"GET\", \"POST\"])\n",
      "num_attempts : int\n",
      "    In case of a failed retrieval, the number of attempts to try again\n",
      "sleep_time : int\n",
      "    The amount of time to wait between requests, in case of\n",
      "    API rate limits\n",
      "**kwargs : dict\n",
      "    The keyword arguments to pass to the request\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "response : requests.models.Response\n",
      "    The server response object. Only returned if request was successful,\n",
      "    otherwise returns None.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "276T\n",
      "Get the full PDB file associated with a PDB_ID\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "pdb_id : A 4 character string giving a pdb entry of interest\n",
      "\n",
      "filetype: The file type.\n",
      "    PDB is the older file format,\n",
      "    CIF is the newer replacement.\n",
      "    XML an also be obtained and parsed using the various xml tools included in PyPDB\n",
      "    STRUCTFACT retrieves structure factors (only available for certain PDB entries)\n",
      "\n",
      "compression : Whether or not to request the data as a compressed (gz) version of the file\n",
      "    (note that the compression is undone by this function)\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "result : string\n",
      "    The string representing the full PDB file as an uncompressed string.\n",
      "    (returns None if the request to RCSB failed)\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> pdb_file = get_pdb_file('4lza', filetype='cif', compression=True)\n",
      ">>> print(pdb_file[:200])\n",
      "data_4LZA\n",
      "#\n",
      "_entry.id   4LZA\n",
      "#\n",
      "_audit_conform.dict_name       mmcif_pdbx.dic\n",
      "_audit_conform.dict_version    4.032\n",
      "_audit_conform.dict_location   http://mmcif.pdb.org/dictionaries/ascii/mmcif_pdbx\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "277T\n",
      "Performs search specified by `search_operator`.\n",
      "Returns entity strings of type `return_type` that match the resulting hits.\n",
      "\n",
      "Strictly a subset of the functionality exposed in\n",
      "`perform_search_with_graph`, this function does not support searching on\n",
      "multiple conditions at once.\n",
      "\n",
      "If you require this functionality, please use `perform_search_with_graph`\n",
      "instead.\n",
      "\n",
      "Args:\n",
      "    search_operator: Parameters defining the search condition.\n",
      "    return_type: What type of RCSB entity to return.\n",
      "    request_options: Object containing information for result pagination\n",
      "      and sorting functionality.\n",
      "    return_with_scores: Whether or not to return the entity results with\n",
      "        their associated scores. For example, you might want to do this to\n",
      "        get\n",
      "        the top X hits that are similar to a certain protein sequence.\n",
      "        (if this is true, returns List[ScoredResult] instead of List[str])\n",
      "    return_raw_json_dict: If True, this function returns the raw JSON\n",
      "        response from RCSB, instead of a\n",
      "\n",
      "Returns:\n",
      "    List of entity ids, corresponding to entities that match the given\n",
      "    query.\n",
      "\n",
      "    If `return_with_scores=True`, returns a list of ScoredResult instead.\n",
      "    If `return_raw_json_dict=True`, returns the raw JSON response from RCSB.\n",
      "\n",
      "Example usage to search for PDB entries that are from 'Mus musculus':\n",
      "```\n",
      "from pypdb.clients.search. import perform_search\n",
      "from pypdb.clients.search. import ReturnType\n",
      "from pypdb.clients.search.operators.text_operators import ExactMatchOperator\n",
      "pdb_ids = perform_search(\n",
      "           search_operator=text_operators.ExactMatchOperator(\n",
      "             attribute=\"rcsb_entity_source_organism.taxonomy_lineage.name\",\n",
      "             value=\"Mus musculus\"\n",
      "           ),\n",
      "           return_type=ReturnType.ENTRY)\n",
      "print(pdb_ids)\n",
      ")\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "278T\n",
      "Performs specified search using RCSB's search node logic.\n",
      "\n",
      "Essentially, this allows you to ask multiple questions in one RCSB query.\n",
      "\n",
      "For example, you can ask for structures that satisfy all of the following\n",
      "conditions at once:\n",
      "    * Are either from Mus musculus or from Homo sapiens lineage\n",
      "    * Are both under 4 angstroms of resolution, and published after 2019\n",
      "    * Are labelled as \"actin-binding protein\" OR\n",
      "        contain \"actin\" AND \"calmodulin\" in their titles.\n",
      "\n",
      "See https://search.rcsb.org/index.html#building-search-request under\n",
      "\"Terminal node\" and \"Group node\" for more details.\n",
      "\n",
      "Args:\n",
      "    query_object: Fully-specified SearchOperator or QueryGroup\n",
      "        object corresponding to the desired search.\n",
      "    return_type: Type of entities to return.\n",
      "    return_with_scores: Whether or not to return the entity results with\n",
      "        their associated scores. For example, you might want to do this to\n",
      "        get the top X hits that are similar to a certain protein sequence.\n",
      "    return_raw_json_dict: Whether to return raw JSON response.\n",
      "        (for example, to analyze the scores of various matches)\n",
      "\n",
      "Returns:\n",
      "    List of strings, corresponding to hits in the database. Will be of the\n",
      "    format specified by the `return_type`.\n",
      "\n",
      "    If `return_with_scores=True`, returns a list of ScoredResult instead.\n",
      "    If `return_raw_json_dict=True`, returns the raw JSON response from RCSB.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "279T\n",
      "Performs RCSB search with JSON query using GraphQL.\n",
      "\n",
      "For details on what the RCSB GraphQL interface is, see:\n",
      "    https://data.rcsb.org/index.html#gql-api\n",
      "\n",
      "This function should return the equivalent information as this site:\n",
      "    https://data.rcsb.org/graphql/index.html\n",
      "\n",
      "Args:\n",
      "    graphql_json_query: GraphQL JSON query, as a string. Whitespace doesn't\n",
      "        matter. e.g. \"{entry(entry_id:\"4HHB\"){exptl{method}}}\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "280T\n",
      "Fetches FASTA sequence associated with PDB structure from RCSB.\n",
      "\n",
      "Args:\n",
      "  rcsb_id: RCSB accession code of the structure of interest. E.g. `\"5RU3\"`\n",
      "\n",
      "Returns:\n",
      "  Dictionary containing FASTA result, from polymer entity id to the\n",
      "  `FastaSequence` object associated with that entity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "281T\n",
      "Look up all information about a given PDB ID\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "pdb_id : string\n",
      "    A 4 character string giving a pdb entry of interest\n",
      "\n",
      "url_root : string\n",
      "    The string root of the specific url for the request type\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "out : dict()\n",
      "    An ordered dictionary object corresponding to entry information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "282T\n",
      "Deprecated wrapper for fetching PDB files from RCSB Database.\n",
      "\n",
      "For new uses, please use `pypdb/clients/pdb/pdb_client.py`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "283T\n",
      "---\n",
      "WARNING: this function is deprecated and slated to be deleted due to RCSB\n",
      "API changes.\n",
      "\n",
      "See `pypdb/clients/search/EXAMPLES.md` for examples to use a\n",
      "`SequenceOperator` search to similar effect\n",
      "---\n",
      "\n",
      "Return BLAST search results for a given PDB ID.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "pdb_id : string\n",
      "    A 4 character string giving a pdb entry of interest\n",
      "\n",
      "chain_id : string\n",
      "    A single character designating the chain ID of interest\n",
      "identity_cutoff: float\n",
      "    Identity % at which to cut off results.\n",
      "\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "out : List of PDB IDs that match the given search.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      ">>> blast_results = get_blast('2F5N', chain_id='A')\n",
      ">>> print(blast_results[50])\n",
      "PELPEVETVRRELEKRIVGQKIISIEATYPRMVL--TGFEQLKKELTGKTIQGISRRGKYLIFEIGDDFRLISHLRMEGKYRLATLDAPREKHDHL\n",
      "TMKFADG-QLIYADVRKFGTWELISTDQVLPYFLKKKIGPEPTYEDFDEKLFREKLRKSTKKIKPYLLEQTLVAGLGNIYVDEVLWLAKIHPEKET\n",
      "NQLIESSIHLLHDSIIEILQKAIKLGGSSIRTY-SALGSTGKMQNELQVYGKTGEKCSRCGAEIQKIKVAGRGTHFCPVCQQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "284T\n",
      "Return a generator of the results returned by a search of\n",
      "the protein data bank. This generator is used internally.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "search_term : str\n",
      "    The search keyword\n",
      "\n",
      "field : str\n",
      "    The type of information to record about each entry\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      ">>> result_gen = find_results_gen('bleb')\n",
      ">>> pprint.pprint([item for item in result_gen][:5])\n",
      "['MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456Y BOUND WITH MGADP-BEFX',\n",
      " 'MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456Y BOUND WITH MGADP-ALF4',\n",
      " 'DICTYOSTELIUM DISCOIDEUM MYOSIN II MOTOR DOMAIN S456E WITH BOUND MGADP-BEFX',\n",
      " 'MYOSIN II DICTYOSTELIUM DISCOIDEUM MOTOR DOMAIN S456E BOUND WITH MGADP-ALF4',\n",
      " 'The structural basis of blebbistatin inhibition and specificity for myosin '\n",
      " 'II']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "285T\n",
      "Return an ordered list of the top papers returned by a keyword search of\n",
      "the RCSB PDB\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "search_term : str\n",
      "    The search keyword\n",
      "\n",
      "max_results : int\n",
      "    The maximum number of results to return\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "all_papers : list of strings\n",
      "    A descending-order list containing the top papers associated with\n",
      "    the search term in the PDB\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      ">>> matching_papers = find_papers('crispr',max_results=3)\n",
      ">>> print(matching_papers)\n",
      "['Crystal structure of a CRISPR-associated protein from thermus thermophilus',\n",
      "'CRYSTAL STRUCTURE OF HYPOTHETICAL PROTEIN SSO1404 FROM SULFOLOBUS SOLFATARICUS P2',\n",
      "'NMR solution structure of a CRISPR repeat binding protein']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "286T\n",
      "Convert OrderedDict to dict\n",
      "\n",
      "Takes a nested, OrderedDict() object and outputs a\n",
      "normal dictionary of the lowest-level key:val pairs\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "odict : OrderedDict\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "out : dict\n",
      "\n",
      "    A dictionary corresponding to the flattened form of\n",
      "    the input OrderedDict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "287T\n",
      "Remove the '@' character from the beginning of key names in a dict()\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "kk : dict\n",
      "    A dictionary containing keys with the @ character\n",
      "    (this pops up a lot in converted XML)\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "kk : dict (modified in place)\n",
      "    A dictionary where the @ character has been removed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "288T\n",
      "Remove duplicate entries from a list while preserving order\n",
      "\n",
      "This function uses Python's standard equivalence testing methods in\n",
      "order to determine if two elements of a list are identical. So if in the list [a,b,c]\n",
      "the condition a == b is True, then regardless of whether a and b are strings, ints,\n",
      "or other, then b will be removed from the list: [a, c]\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "list_with_dupes : list\n",
      "    A list containing duplicate elements\n",
      "\n",
      "Returns\n",
      "-------\n",
      "out : list\n",
      "    The list with the duplicate entries removed by the order preserved\n",
      "\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> a = [1,3,2,4,2]\n",
      ">>> print(remove_dupes(a))\n",
      "[1,3,2,4]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "289T\n",
      "For a nested dictionary that may itself comprise lists of\n",
      "dictionaries of unknown length, determine if a key is anywhere\n",
      "in any of the dictionaries using a depth-first search\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "my_result : dict\n",
      "    A nested dict containing lists, dicts, and other objects as vals\n",
      "\n",
      "term : str\n",
      "    The name of the key stored somewhere in the tree\n",
      "\n",
      "maxdepth : int\n",
      "    The maximum depth to search the results tree\n",
      "\n",
      "depth : int\n",
      "    The depth of the search so far.\n",
      "    Users don't usually access this.\n",
      "\n",
      "outputs : list\n",
      "    All of the positive search results collected so far.\n",
      "    Users don't usually access this.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "outputs : list\n",
      "    All of the search results.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "290T\n",
      "Merge overlapping regions within a chromosome/strand.\n",
      "\n",
      "Assume chromosome and (if relevant) strand are already identical, so only\n",
      "start and end coordinates are considered.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "291T\n",
      "Divide multiple rows where they overlap.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "keyed_rows : iterable\n",
      "    pairs of (non-overlapping-group index, overlapping rows)\n",
      "combine : dict\n",
      "    Mapping of field names to functions applied to combine overlapping\n",
      "    regions.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "292T\n",
      "Divide multiple rows where they overlap.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "keyed_rows : iterable\n",
      "    pairs of (non-overlapping-group index, overlapping rows)\n",
      "combine : dict\n",
      "    Mapping of field names to functions applied to combine overlapping\n",
      "    regions.\n",
      "split_columns : list or tuple\n",
      "    Field names where numeric values should be subdivided a region.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "293T\n",
      "Merge overlapping regions within a chromosome/strand.\n",
      "\n",
      "Assume chromosome and (if relevant) strand are already identical, so only\n",
      "start and end coordinates are considered.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "294T\n",
      "Identify and enumerate groups of overlapping rows.\n",
      "\n",
      "That is, increment the group ID after each non-negative gap between\n",
      "intervals. Intervals (rows) will be merged if any bases overlap by at least\n",
      "`bp`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "295T\n",
      "Combine multiple rows into one NamedTuple.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "keyed_rows : iterable\n",
      "    pairs of (non-overlapping-group index, overlapping rows)\n",
      "combine : dict\n",
      "\n",
      "Returns\n",
      "-------\n",
      "namedtuple\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "296T\n",
      "Split large regions into smaller, consecutive regions.\n",
      "\n",
      "Output bin metadata and additional columns match the input dataframe.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "avg_size : int\n",
      "    Split regions into equal-sized subregions of about this size.\n",
      "    Specifically, subregions are no larger than 150% of this size, no\n",
      "    smaller than 75% this size, and the average will approach this size\n",
      "    when subdividing a large region.\n",
      "min_size : int\n",
      "    Drop any regions smaller than this size.\n",
      "verbose : bool\n",
      "    Print a log message when subdividing a region.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "297T\n",
      "Parse a chromosomal range specification.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "text : string\n",
      "    Range specification, which should look like ``chr1:1234-5678`` or\n",
      "    ``chr1:1234-`` or ``chr1:-5678``, where missing start becomes 0 and\n",
      "    missing end becomes None.\n",
      "keep_gene : bool\n",
      "    If True, include gene names as a 4th field where available; otherwise return a\n",
      "    3-field Region of chromosomal coordinates without gene labels.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "298T\n",
      "Extract chromosome, start, end from a string or tuple.\n",
      "\n",
      "Examples::\n",
      "\n",
      "    \"chr1\" -> (\"chr1\", None, None)\n",
      "    \"chr1:100-123\" -> (\"chr1\", 99, 123)\n",
      "    (\"chr1\", 100, 123) -> (\"chr1\", 100, 123)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "299T\n",
      "Determine the number of \"big\" chromosomes from their lengths.\n",
      "\n",
      "In the human genome, this returns 24, where the canonical chromosomes 1-22,\n",
      "X, and Y are considered \"big\", while mitochrondria and the alternative\n",
      "contigs are not. This allows us to exclude the non-canonical chromosomes\n",
      "from an analysis where they're not relevant.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "n_big : int\n",
      "    Number of \"big\" chromosomes in the genome.\n",
      "thresh : int\n",
      "    Length of the smallest \"big\" chromosomes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "300T\n",
      "Create a sorting key from chromosome label.\n",
      "\n",
      "Sort by integers first, then letters or strings. The prefix \"chr\"\n",
      "(case-insensitive), if present, is stripped automatically for sorting.\n",
      "\n",
      "E.g. chr1 < chr2 < chr10 < chrX < chrY < chrM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "301T\n",
      "Yields indices to extract ranges from `table`.\n",
      "\n",
      "Returns an iterable of integer arrays that can apply to Series objects,\n",
      "i.e. columns of `table`. These indices are of the DataFrame/Series' Index,\n",
      "not array coordinates -- so be sure to use DataFrame.loc, Series.loc, or\n",
      "Series getitem, as opposed to .iloc or indexing directly into Numpy arrays.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "302T\n",
      "Get a `combine` lookup suitable for `table`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "table : DataFrame\n",
      "stranded : bool\n",
      "combine : dict or None\n",
      "    Column names to their value-combining functions, replacing or in\n",
      "    addition to the defaults.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "dict:\n",
      "    Column names to their value-combining functions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "303T\n",
      "Read a GFF3/GTF/GFF2 file into a DataFrame.\n",
      "\n",
      "Works for all three formats because we only try extract the gene name, at\n",
      "most, from column 9.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "infile : filename or open handle\n",
      "    Source file.\n",
      "tag : str\n",
      "    GFF attributes tag to use for extracting gene names. In GFF3, this is\n",
      "    standardized as \"Name\", and in GTF it's \"gene_id\". (Neither spec is\n",
      "    consistently followed, so the parser will by default look for eith er\n",
      "    of those tags and also \"gene_name\" and \"gene\".)\n",
      "keep_type : str\n",
      "    If specified, only keep rows with this value in the 'type' field\n",
      "    (column 3). In GFF3, these terms are standardized in the Sequence\n",
      "    Ontology Feature Annotation (SOFA).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "304T\n",
      "Text coordinate format: \"chr:start-end\", one per line.\n",
      "\n",
      "Or sometimes: \"chrom:start-end gene\" or \"chrom:start-end REF>ALT\"\n",
      "\n",
      "Coordinate indexing is assumed to be from 1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "305T\n",
      "Read tab-separated data with column names in the first row.\n",
      "\n",
      "The format is BED-like, but with a header row included and with\n",
      "arbitrary extra columns.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "306T\n",
      "Read one tumor-normal pair or unmatched sample from a VCF file.\n",
      "\n",
      "By default, return the first tumor-normal pair or unmatched sample in the\n",
      "file.  If `sample_id` is a string identifier, return the (paired or single)\n",
      "sample  matching that ID.  If `sample_id` is a positive integer, return the\n",
      "sample or pair at that index position, counting from 0.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "307T\n",
      "Emit the sample IDs of all samples or tumor-normal pairs in the VCF.\n",
      "\n",
      "Determine tumor-normal pairs from the PEDIGREE tag(s). If no PEDIGREE tag\n",
      "is present, use the specified sample_id and normal_id as the pair, or if\n",
      "unspecified, emit all samples as unpaired tumors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "308T\n",
      "Extract tumor/normal pair sample IDs from the VCF header.\n",
      "\n",
      "Return an iterable of (tumor sample ID, normal sample ID).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "309T\n",
      "Parse VCF records into DataFrame rows.\n",
      "\n",
      "Apply filters to skip records with low depth, homozygosity, the REJECT\n",
      "flag, or the SOMATIC info field.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "310T\n",
      "UCSC Browser Extensible Data (BED) format.\n",
      "\n",
      "A BED file has these columns::\n",
      "\n",
      "    chromosome, start position, end position, [gene, strand, other stuff...]\n",
      "\n",
      "Coordinate indexing is from 0.\n",
      "\n",
      "Sets of regions are separated by \"track\" lines. This function stops reading\n",
      "after encountering a track line other than the first one in the file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "311T\n",
      "Parse the \"name\" field of a BED track definition line.\n",
      "\n",
      "Example:\n",
      "track name=146793_BastianLabv2_P2_target_region description=\"146793_BastianLabv2_P2_target_region\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "312T\n",
      "Group the parsed rows in a BED file by track.\n",
      "\n",
      "Yields (track_name, iterable_of_lines), much like itertools.groupby.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "313T\n",
      "GATK/Picard-compatible interval list format.\n",
      "\n",
      "Expected tabular columns:\n",
      "    chromosome, start position, end position, strand, gene\n",
      "\n",
      "Coordinate indexing is from 1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "314T\n",
      "Picard CalculateHsMetrics PER_TARGET_COVERAGE.\n",
      "\n",
      "The format is BED-like, but with a header row and the columns::\n",
      "\n",
      "    chrom (str),\n",
      "    start, end, length (int),\n",
      "    name (str),\n",
      "    %gc, mean_coverage, normalized_coverage (float)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "315T\n",
      "Read one sample from a SEG file.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "sample_id : string, int or None\n",
      "    If a string identifier, return the sample matching that ID.  If a\n",
      "    positive integer, return the sample at that index position, counting\n",
      "    from 0. If None (default), return the first sample in the file.\n",
      "chrom_names : dict\n",
      "    Map (string) chromosome IDs to names. (Applied before chrom_prefix.)\n",
      "    e.g. {'23': 'X', '24': 'Y', '25': 'M'}\n",
      "chrom_prefix : str\n",
      "    Prepend this string to chromosome names. (Usually 'chr' or None)\n",
      "from_log10 : bool\n",
      "    Convert values from log10 to log2.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame of the selected sample's segments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "316T\n",
      "Parse a SEG file as an iterable of samples.\n",
      "\n",
      "Coordinates are automatically converted from 1-indexed to half-open\n",
      "0-indexed (Python-style indexing).\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "chrom_names : dict\n",
      "    Map (string) chromosome IDs to names. (Applied before chrom_prefix.)\n",
      "    e.g. {'23': 'X', '24': 'Y', '25': 'M'}\n",
      "chrom_prefix : str\n",
      "    Prepend this string to chromosome names. (Usually 'chr' or None)\n",
      "from_log10 : bool\n",
      "    Convert values from log10 to log2.\n",
      "\n",
      "Yields\n",
      "------\n",
      "Tuple of (string sample ID, DataFrame of segments)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "317T\n",
      "Format a dataframe or list of dataframes as SEG.\n",
      "\n",
      "To put multiple samples into one SEG table, pass `dframe` and `sample_id`\n",
      "as equal-length lists of data tables and sample IDs in matching order.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "318T\n",
      "Gene Predictions.\n",
      "\n",
      "::\n",
      "\n",
      "    table genePred\n",
      "    \"A gene prediction.\"\n",
      "        (\n",
      "        string  name;               \"Name of gene\"\n",
      "        string  chrom;              \"Chromosome name\"\n",
      "        char[1] strand;             \"+ or - for strand\"\n",
      "        uint    txStart;            \"Transcription start position\"\n",
      "        uint    txEnd;              \"Transcription end position\"\n",
      "        uint    cdsStart;           \"Coding region start\"\n",
      "        uint    cdsEnd;             \"Coding region end\"\n",
      "        uint    exonCount;          \"Number of exons\"\n",
      "        uint[exonCount] exonStarts; \"Exon start positions\"\n",
      "        uint[exonCount] exonEnds;   \"Exon end positions\"\n",
      "        )\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "319T\n",
      "Gene Predictions (Extended).\n",
      "\n",
      "The refGene table is an example of the genePredExt format.\n",
      "\n",
      "::\n",
      "\n",
      "    table genePredExt\n",
      "    \"A gene prediction with some additional info.\"\n",
      "        (\n",
      "        string name;                \"Name of gene (usually transcript_id from GTF)\"\n",
      "        string chrom;               \"Chromosome name\"\n",
      "        char[1] strand;             \"+ or - for strand\"\n",
      "        uint txStart;               \"Transcription start position\"\n",
      "        uint txEnd;                 \"Transcription end position\"\n",
      "        uint cdsStart;              \"Coding region start\"\n",
      "        uint cdsEnd;                \"Coding region end\"\n",
      "        uint exonCount;             \"Number of exons\"\n",
      "        uint[exonCount] exonStarts; \"Exon start positions\"\n",
      "        uint[exonCount] exonEnds;   \"Exon end positions\"\n",
      "        int score;                  \"Score\"\n",
      "        string name2;               \"Alternate name (e.g. gene_id from GTF)\"\n",
      "        string cdsStartStat;        \"enum('none','unk','incmpl','cmpl')\"\n",
      "        string cdsEndStat;          \"enum('none','unk','incmpl','cmpl')\"\n",
      "        lstring exonFrames;         \"Exon frame offsets {0,1,2}\"\n",
      "        )\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "320T\n",
      "Gene predictions (extended) plus a \"bin\" column (e.g. refGene.txt)\n",
      "\n",
      "Same as genePredExt, but an additional first column of integers with the\n",
      "label \"bin\", which UCSC Genome Browser uses for optimization.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "321T\n",
      "Gene predictions and RefSeq genes with gene names (e.g. refFlat.txt).\n",
      "\n",
      "This version of genePred associates the gene name with the gene prediction\n",
      "information. For example, the UCSC \"refFlat\" database lists HGNC gene names\n",
      "and RefSeq accessions for each gene, alongside the gene model coordinates\n",
      "for transcription region, coding region, and exons.\n",
      "\n",
      "::\n",
      "\n",
      "    table refFlat\n",
      "    \"A gene prediction with additional geneName field.\"\n",
      "        (\n",
      "        string  geneName;           \"Name of gene as it appears in Genome Browser.\"\n",
      "        string  name;               \"Name of gene\"\n",
      "        string  chrom;              \"Chromosome name\"\n",
      "        char[1] strand;             \"+ or - for strand\"\n",
      "        uint    txStart;            \"Transcription start position\"\n",
      "        uint    txEnd;              \"Transcription end position\"\n",
      "        uint    cdsStart;           \"Coding region start\"\n",
      "        uint    cdsEnd;             \"Coding region end\"\n",
      "        uint    exonCount;          \"Number of exons\"\n",
      "        uint[exonCount] exonStarts; \"Exon start positions\"\n",
      "        uint[exonCount] exonEnds;   \"Exon end positions\"\n",
      "        )\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "cds : bool\n",
      "    Emit each gene's CDS region (coding and introns, but not UTRs) instead\n",
      "    of the full transcript region (default).\n",
      "exons : bool\n",
      "    Emit individual exonic regions for each gene instead of the full\n",
      "    transcribed genomic region (default). Mutually exclusive with `cds`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "322T\n",
      "Print the measurements that would be output by a pipeline\n",
      "\n",
      "This function calls Pipeline.get_measurement_columns() to get the\n",
      "measurements that would be output by a pipeline. This can be used in\n",
      "a workflow tool or LIMS to find the outputs of a pipeline without\n",
      "running it. For instance, someone might want to integrate CellProfiler\n",
      "with Knime and write a Knime node that let the user specify a pipeline\n",
      "file. The node could then execute CellProfiler with the --measurements\n",
      "switch and display the measurements as node outputs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "323T\n",
      "Print the image set groups for this pipeline\n",
      "\n",
      "This function outputs a JSON string to the console composed of a list\n",
      "of the groups in the pipeline image set. Each element of the list is\n",
      "a two-tuple whose first element is a key/value dictionary of the\n",
      "group's key and the second is a tuple of the image numbers in the group.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "324T\n",
      "Print the commands needed to run the given batch data file headless\n",
      "\n",
      "filename - the name of a Batch_data.h5 file. The file should group image sets.\n",
      "\n",
      "The output assumes that the executable, \"CellProfiler\", can be used\n",
      "to run the command from the shell. Alternatively, the output could be\n",
      "run through a utility such as \"sed\":\n",
      "\n",
      "CellProfiler --get-batch-commands Batch_data.h5 | sed s/CellProfiler/farm_job.sh/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "325T\n",
      "Print the commands needed to run the given batch data file headless\n",
      "\n",
      "filename - the name of a Batch_data.h5 file. The file may (but need not) group image sets.\n",
      "\n",
      "You can explicitly set the batch size with --images-per-batch, but note that\n",
      "it will override existing groupings, so use with caution\n",
      "\n",
      "The output assumes that the executable, \"CellProfiler\", can be used\n",
      "to run the command from the shell. Alternatively, the output could be\n",
      "run through a utility such as \"sed\":\n",
      "\n",
      "CellProfiler --get-batch-commands Batch_data.h5 | sed s/CellProfiler/farm_job.sh/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "326T\n",
      "Calculate threshold based on mean & standard deviation.\n",
      "The threshold is calculated by trimming the top and bottom 5% of\n",
      "pixels off the image, then calculating the mean and standard deviation\n",
      "of the remaining image. The threshold is then set at 2 (empirical\n",
      "value) standard deviations above the mean.\n",
      "\n",
      "\n",
      "lower_outlier_fraction - after ordering the pixels by intensity, remove\n",
      "    the pixels from 0 to len(image) * lower_outlier_fraction from\n",
      "    the threshold calculation (default = 0.05).\n",
      "upper_outlier_fraction - remove the pixels from\n",
      "    len(image) * (1 - upper_outlier_fraction) to len(image) from\n",
      "    consideration (default = 0.05).\n",
      "averaging_method - Determines how the intensity midpoint is determined\n",
      "    after discarding outliers. (default \"Mean\". Options: \"Mean\", \"Median\",\n",
      "    \"Mode\").\n",
      "variance_method - Method to calculate variance (default =\n",
      "    \"Standard deviation\". Options: \"Standard deviation\",\n",
      "    \"Median absolute deviation\")\n",
      "number_of_deviations - Following calculation of the standard deviation\n",
      "    or MAD, multiply this number and add to the average to get the final\n",
      "    threshold (default = 2)\n",
      "average_fn - function used to calculate the average intensity (e.g.\n",
      "    np.mean, np.median or some sort of mode function). Default = np.mean\n",
      "variance_fn - function used to calculate the amount of variance.\n",
      "                Default = np.sd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "327T\n",
      "For a given input_objects array, save crops for each \n",
      "object of the provided input_image.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "328T\n",
      "Remove all pixels but one from filled objects.\n",
      "If `fill` = False, thin objects with holes to loops.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "329T\n",
      "Remove pixels around the perimeter of an object unless\n",
      "doing so would change the objects Euler number `iterations` times. \n",
      "Processing stops automatically when there are no more pixels to\n",
      "remove.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "331T\n",
      "Remove or reduce the length of spurs in a skeletonized\n",
      "image. The algorithm reduces spur size by `iterations` pixels.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "332T\n",
      "Expand objects, assigning every pixel in the\n",
      "image to an object. Background pixels are assigned to the nearest\n",
      "object.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "333T\n",
      "Expand each object by adding background pixels\n",
      "adjacent to the image `iterations` times. Processing stops \n",
      "automatically if there are no more background pixels.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "334T\n",
      "Make overlapping objects combine into a single object, taking \n",
      "on the label of the object from the initial set.\n",
      "\n",
      "If an object overlaps multiple objects, each pixel of the added \n",
      "object will be assigned to the closest object from the initial \n",
      "set. This is primarily useful when the same objects appear in \n",
      "both sets.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "335T\n",
      "Preserve the initial object set. Any overlapping regions from \n",
      "the second set will be ignored in favour of the object from \n",
      "the initial set. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "336T\n",
      "Combine object sets and re-draw segmentation for overlapping\n",
      "objects.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "337T\n",
      "Returns three threshold values and a binary image.\n",
      "Thresholds returned are:\n",
      "\n",
      "Final threshold: Threshold following application of the\n",
      "threshold_correction_factor and clipping to min/max threshold\n",
      "\n",
      "orig_threshold: The threshold following either adaptive or global\n",
      "thresholding strategies, prior to correction\n",
      "\n",
      "guide_threshold: Only produced by adaptive threshold, otherwise None.\n",
      "This is the global threshold that constrains the adaptive threshold\n",
      "within a certain range, as defined by global_limits (default [0.7, 1.5])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "338T\n",
      "EnhanceEdges module\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "image : numpy.array\n",
      "    Input image\n",
      "mask : numpy.array, optional\n",
      "    Boolean mask, by default None\n",
      "method : str, optional\n",
      "    Enhance edges algorithm to apply to the input image, by default \"sobel\"\n",
      "direction : str, optional\n",
      "    Applicable to only the Sobel and Prewitt algorithms, by default \"all\"\n",
      "sigma : int, optional\n",
      "    Applicable to only the Canny and Laplacian of Gaussian algorithms, by default 10. Only considered if automatic_gaussian is False.\n",
      "automatic_threshold : bool, optional\n",
      "    Applicable only to the Canny algorithm, by default True\n",
      "manual_threshold : float, optional\n",
      "    Applicable only to the Canny algorithm, by default 0.2\n",
      "threshold_adjustment_factor : float, optional\n",
      "    Applicable only to the Canny algorithm, by default 1.0\n",
      "automatic_low_threshold : bool, optional\n",
      "    Applicable only to the Canny algorithm, by default True\n",
      "low_threshold : float, optional\n",
      "    Applicable only to the Canny algorithm, by default 0.1\n",
      "\n",
      "Returns\n",
      "-------\n",
      "numpy.array\n",
      "    Image with enhanced edges\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "339T\n",
      "Generate a presigned URL, if necessary (e.g., s3).\n",
      "\n",
      ":param url: An unsigned URL.\n",
      ":return: The presigned URL.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "340T\n",
      "xcol is (Nobservations,1) column vector of grouping values\n",
      "    (in terms of dose curve it may be Dose).\n",
      "ymatr is (Nobservations, Nmeasures) matrix, where rows correspond to\n",
      "    observations and columns corresponds to different measures.\n",
      "\n",
      "returns v, z, z_one_tailed, OrderedUniqueDoses, OrderedAverageValues\n",
      "z and z_bwtn_mean are (1, Nmeasures) row vectors containing Z'- and\n",
      "between-mean Z'-factors for the corresponding measures.\n",
      "\n",
      "When ranges are zero, we set the Z' factors to a very negative\n",
      "value.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "341T\n",
      "xcol is (Nobservations,1) column vector of grouping values\n",
      "    (in terms of dose curve it may be Dose).\n",
      "ymatr is (Nobservations, Nmeasures) matrix, where rows correspond to\n",
      "    observations and columns corresponds to different measures.\n",
      "\n",
      " Calculate the V factor = 1-6 * mean standard deviation / range\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "342T\n",
      "Compute mean and standard deviation per label\n",
      "\n",
      "xcol - column of image labels or doses\n",
      "ymatr - a matrix with rows of values per image and columns\n",
      "        representing different measurements\n",
      "\n",
      "returns xs - a vector of unique doses\n",
      "        avers - the average value per label\n",
      "        stds - the standard deviation per label\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "343T\n",
      "Identify unique labels from the vector of image labels\n",
      "\n",
      "x - a vector of one label or dose per image\n",
      "\n",
      "returns labels, labnum, uniqsortvals\n",
      "labels - a vector giving an ordinal per image where that ordinal\n",
      "         is an index into the vector of unique labels (uniqsortvals)\n",
      "labnum - # of unique labels in x\n",
      "uniqsortvals - a vector containing the unique labels in x\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "344T\n",
      "EC50 Function to fit a dose-response data to a 4 parameter dose-response\n",
      "curve.\n",
      "\n",
      "Inputs: 1. a 1 dimensional array of drug concentrations\n",
      "        2. the corresponding m x n array of responses\n",
      "Algorithm: generate a set of initial coefficients including the Hill\n",
      "           coefficient\n",
      "           fit the data to the 4 parameter dose-response curve using\n",
      "           nonlinear least squares\n",
      "Output: a matrix of the 4 parameters\n",
      "        results[m,1]=min\n",
      "        results[m,2]=max\n",
      "        results[m,3]=ec50\n",
      "        results[m,4]=Hill coefficient\n",
      "\n",
      "Original Matlab code Copyright 2004 Carlos Evangelista\n",
      "send comments to CCEvangelista@aol.com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "345T\n",
      "This is the EC50 sigmoid function\n",
      "\n",
      "v is a vector of parameters:\n",
      "    v[0] = minimum allowed value\n",
      "    v[1] = maximum allowed value\n",
      "    v[2] = ec50\n",
      "    v[3] = Hill coefficient\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "346T\n",
      "This generates the min, max, x value at the mid-y value, and Hill\n",
      "coefficient. These values are starting points for the sigmoid fitting.\n",
      "\n",
      "x & y are the points to be fit\n",
      "returns minimum, maximum, ec50 and hill coefficient starting points\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "347T\n",
      "Write out figure scripts for each measurement\n",
      "\n",
      "prefix - prefix for file names\n",
      "directory - write files into this directory\n",
      "dose_name - name of the dose measurement\n",
      "dose_data - doses per image\n",
      "data - data per image\n",
      "ec50_coeffs - coefficients calculated by calculate_ec50\n",
      "feature_set - tuples of object name and feature name in same order as data\n",
      "log_transform - true to log-transform the dose data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "348T\n",
      "Read a training set parameters  file\n",
      "\n",
      "training_set_directory - the training set directory setting\n",
      "\n",
      "training_set_file_name - the training set file name setting\n",
      "\n",
      "d - a dictionary that stores cached parameters\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "349T\n",
      "Recalculate the control points for labeled single worms\n",
      "\n",
      "Given a labeling of single worms, recalculate the control points\n",
      "for those worms.\n",
      "\n",
      "all_labels - a sequence of label matrices\n",
      "\n",
      "ncontrolpoints - the # of desired control points\n",
      "\n",
      "returns a two tuple:\n",
      "\n",
      "an N x M x 2 array where the first index is the object number,\n",
      "the second index is the control point number and the third index is 0\n",
      "for the Y or I coordinate of the control point and 1 for the X or J\n",
      "coordinate.\n",
      "\n",
      "a vector of N lengths.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "351T\n",
      "Look up the current value for a setting choice w/backwards compatibility\n",
      "\n",
      "x - setting value from pipeline\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "352T\n",
      "Picks out the hostname and port number, if any, from the specified MySQL host.\n",
      "Has to be in one of the following formats:\n",
      "    * IPv4 no port specified\n",
      "    192.168.1.10\n",
      "\n",
      "    * IPv4 with port specified\n",
      "    192.168.1.10:3306\n",
      "\n",
      "    * IPv6 no port specified\n",
      "    9001:0db8:85a3:0000:0000:8a2e:0370:7334\n",
      "\n",
      "    * IPv6 with port specified\n",
      "    [9001:0db8:85a3:0000:0000:8a2e:0370:7334]:3306\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "353T\n",
      "This is a very repeatable pseudorandom number generator\n",
      "\n",
      "seed - a string to seed the generator\n",
      "\n",
      "yields integers in the range 0-65535 on iteration\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "359T\n",
      "Bind ImageData etc to synchronize to color select button\n",
      "\n",
      "data_class - ImageData, ObjectData or MaskData\n",
      "color_select - a color select button whose color synchronizes\n",
      "               to that of the data\n",
      "fn_redraw - function to be called\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "361T\n",
      "Return the name of a setting's button\n",
      "\n",
      "v - the setting\n",
      "\n",
      "idx - if present, the index of one of several buttons for the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "364T\n",
      "For a range, return the control that sets the maximum value\n",
      "v - the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "365T\n",
      "For a range, return the control that chooses between absolute and relative\n",
      "\n",
      "v - the setting\n",
      "Absolute - far coordinate is an absolute value\n",
      "From edge - far coordinate is a distance from the far edge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "366T\n",
      "For coordinates, return the control that sets the x value\n",
      "v - the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "367T\n",
      "For coordinates, return the control that sets the y value\n",
      "v - the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "368T\n",
      "For measurements, return the control that sets the measurement category\n",
      "\n",
      "v - the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "369T\n",
      "For measurements, return the control that sets the feature name\n",
      "\n",
      "v - the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "370T\n",
      "For measurements, return the control that sets the image name\n",
      "\n",
      "v - the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "371T\n",
      "For measurements, return the control that sets the object name\n",
      "\n",
      "v - the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "372T\n",
      "For measurements, return the control that sets the measurement scale\n",
      "\n",
      "v - the setting\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "374T\n",
      "Validate a module and execute the callback on error on the main thread\n",
      "\n",
      "pipeline - a pipeline to be validated\n",
      "module_num - the module number of the module to be validated\n",
      "callback - a callback with the signature, \"fn(setting, message, pipeline_data)\"\n",
      "where setting is the setting that is in error and message is the message to\n",
      "display.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "375T\n",
      "Request that a module be validated\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "377T\n",
      "Display an error dialog, returning an indication of whether to continue\n",
      "\n",
      "frame - parent frame for application\n",
      "exc - exception that caused the error\n",
      "pipeline - currently executing pipeline\n",
      "message - message to display\n",
      "tb - traceback\n",
      "continue_only - show \"continue\" option, only\n",
      "remote_exc_info - None (the default) for exceptions in the current process.\n",
      "    For remote processes:\n",
      "        (exc_name, exc_message, traceback_text, filename, line_number, remote_debug_callback)\n",
      "\n",
      "Returns either ED_STOP or ED_CONTINUE indicating how to handle.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "378T\n",
      "Display an error dialog, returning an indication of whether to continue\n",
      "\n",
      "frame - parent frame for application\n",
      "exc - exception that caused the error\n",
      "pipeline - currently executing pipeline\n",
      "message - message to display\n",
      "tb - traceback\n",
      "continue_only - show \"continue\" option, only\n",
      "remote_exc_info - None (the default) for exceptions in the current process.\n",
      "    For remote processes:\n",
      "        (exc_name, exc_message, traceback_text, filename,\n",
      "         line_number, remote_event_queue)\n",
      "\n",
      "Returns either ED_STOP or ED_CONTINUE indicating how to handle.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "379T\n",
      "Show a silenceable warning message to the user\n",
      "\n",
      "title - title for the dialog box\n",
      "\n",
      "message - message to be displayed\n",
      "\n",
      "get_preference - function that gets a user preference: do you want to\n",
      "                 show this warning?\n",
      "\n",
      "set_preference - function that sets the user preference if they choose\n",
      "                 not to see the warning again.\n",
      "\n",
      "The message is printed to the console if headless.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "380T\n",
      "Display an error in a scrolling message box\n",
      "\n",
      "parent - parent window to the error message\n",
      "message - message to display in scrolling box\n",
      "title - title to display in frame\n",
      "buttons - a list of buttons to put at bottom of dialog. For instance,\n",
      "          [wx.ID_YES, wx.ID_NO]. Defaults to OK button\n",
      "size - size of frame. Defaults to 300 x 200 but will fit.\n",
      "\n",
      "returns the code from ShowModal.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "381T\n",
      "Set up a figure so that the image occupies the entire figure\n",
      "\n",
      "figure - a matplotlib figure\n",
      "shape - i/j size of the image being displayed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "382T\n",
      "Scramble the label numbers randomly to make the display more discernable\n",
      "\n",
      "The colors of adjacent indices in a color map are less discernable than\n",
      "those of far-apart indices. Nearby labels tend to be adjacent or close,\n",
      "so a random numbering has more color-distance between labels than a\n",
      "straightforward one\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "383T\n",
      "Find the beginning and ending indices of case insensitive matches of \"text\"\n",
      "within the text-data of the HTML, searching only in its body and excluding\n",
      "text in the HTML tags.\n",
      "\n",
      ":param html: an HTML document\n",
      ":param text: a search string\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "384T\n",
      "Search the help for a string\n",
      "\n",
      ":param text: find text in the module help using case-insensitive matching\n",
      ":return: an html document of all the module help pages that matched or None if no match found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "385T\n",
      "Find spacing for a list of pairs of text such that the left texts are\n",
      "left justified and the right texts (roughly) right justified.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "386T\n",
      "Get the likelihood of reaction equations\n",
      "\n",
      "Args:\n",
      "    r1, r2: two `RactionEntry` objects to be compared\n",
      "args, kwargs:\n",
      "    cpd_map: dictionary mapping compound id in the query model to a set\n",
      "             of best-mapping compound ids in the target model.\n",
      "    cpd_score: dictionary mapping compound id in the query model to\n",
      "               its best mapping score during compound mapping.\n",
      "    compartment_map: dictionary mapping compartment id in the query model\n",
      "                     to the id in the target model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "387T\n",
      "Merge the left hand side and right hand side p values together.\n",
      "\n",
      "The compound mapping is done separately on left hand side and\n",
      "right hand side.\n",
      "Then the corresponding p_match and p_no_match are merged together.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "388T\n",
      "Compute likelihood of all pairwise comparisons.\n",
      "\n",
      "Returns likelihoods as a dataframe with a column for each hypothesis.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "389T\n",
      "Run FastGapFill gap-filling algorithm by calling\n",
      ":func:`psamm.fastcore.fastcore`.\n",
      "\n",
      "FastGapFill will try to find a minimum subset of reactions that includes\n",
      "the core reactions and it also has no blocked reactions.\n",
      "Return the set of reactions in the minimum subset. An extended model that\n",
      "includes artificial transport and exchange reactions can be generated by\n",
      "calling :func:`.create_extended_model`.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "    core: reactions in the original metabolic model.\n",
      "    weights: a weight dictionary for reactions in the model.\n",
      "    solver: linear programming library to use.\n",
      "    epsilon: float number, threshold for Fastcore algorithm.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "390T\n",
      "Parse through model compounds and return a formula dict.\n",
      "\n",
      "This function will parse the compound information\n",
      "in a given model and return a dictionary of compound IDs to\n",
      "compound formula objects.\n",
      "\n",
      "Args:\n",
      "    model: <class 'psamm.datasource.native.NativeModel'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "391T\n",
      "Create a dictionary of reactant/product pairs to reaction directions.\n",
      "\n",
      "Returns a dictionary that connects predicted reactant/product pairs\n",
      "to their original reaction directions. This dictionary is used when\n",
      "generating bipartite graph objects. This can be done either using\n",
      "the FindPrimaryPairs method to predict reactant/product pairs or\n",
      "by mapping all possible pairs.\n",
      "\n",
      "Args:\n",
      "    nm: <class 'psamm.datasource.native.NativeModel'>, the native model\n",
      "    mm: <class 'psamm.metabolicmodel.MetabolicModel'>, the metabolic model.\n",
      "    subset: None or path to a file that contains a list of reactions or\n",
      "        compound ids. By default, it is None. It defines which reaction\n",
      "        need to be visualized.\n",
      "    method: 'fpp' or 'no-fpp', the method used for visualization.\n",
      "    element: Symbol of chemical atom, such as 'C' ('C' indicates carbon).\n",
      "    excluded_reactions: a list that contains reactions excluded from\n",
      "        visualization.\n",
      "    reaction_dict: dictionary of FBA or FVA results. By default it is an\n",
      "        empty dictionary.\n",
      "    analysis: \"None\" type or a string indicates if FBA or FVA file is\n",
      "        given in command line.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "392T\n",
      "Print out network dictionary object to a tab separated table.\n",
      "\n",
      "Print information from the dictionary \"network_dict\". This information\n",
      "includes four items: reaction ID, reactant, product, and direction.\n",
      "this can table can be used as an input to other graph visualization\n",
      "and analysis software.\n",
      "\n",
      "\n",
      " Args:\n",
      "     network_dict: Dictionary object from make_network_dict()\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "393T\n",
      "Create a mapping from compound pair to a defaultdict containing\n",
      "lists of reactions for the forward, reverse, and both directions.\n",
      "\n",
      "Returns a dictionary that connects reactant/product pair to all reactions\n",
      "that contain this pair. Those reactions are stored in a dictionary and\n",
      "classified by reaction direction. For example:\n",
      "{(c1, c2): {'forward': [rxn1], 'back': [rxn3], 'both': [rxn4, rxn5]},\n",
      "(c3, c4): {...}, ...}\n",
      "\n",
      "Args:\n",
      "    filter_dict: A dictionary mapping reaction entry to\n",
      "        compound pairs (inside of the pairs there are cpd\n",
      "        objects, not cpd IDs)\n",
      "    args_method: a string, including 'fpp' and 'no-fpp'.\n",
      "    args_combine: combine level, default = 0, optional: 1 and 2.\n",
      "    style_flux_dict: a dictionary to set the edge style when fba or\n",
      "        fva input is given.\n",
      "    hide_edges: to determine edges between which compound pair need\n",
      "        to be hidden.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "394T\n",
      "Makes a bipartite graph object from a cpair_dict object.\n",
      "\n",
      "Start from empty graph() and cpair dict to make a graph object.\n",
      "Nodes only have rxn/cpd ID and rxn/cpd entry info in this initial\n",
      "graph. The information can be modified to add on other properties\n",
      "like color, or names.\n",
      "\n",
      "Args:\n",
      "    cpairs_dict: defaultdict of compound_pair:\n",
      "        defaultdict of direction: reaction list.\n",
      "        e.g. {(c1, c2): {'forward\":[rx1],\n",
      "        'both':[rx2}}.\n",
      "    new_id_mapping: dictionary of rxn_id_suffix: rxn_id.\n",
      "    method: options=['fpp', 'no-fpp', file_path].\n",
      "    args_combine: Command line argument, could be 0, 1, 2.\n",
      "    model_compound_entries: dict of cpd_id:compound_entry.\n",
      "    new_style_flux_dict: a dictionary to determine the edge style with\n",
      "        the new reaction IDs.\n",
      "return: A graph object that contains basic nodes and edges.\n",
      "    only ID and rxn/cpd entry are in node properties,\n",
      "    no features like color, shape.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "395T\n",
      "Search a set of compounds, then print detailed properties.\n",
      "\n",
      "Args:\n",
      "    id: a list of compound ids\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "396T\n",
      "Search a set of reactions, print detailed properties, then return a\n",
      "generator. Each item in the generator is a list of compounds in the\n",
      "corresponding reaction.\n",
      "\n",
      "Args:\n",
      "    ids: a list of reaction ids\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "397T\n",
      "Make a flux balance problem with TMFA related variables\n",
      "\n",
      "The TMFA problem contains 4 classes of variables. Flux variables\n",
      "defined in the v namespace, binary reaction indicator variables\n",
      "defined in the zi namespace, Gibbs free energy variables defined\n",
      "in the dgri namespace, and concentration variables defined in the\n",
      "xij namespace. This function will add these variables with their\n",
      "default lower and upper bounds. The function will then return\n",
      "the flux problem, variable namespaces and a list of the compounds.\n",
      "\n",
      "Args:\n",
      "    mm_irreversible: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "    solver: linear programming library to use.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "398T\n",
      "Gets upper or lower bound of a variable in an LP problem.\n",
      "\n",
      "Args:\n",
      "    prob: :class:`psamm.lpsolver.lp.Problem`.\n",
      "    var: LP problem variable\n",
      "    objective_sense: :class:`psamm.lpsolver.lp.ObjectiveSense`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "399T\n",
      "Prints FVA like result from TMFA problem.\n",
      "\n",
      "This function will take a TMFA problem along with the associated\n",
      "variable namespaces and print out all of the upper and lower bounds\n",
      "of the variables.\n",
      "\n",
      "Args:\n",
      "    prob: :class:`psamm.lpsolver.lp.Problem`.\n",
      "    mm_irreversible: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "    cp_list: List of compounds in the metabolic model.\n",
      "    exclude_unknown_list: List of reactions excluded from thermodynamic\n",
      "        constraints.\n",
      "    _v: variable namespace for flux variables.\n",
      "    _dgri: variable namespace for gibbs free energy variables\n",
      "    _zi: variables namespace for indicator variables\n",
      "    _xij: variable namespace for concentrations\n",
      "    excluded_compounds: list of compounds that are not constrained\n",
      "    split_reversible_list: list of all split reactions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "400T\n",
      "Prints FBA like result from TMFA problem.\n",
      "\n",
      "This function will take a TMFA problem along with the associated\n",
      "variable namespaces and print out a single solution for the\n",
      "associated problem. The solution can be either a single FBA-like\n",
      "solution, a L1min solution, or a random solution from the\n",
      "boundary of the solution space.\n",
      "\n",
      "Args:\n",
      "    simulation: type of solving for the problem. ['fba', 'l1min', 'random']\n",
      "    prob: :class:`psamm.lpsolver.lp.Problem`.\n",
      "    objective: Objective reaction ID.\n",
      "    mm_irreversible: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "    cp_list: List of compounds in the metabolic model.\n",
      "    exclude_unknown_list: List of reactions excluded\n",
      "        from thermo constraints.\n",
      "    _v: variable namespace for flux variables.\n",
      "    _dgri: variable namespace for gibbs free energy variables\n",
      "    _zi: variables namespace for indicator variables\n",
      "    _xij: variable namespace for concentrations\n",
      "    excluded_compounds: list of compounds that are not constrained\n",
      "    split_reversible_list: list of all split reactions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "401T\n",
      "Parses a supplied file and returns\n",
      "dictionaries containing lump information.\n",
      "\n",
      "The supplied file should be in a tab separated table in the format of\n",
      "lumpID  lump_deltaG rxn1:1,rxn2:-1,rxn3:1  lumpRXN\n",
      "Returns dictionaries storing this information, linking reactions to the\n",
      "lumps.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "402T\n",
      "Parse a transport parameter file.\n",
      "\n",
      "This file contains reaction IDs, net charge transported into\n",
      "the cell, and net protons transported into the cell.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "403T\n",
      "A function that will parse a supplied deltaG of formation file.\n",
      "\n",
      "Compound IDs in this file do not need to contain the compartments.\n",
      "compound deltaGf values should be in Kcal/mol\n",
      "\n",
      "Args:\n",
      "    mm: a metabolic model object\n",
      "    dgf_file: a file that containing 2 columns of\n",
      "        compound ids and deltaGf values\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "404T\n",
      "Add concentration constraints to TMFA problem\n",
      "based on parsed concentration dictionary.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "405T\n",
      "Parses DeltaG of reaction file and returns a dictionary of the values.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "406T\n",
      "Calculates DeltaG values from DeltaG of formation values of compounds.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "407T\n",
      "Adds reaction constraints to a TMFA problem\n",
      "\n",
      "This function will add in gibbs free energy constraints to a TMFA flux\n",
      "problem. These constraints will be added based on provided Gibbs free\n",
      "energy values and temperature.\n",
      "\n",
      "Args:\n",
      "    problem: :class:`psamm.lpsolver.lp.Problem`.\n",
      "    mm: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "    _v: variable namespace for flux variables.\n",
      "    _dgri: variable namespace for gibbs free energy variables\n",
      "    _zi: variables namespace for indicator variables\n",
      "    _xij: variable namespace for concentrations\n",
      "    exclude_unknown: List of reactions excluded from thermo constraints.\n",
      "    exclude_lumps_unknown: List of excluded reactions and lumped reactions.\n",
      "    dgr_dict: dictionary where keys are reaction ids and values\n",
      "        are deltag values.\n",
      "    lump_rxn_list: List of lump reaction IDs.\n",
      "    split_rxns: List of tuples of (reaction_forward, reaction_reverse)\n",
      "    transport_parameters: dictionary of reaction IDs to proton\n",
      "        transport and charge transport values.\n",
      "    testing_list: List of reactions to add deltaG constraints for.\n",
      "    water: list of water compound IDs.\n",
      "    hin: ID of proton compound from inside cell compartment\n",
      "    hout: ID of proton compound from outside cell compartment\n",
      "    hother: List of other proton IDs\n",
      "    ph: list of two tuples containing pH bounds. [(in_lower, in_upper),\n",
      "        (out_lower, out_upper)]\n",
      "    temp: Temperature in Celsius\n",
      "    err_est: True or False for using error estimates for deltaG values.\n",
      "    hamilton: True or False for using Hamilton TMFA method.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "408T\n",
      "Create a collection of reaction IDs that need to be visualized.\n",
      "\n",
      "Generate a set that contains IDs of all reactions that need to be\n",
      "visualized. This set can be generated from a file containing\n",
      "reaction or compound IDs.\n",
      "\n",
      "Args:\n",
      "    mm: Metabolic model, class 'psamm.metabolicmodel.MetabolicModel'.\n",
      "    subset_file: None or an open file containing a list of reactions\n",
      "                 or compound ids.\n",
      "    exclude: rxns to exclude\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "409T\n",
      "Adds biomass reaction nodes and edges to a graph object.\n",
      "\n",
      "This function is used to add nodes and edges of biomass reaction to\n",
      "the graph object. All the following properties are defined\n",
      "for added nodes: shape, style(filled), fill color, label.\n",
      "\n",
      "Args:\n",
      "    g: Graph object.\n",
      "    nm_bio_reaction: Biomass reaction DictReactionEntry.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "410T\n",
      "Update node color in Graph object based on a mapping dictionary\n",
      "\n",
      "This function adds shape and style(filled) properties to nodes in a graph.\n",
      "\n",
      "Args:\n",
      "    g: A Graph object that contains nodes and edges.\n",
      "    recolor_dict: dict of rxn_id/cpd_id[compartment] : hex color code.\n",
      "return: a graph object that contains a set of node with defined color.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "411T\n",
      "Set label of nodes in graph object,\n",
      "\n",
      "Set the label of nodes in a graph object based on compound/reaction\n",
      "properties provided through the cpd_detail or rxn_detail arguments.\n",
      "\n",
      "Args:\n",
      "    g: A graph object, contain a set of nodes and a dictionary of edges.\n",
      "    cpd_detail: A list that contains only one\n",
      "        element, this element is a compound properties name list,\n",
      "        e.g. detail = [['id', 'name', 'formula']].\n",
      "    rxn_detail: A list that contains only one\n",
      "        element, this element is a reaction properties name list,\n",
      "        e.g. detail = [['id', genes', 'equation']].\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "412T\n",
      "This function will create a tree-like dictionary that can be used to\n",
      "determine compartment location.\n",
      "\n",
      "    This function will take a list of compartment boundary tuples\n",
      "    (eg: [(c, e), (c, p)]) and use this data to construct a tree like\n",
      "    dictionary of parent compartments to lists of compartments they are\n",
      "    adjacent to. An extracellular compartment will also be set to use as\n",
      "    a starting point for the 'outermost' compartment in the model. If\n",
      "    none is explicitly defined then 'e' will be used by default.\n",
      "    If an 'e' compartment is not in the model either then a random\n",
      "    compartment will be used as a starting point.\n",
      "\n",
      "    args:\n",
      "    boundaries: a list of tuples of adjacent compartment ids.\n",
      "    extracellular: the extracellular compartment in the model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "413T\n",
      "This function will determine the compartment boundaries in a model\n",
      "\n",
      "This function will take a native model object and determine the\n",
      "compartment boundaries either based on the predefined compartments in\n",
      "the model.yaml or based on the reaction equations in the model.\n",
      "\n",
      "args:\n",
      "model: Native model, class 'psamm.datasource.native.NativeModel'.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "414T\n",
      "Add exchange reaction nodes and edges to graph object.\n",
      "\n",
      "This function is used to add nodes and edges of exchange reactions to\n",
      "the graph object. It will return an updated graph object that contains\n",
      "nodes representing exchange reactions.\n",
      "\n",
      "Args:\n",
      "    g: A graph object that contains a set of nodes and some edges.\n",
      "    rxn_id: Exchange reaction id,\n",
      "    reaction: Exchange reaction object(metabolic model reaction),\n",
      "        class 'psamm.reaction.Reaction'.\n",
      "    style_flux_dict: dictionary of reaction ID maps to edge style and\n",
      "        edge width.\n",
      "    analysis: \"None\" type or a string indicates if FBA or FVA file is\n",
      "        given in command line.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "415T\n",
      "This function will read in a gene mapping file and produce a\n",
      "gene to gene dictionary.\n",
      "\n",
      "The input is the app_file argument that the user enters, the query genome,\n",
      "which is the number of the genome that the user wishes to make a model of,\n",
      "and the number of the template genome which the user wishes to use to\n",
      "create the new model from.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "416T\n",
      "This function will translate genes in one model\n",
      "based on a gene to gene mapping dictionary.\n",
      "\n",
      "The function takes a native model object, a true or false\n",
      "ignore_na argument, and a gene mapping dictionary translation_dict\n",
      "that is made by the app_reader function. The ignore_na argument\n",
      "will determine if reactions with no genes will be kept in the final\n",
      "translated model or not. This function will then translate the gene\n",
      "associations based on the gene mapping dictionary and produce a\n",
      "new set of gene associations based on the mapped associations. The\n",
      "gene expressions are evaluated based on the logic to see if they should\n",
      "be included in the new model.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "419T\n",
      "Formulates and Solves a GIMME model.\n",
      "\n",
      "Implementation of the GIMME algorithm (Becker and Pallson 2008).\n",
      "Accepts an irreversible metabolic model, LP problem, and GIMME specific\n",
      "data. Will add in relevant GIMME constraints to the LP problem and\n",
      "generates a contextualized model based on the media constraints and the\n",
      "transcriptome data provided.\n",
      "\n",
      "Args:\n",
      "    problem: :class:`FluxBalanceProblem` to solve.\n",
      "    mm: An irreversible metabolic model.\n",
      "    biomass: Biomass reaction ID.\n",
      "    reversible_gene_assoc: A dictionary of gene IDs from make_irreversible.\n",
      "    split_rxns: A set of tuples of reaction IDs from make_irreversible.\n",
      "    transcript_values: A dictionary returned from parse_transcriptome_file.\n",
      "    threshold: A threshold that the biomass flux needs to stay above.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "420T\n",
      "Parses a file containing a gene to expression mapping.\n",
      "\n",
      "Parses a tab separated two column table. The first column contains gene\n",
      "IDs while the second column contains expression values. Will compare the\n",
      "expression values to the given threshold and return a dict with any\n",
      "genes that fall under the expression threshold, and the amount that they\n",
      "fall under the threshold.\n",
      "\n",
      "Args:\n",
      "    f: a file containing a two column gene to expression table.\n",
      "    threshold: Expression threshold value.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "421T\n",
      "Gets overall expression value for a reaction gene association.\n",
      "\n",
      "Recursive function designed to parse a gene expression and return\n",
      "a penalty value to use in the GIMME algorithm. This function is\n",
      "designed to return the value directly if the expression only has\n",
      "one gene. If the expression has multiple genes related by 'OR'\n",
      "associations, then it will return the highest lowest penalty value.\n",
      "If the genes are associated with 'AND' logic then the function\n",
      "will return the highest penalty value of the set of values.\n",
      "\n",
      "Args:\n",
      "    root: object of boolean.Expression()._root\n",
      "    gene_dict: dict of gene expression from parse_transcriptome_file\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "422T\n",
      "Return default element weight.\n",
      "\n",
      "This is the default element weight function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "423T\n",
      "Calculate generalized Jaccard similarity of formulas.\n",
      "\n",
      "Returns the weighted similarity value or None if there is no overlap\n",
      "at all. If the union of the formulas has a weight of zero (i.e. the\n",
      "denominator in the Jaccard similarity is zero), a value of zero is\n",
      "returned.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "424T\n",
      "Convert a reaction to reduced left, right dictionaries.\n",
      "\n",
      "Returns a pair of (left, right) dictionaries mapping compounds to\n",
      "normalized integer stoichiometric values. If a compound occurs multiple\n",
      "times on one side, the occurences are combined into a single entry in the\n",
      "dictionary.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "425T\n",
      "Predict reaction pairs using iterated method.\n",
      "\n",
      "Returns a tuple containing a dictionary of predictions keyed by the\n",
      "reaction IDs, and the final number of iterations. Each reaction prediction\n",
      "entry contains a tuple with a dictionary of transfers and a dictionary of\n",
      "unbalanced compounds. The dictionary of unbalanced compounds is empty only\n",
      "if the reaction is balanced.\n",
      "\n",
      "Args:\n",
      "    reactions: Dictionary or pair-iterable of (id, equation) pairs.\n",
      "        IDs must be any hashable reaction identifier (e.g. string) and\n",
      "        equation must be :class:`psamm.reaction.Reaction` objects.\n",
      "    ambiguous: True or False value to indicate if the ambiguous\n",
      "        reactions should be printed in the debugging output.\n",
      "    formulas: Dictionary mapping compound IDs to\n",
      "        :class:`psamm.formula.Formula`. Formulas must be flattened.\n",
      "    prior: Tuple of (alpha, beta) parameters for the MAP inference.\n",
      "        If not provided, the default parameters will be used: (1, 43).\n",
      "    max_iterations: Maximum iterations to run before stopping. If the\n",
      "        stopping condition is reached before this number of iterations,\n",
      "        the procedure also stops. If None, the procedure only stops when\n",
      "        the stopping condition is reached.\n",
      "    element_weight: A function providing returning weight value for the\n",
      "        given :class:`psamm.formula.Atom` or\n",
      "        :class:`psamm.formula.Radical`. If not provided, the default weight\n",
      "        will be used (H=0, C=1, *=0.82)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "426T\n",
      "Match compounds greedily based on score function.\n",
      "\n",
      "Args:\n",
      "    reaction: Reaction equation :class:`psamm.reaction.Reaction`.\n",
      "    compound_formula: Dictionary mapping compound IDs to\n",
      "        :class:`psamm.formula.Formula`. Formulas must be flattened.\n",
      "    score_func: Function that takes two :class:`_CompoundInstance` and\n",
      "        returns the score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "427T\n",
      "Predict compound pairs for a single reaction.\n",
      "\n",
      "Performs greedy matching on reaction compounds using a scoring function\n",
      "that uses generalized Jaccard similarity corrected by the weights in the\n",
      "given dictionary. Returns a tuple of a transfer dictionary and a dictionary\n",
      "of unbalanced compounds. The dictionary of unbalanced compounds is empty\n",
      "only if the reaction is balanced.\n",
      "\n",
      "Args:\n",
      "    reaction: :class:`psamm.reaction.Reaction`.\n",
      "    compound_formula: Dictionary mapping compound IDs to\n",
      "        :class:`psamm.formula.Formula`. Formulas must be flattened.\n",
      "    pair_weights: Dictionary mapping pairs of compound IDs to correction\n",
      "        values. This value is multiplied by the calculated Jaccard\n",
      "        similarity. If a pair is not in the dictionary, the value 1 is\n",
      "        used. Pairs are looked up in the weights dictionary as a tuple of\n",
      "        compound names (``c1``, ``c2``) where ``c1`` is the left-hand side\n",
      "        and ``c2`` is the right-hand side.\n",
      "    weight_func: Weight function for caclulating the generalized Jaccard\n",
      "        similarity. This function will be given an\n",
      "        :class:`psamm.formula.Atom` or :class:`psamm.formula.Radical` and\n",
      "        should return a corresponding weight.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "429T\n",
      "Find the variability of each reaction while fixing certain fluxes.\n",
      "\n",
      "Yields the reaction id, and a tuple of minimum and maximum value for each\n",
      "of the given reactions. The fixed reactions are given in a dictionary as\n",
      "a reaction id to value mapping.\n",
      "\n",
      "This is an implementation of flux variability analysis (FVA) as described\n",
      "in [Mahadevan03]_.\n",
      "\n",
      "Args:\n",
      "    model: MetabolicModel to solve.\n",
      "    reactions: Reactions on which to report variablity.\n",
      "    fixed: dict of additional lower bounds on reaction fluxes.\n",
      "    tfba: If True enable thermodynamic constraints.\n",
      "    solver: LP solver instance to use.\n",
      "\n",
      "Returns:\n",
      "    Iterator over pairs of reaction ID and bounds. Bounds are returned as\n",
      "    pairs of lower and upper values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "430T\n",
      "Minimize flux of all reactions while keeping certain fluxes fixed.\n",
      "\n",
      "The fixed reactions are given in a dictionary as reaction id\n",
      "to value mapping. The weighted L1-norm of the fluxes is minimized.\n",
      "\n",
      "Args:\n",
      "    model: MetabolicModel to solve.\n",
      "    fixed: dict of additional lower bounds on reaction fluxes.\n",
      "    solver: LP solver instance to use.\n",
      "    weights: dict of weights on the L1-norm terms.\n",
      "\n",
      "Returns:\n",
      "    An iterator of reaction ID and reaction flux pairs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "431T\n",
      "Find a random flux solution on the boundary of the solution space.\n",
      "\n",
      "The reactions in the threshold dictionary are constrained with the\n",
      "associated lower bound.\n",
      "\n",
      "Args:\n",
      "    model: MetabolicModel to solve.\n",
      "    threshold: dict of additional lower bounds on reaction fluxes.\n",
      "    tfba: If True enable thermodynamic constraints.\n",
      "    solver: LP solver instance to use.\n",
      "\n",
      "Returns:\n",
      "    An iterator of reaction ID and reaction flux pairs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "432T\n",
      "Check that reaction subset of model is consistent using FBA.\n",
      "\n",
      "Yields all reactions that are *not* flux consistent. A reaction is\n",
      "consistent if there is at least one flux solution to the model that both\n",
      "respects the model constraints and also allows the reaction in question to\n",
      "have non-zero flux.\n",
      "\n",
      "This can be determined by running FBA on each reaction in turn\n",
      "and checking whether the flux in the solution is non-zero. Since FBA\n",
      "only tries to maximize the flux (and the flux can be negative for\n",
      "reversible reactions), we have to try to both maximize and minimize\n",
      "the flux. An optimization to this method is implemented such that if\n",
      "checking one reaction results in flux in another unchecked reaction,\n",
      "that reaction will immediately be marked flux consistent.\n",
      "\n",
      "Args:\n",
      "    model: MetabolicModel to check for consistency.\n",
      "    subset: Subset of model reactions to check.\n",
      "    epsilon: The threshold at which the flux is considered non-zero.\n",
      "    tfba: If True enable thermodynamic constraints.\n",
      "    solver: LP solver instance to use.\n",
      "\n",
      "Returns:\n",
      "    An iterator of flux inconsistent reactions in the subset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "433T\n",
      "Return a trimmed docstring.\n",
      "\n",
      "Code taken from 'PEP 257 -- Docstring Conventions' article.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "434T\n",
      "Check whether the ID is valid.\n",
      "\n",
      "First check if the ID is missing, and then check if it is a qualified\n",
      "string type, finally check if the string is empty. For all checks, it\n",
      "would raise a ParseError with the corresponding message.\n",
      "\n",
      "Args:\n",
      "    entity: a string type object to be checked.\n",
      "    entity_type: a string that shows the type of entities to check, usually\n",
      "        `Compound` or 'Reaction'.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "435T\n",
      "Parse a structured compound definition as obtained from a YAML file\n",
      "\n",
      "Returns a CompoundEntry.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "436T\n",
      "Parse a structured list of compounds as obtained from a YAML file\n",
      "\n",
      "Yields CompoundEntries. Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "437T\n",
      "Parse a tab-separated file containing compound IDs and properties\n",
      "\n",
      "The compound properties are parsed according to the header which specifies\n",
      "which property is contained in each column.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "438T\n",
      "Parse a file as a YAML-format list of compounds\n",
      "\n",
      "Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "439T\n",
      "Looks at a file's extension and format (if any) and returns format.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "440T\n",
      "Open and parse reaction file based on file extension or given format\n",
      "\n",
      "Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "441T\n",
      "Parse a string representation of a reaction equation.\n",
      "\n",
      "Converts undefined compartments to the default compartment.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "442T\n",
      "Parse a structured reaction equation as obtained from a YAML file\n",
      "\n",
      "Returns a Reaction.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "443T\n",
      "Parse a structured reaction definition as obtained from a YAML file\n",
      "\n",
      "Returns a ReactionEntry.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "444T\n",
      "Parse a structured list of reactions as obtained from a YAML file\n",
      "\n",
      "Yields tuples of reaction ID and reaction object. Path can be given as a\n",
      "string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "446T\n",
      "Parse a tab-separated file containing reaction IDs and properties\n",
      "\n",
      "The reaction properties are parsed according to the header which specifies\n",
      "which property is contained in each column.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "447T\n",
      "Open and parse reaction file based on file extension\n",
      "\n",
      "Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "448T\n",
      "Parse a structured exchange definition as obtained from a YAML file.\n",
      "\n",
      "Returns in iterator of compound, reaction, lower and upper bounds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "449T\n",
      "Parse a structured exchange list as obtained from a YAML file.\n",
      "\n",
      "Yields tuples of compound, reaction ID, lower and upper flux bounds. Path\n",
      "can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "450T\n",
      "Parse a file as a YAML-format exchange definition.\n",
      "\n",
      "Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "451T\n",
      "Parse a space-separated file containing exchange compound flux limits.\n",
      "\n",
      "The first two columns contain compound IDs and compartment while the\n",
      "third column contains the lower flux limits. The fourth column is\n",
      "optional and contains the upper flux limit.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "452T\n",
      "Parse a file as a list of exchange compounds with flux limits.\n",
      "\n",
      "The file format is detected and the file is parsed accordingly. Path can\n",
      "be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "453T\n",
      "Parse a structured flux limit definition as obtained from a YAML file\n",
      "\n",
      "Returns a tuple of reaction, lower and upper bound.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "454T\n",
      "Parse a structured list of flux limits as obtained from a YAML file\n",
      "\n",
      "Yields tuples of reaction ID, lower and upper flux bounds. Path can be\n",
      "given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "455T\n",
      "Parse a space-separated file containing reaction flux limits\n",
      "\n",
      "The first column contains reaction IDs while the second column contains\n",
      "the lower flux limits. The third column is optional and contains the\n",
      "upper flux limit.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "456T\n",
      "Parse a file as a YAML-format flux limits definition\n",
      "\n",
      "Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "457T\n",
      "Parse a file as a list of reaction flux limits\n",
      "\n",
      "The file format is detected and the file is parsed accordingly. Path can\n",
      "be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "458T\n",
      "Parse a structured model group as obtained from a YAML file\n",
      "\n",
      "Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "459T\n",
      "Parse a structured list of model groups as obtained from a YAML file\n",
      "\n",
      "Yields reaction IDs. Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "460T\n",
      "Parse a file as a YAML-format list of model reaction groups\n",
      "\n",
      "Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "461T\n",
      "Parse a file as a list of model reactions\n",
      "\n",
      "Yields reactions IDs. Path can be given as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "462T\n",
      "Parse a file as a list of model reactions\n",
      "\n",
      "The file format is detected and the file is parsed accordinly. The file is\n",
      "specified as a file path that will be opened for reading. Path can be given\n",
      "as a string or a context.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "463T\n",
      "Generate a parsable reaction representation to the YAML parser.\n",
      "\n",
      "Check the number of compounds in the reaction, if it is larger than 10,\n",
      "then transform the reaction data into a list of directories with all\n",
      "attributes in the reaction; otherwise, just return the text_type format\n",
      "of the reaction data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "464T\n",
      "Return unique signature object for :class:`Reaction`.\n",
      "\n",
      "Signature objects are hashable, and compare equal only if the reactions\n",
      "are considered the same according to the specified rules.\n",
      "\n",
      "Args:\n",
      "    direction: Include reaction directionality when considering equality.\n",
      "    stoichiometry: Include stoichiometry when considering equality.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "465T\n",
      "Convert raw SBML model to extended model.\n",
      "\n",
      "Args:\n",
      "    model: :class:`NativeModel` obtained from :class:`SBMLReader`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "468T\n",
      "Yield key, value pairs parsed from the XHTML notes section.\n",
      "\n",
      "Each key, value pair must be defined in its own text block, e.g.\n",
      "``<p>key: value</p><p>key2: value2</p>``. The key and value must be\n",
      "separated by a colon. Whitespace is stripped from both key and value, and\n",
      "quotes are removed from values if present. The key is normalized by\n",
      "conversion to lower case and spaces replaced with underscores.\n",
      "\n",
      "Args:\n",
      "    entry: :class:`_SBMLEntry`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "469T\n",
      "Return species properties defined in the XHTML notes.\n",
      "\n",
      "Older SBML models often define additional properties in the XHTML notes\n",
      "section because structured methods for defining properties had not been\n",
      "developed. This will try to parse the following properties: ``PUBCHEM ID``,\n",
      "``CHEBI ID``, ``FORMULA``, ``KEGG ID``, ``CHARGE``.\n",
      "\n",
      "Args:\n",
      "    entry: :class:`SBMLSpeciesEntry`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "470T\n",
      "Return reaction properties defined in the XHTML notes.\n",
      "\n",
      "Older SBML models often define additional properties in the XHTML notes\n",
      "section because structured methods for defining properties had not been\n",
      "developed. This will try to parse the following properties: ``SUBSYSTEM``,\n",
      "``GENE ASSOCIATION``, ``EC NUMBER``, ``AUTHORS``, ``CONFIDENCE``.\n",
      "\n",
      "Args:\n",
      "    entry: :class:`SBMLReactionEntry`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "471T\n",
      "Return objective value for reaction entry.\n",
      "\n",
      "Detect objectives that are specified using the non-standardized\n",
      "kinetic law parameters which are used by many pre-FBC SBML models. The\n",
      "objective coefficient is returned for the given reaction, or None if\n",
      "undefined.\n",
      "\n",
      "Args:\n",
      "    entry: :class:`SBMLReactionEntry`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "472T\n",
      "Return flux bounds for reaction entry.\n",
      "\n",
      "Detect flux bounds that are specified using the non-standardized\n",
      "kinetic law parameters which are used by many pre-FBC SBML models. The\n",
      "flux bounds are returned as a pair of lower, upper bounds. The returned\n",
      "bound is None if undefined.\n",
      "\n",
      "Args:\n",
      "    entry: :class:`SBMLReactionEntry`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "474T\n",
      "Convert exchange reactions in model to exchange compounds.\n",
      "\n",
      "Only exchange reactions in the extracellular compartment are converted.\n",
      "The extracelluar compartment must be defined for the model.\n",
      "\n",
      "Args:\n",
      "    model: :class:`NativeModel`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "475T\n",
      "Merge equivalent compounds in various compartments.\n",
      "\n",
      "Tries to detect and merge compound entries that represent the same\n",
      "compound in different compartments. The entries are only merged if all\n",
      "properties are equivalent. Compound entries must have an ID with a suffix\n",
      "of an underscore followed by the compartment ID. This suffix will be\n",
      "stripped and compounds with identical IDs are merged if the properties\n",
      "are identical.\n",
      "\n",
      "Args:\n",
      "    model: :class:`NativeModel`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "476T\n",
      "Parse a compound specification.\n",
      "\n",
      "If no compartment is specified in the string, the global compartment\n",
      "will be used.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "477T\n",
      "Return a unique string ID from the prefix.\n",
      "\n",
      "First check if the prefix is itself a unique ID in the set-like parameter\n",
      "existing_ids. If not, try integers in ascending order appended to the\n",
      "prefix until a unique ID is found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "478T\n",
      "Try to describe the current commit of a Git repository.\n",
      "\n",
      "Return a string containing a string with the commit ID and/or a base tag,\n",
      "if successful. Otherwise, return None.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "479T\n",
      "Transform L1-norm optimization function into cardinality optimization.\n",
      "\n",
      "The given function must optimize a convex problem with\n",
      "a weighted L1-norm as the objective. The transformed function\n",
      "will apply the iterated weighted L1 heuristic to approximately\n",
      "optimize the cardinality of the solution. This method is\n",
      "described by S. Boyd, \"L1-norm norm methods for convex cardinality\n",
      "problems.\" Lecture Notes for EE364b, Stanford University, 2007.\n",
      "Available online at www.stanford.edu/class/ee364b/.\n",
      "\n",
      "The given function must take an optional keyword parameter weights\n",
      "(dictionary), and the weights must be set to one if not specified.\n",
      "The function must return the non-weighted solution as an iterator\n",
      "over (identifier, value)-tuples, either directly or as the first\n",
      "element of a tuple.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "480T\n",
      "Return True if the two gene association strings are considered equal.\n",
      "\n",
      "Args:\n",
      "    g1, g2: gene association strings\n",
      "    gene_map: a dict that maps gene ids in g1 to gene ids in g2 if\n",
      "              they have different naming system\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "481T\n",
      "Return what the default compartment should be set to.\n",
      "\n",
      "If some compounds have no compartment, unique compartment\n",
      "name is returned to avoid collisions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "482T\n",
      "Detect the best default flux limit to use for model output.\n",
      "\n",
      "The default flux limit does not change the model but selecting a good\n",
      "value reduced the amount of output produced and reduces clutter in the\n",
      "output files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "483T\n",
      "Turn the reaction subsystems into their own files.\n",
      "\n",
      "If a subsystem has a number of reactions over the threshold, it gets its\n",
      "own YAML file. All other reactions, those that don't have a subsystem or\n",
      "are in a subsystem that falls below the threshold, get added to a common\n",
      "reaction file.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm_import.model.MetabolicModel`.\n",
      "    dest: output path for model files.\n",
      "    writer: :class:`psamm.datasource.native.ModelWriter`.\n",
      "    split_subsystem: Divide reactions into multiple files by subsystem.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "484T\n",
      "Yield key, value pairs for limits dictionary.\n",
      "\n",
      "Yield pairs of key, value where key is ``lower``, ``upper`` or ``fixed``.\n",
      "A key, value pair is emitted if the bounds are not None.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "485T\n",
      "Write the given NativeModel to YAML files in dest folder.\n",
      "\n",
      "The parameter ``convert_exchange`` indicates whether the exchange reactions\n",
      "should be converted automatically to an exchange file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "486T\n",
      "Entry point for BiGG import program.\n",
      "\n",
      "If the ``args`` are provided, these should be a list of strings that will\n",
      "be used instead of ``sys.argv[1:]``. This is mostly useful for testing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "487T\n",
      "Check consistency of model reactions.\n",
      "\n",
      "Yield all reactions in the model that are not part of the consistent\n",
      "subset.\n",
      "\n",
      "Args:\n",
      "    model: :class:`MetabolicModel` to solve.\n",
      "    epsilon: Flux threshold value.\n",
      "    solver: LP solver instance to use.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "488T\n",
      "Quickly check whether model is consistent\n",
      "\n",
      "Return true if the model is consistent. If it is only necessary to know\n",
      "whether a model is consistent, this function is fast as it will return\n",
      "the result as soon as it finds a single inconsistent reaction.\n",
      "\n",
      "Args:\n",
      "    model: :class:`MetabolicModel` to solve.\n",
      "    epsilon: Flux threshold value.\n",
      "    solver: LP solver instance to use.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "489T\n",
      "Return consistent subset of model.\n",
      "\n",
      "The largest consistent subset is returned as\n",
      "a set of reaction names.\n",
      "\n",
      "Args:\n",
      "    model: :class:`MetabolicModel` to solve.\n",
      "    epsilon: Flux threshold value.\n",
      "    solver: LP solver instance to use.\n",
      "\n",
      "Returns:\n",
      "    Set of reaction IDs in the consistent reaction subset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "490T\n",
      "Find a flux consistent subnetwork containing the core subset.\n",
      "\n",
      "The result will contain the core subset and as few of the additional\n",
      "reactions as possible.\n",
      "\n",
      "Args:\n",
      "    model: :class:`MetabolicModel` to solve.\n",
      "    core: Set of core reaction IDs.\n",
      "    epsilon: Flux threshold value.\n",
      "    solver: LP solver instance to use.\n",
      "    scaling: Scaling value to apply (see [Vlassis14]_ for more\n",
      "        information on this parameter).\n",
      "    weights: Dictionary with reaction IDs as keys and values as weights.\n",
      "        Weights specify the cost of adding a reaction to the consistent\n",
      "        subnetwork. Default value is 1.\n",
      "\n",
      "Returns:\n",
      "    Set of reaction IDs in the consistent reaction subset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "491T\n",
      "Identify compounds in the model that cannot be produced.\n",
      "\n",
      "Yields all compounds that cannot be produced. This method\n",
      "assumes implicit sinks for all compounds in the model so\n",
      "the only factor that influences whether a compound can be\n",
      "produced is the presence of the compounds needed to produce it.\n",
      "\n",
      "Epsilon indicates the threshold amount of reaction flux for the products\n",
      "to be considered non-blocked. V_max indicates the maximum flux.\n",
      "\n",
      "This method is implemented as a MILP-program. Therefore it may\n",
      "not be efficient for larger models.\n",
      "\n",
      "Args:\n",
      "    model: :class:`MetabolicModel` containing core reactions and reactions\n",
      "        that can be added for gap-filling.\n",
      "    solver: MILP solver instance.\n",
      "    epsilon: Threshold amount of a compound produced for it to not be\n",
      "        considered blocked.\n",
      "    v_max: Maximum flux.\n",
      "    implicit_sinks: Whether implicit sinks for all compounds are included\n",
      "        when gap-filling (traditional GapFill uses implicit sinks).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "492T\n",
      "Find a set of reactions to add such that no compounds are blocked.\n",
      "\n",
      "Returns two iterators: first an iterator of reactions not in\n",
      "core, that were added to resolve the model. Second, an\n",
      "iterator of reactions in core that had flux bounds expanded (i.e.\n",
      "irreversible reactions become reversible). Similarly to\n",
      "GapFind, this method assumes, by default, implicit sinks for all compounds\n",
      "in the model so the only factor that influences whether a compound\n",
      "can be produced is the presence of the compounds needed to produce\n",
      "it. This means that the resulting model will not necessarily be\n",
      "flux consistent.\n",
      "\n",
      "This method is implemented as a MILP-program. Therefore it may\n",
      "not be efficient for larger models.\n",
      "\n",
      "Args:\n",
      "    model: :class:`MetabolicModel` containing core reactions and reactions\n",
      "        that can be added for gap-filling.\n",
      "    core: The set of core (already present) reactions in the model.\n",
      "    blocked: The compounds to unblock.\n",
      "    exclude: Set of reactions in core to be excluded from gap-filling (e.g.\n",
      "        biomass reaction).\n",
      "    solver: MILP solver instance.\n",
      "    epsilon: Threshold amount of a compound produced for it to not be\n",
      "        considered blocked.\n",
      "    v_max: Maximum flux.\n",
      "    weights: Dictionary of weights for reactions. Weight is the penalty\n",
      "        score for adding the reaction (non-core reactions) or expanding the\n",
      "        flux bounds (all reactions).\n",
      "    implicit_sinks: Whether implicit sinks for all compounds are included\n",
      "        when gap-filling (traditional GapFill uses implicit sinks).\n",
      "    allow_bounds_expansion: Allow flux bounds to be expanded at the cost\n",
      "        of a penalty which can be specified using weights (traditional\n",
      "        GapFill does not allow this). This includes turning irreversible\n",
      "        reactions reversible.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "493T\n",
      "Create gene association for class :class:`.GeneDeletionStrategy`.\n",
      "\n",
      "Return a dict mapping reaction IDs to\n",
      ":class:`psamm.expression.boolean.Expression` objects,\n",
      "representing relationships between reactions and related genes. This helper\n",
      "function should be called when creating :class:`.GeneDeletionStrategy`\n",
      "objects.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm.datasource.native.NativeModel`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "494T\n",
      "Yield IDs of all exchange reactions from model.\n",
      "\n",
      "This helper function would be useful when creating\n",
      ":class:`.ReactionDeletionStrategy` objects.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "495T\n",
      "Find a random minimal network of model reactions.\n",
      "\n",
      "Given a reaction to optimize and a threshold, delete entities randomly\n",
      "until the flux of the reaction to optimize falls under the threshold.\n",
      "Keep deleting until no more entities can be deleted. It works\n",
      "with two strategies: deleting reactions or deleting genes (reactions\n",
      "related to certain genes).\n",
      "\n",
      "Args:\n",
      "    strategy: :class:`.ReactionDeletionStrategy` or\n",
      "        :class:`.GeneDeletionStrategy`.\n",
      "    prob: :class:`psamm.fluxanalysis.FluxBalanceProblem`.\n",
      "    obj_reaction: objective reactions to optimize.\n",
      "    flux_threshold: threshold of max reaction flux.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "496T\n",
      "Find a random minimal network of model reactions.\n",
      "\n",
      "Given a reaction to optimize and a threshold, delete entities randomly\n",
      "until the flux of the reaction to optimize falls under the threshold.\n",
      "Keep deleting until no more entities can be deleted. It works\n",
      "with two strategies: deleting reactions or deleting genes (reactions\n",
      "related to certain genes).\n",
      "\n",
      "Args:\n",
      "    strategy: :class:`.ReactionDeletionStrategy` or\n",
      "        :class:`.GeneDeletionStrategy`.\n",
      "    prob: :class:`psamm.fluxanalysis.FluxBalanceProblem`.\n",
      "    obj_reaction: objective reactions to optimize.\n",
      "    flux_threshold: threshold of max reaction flux.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "497T\n",
      "function to parse orthology tables. The default format for these\n",
      "tables is the default eggnog output, but custom colummn numers can be\n",
      "passed through the --col argument\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "498T\n",
      "Function to sort the downloaded kegg object into a format\n",
      "that is compatible with the psamm api for storage in\n",
      "a reactions.yaml file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "499T\n",
      "This function handles specific edge cases in the kegg\n",
      "format of reaction equations that are incompatible with\n",
      "the psamm format. Takes the downloaded dictionary of reactions\n",
      "and returns the same dictionary with modified equations,\n",
      "if necessary.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "500T\n",
      "This function creates a draft model based on the reactions:genes\n",
      "file specified in the rxn variable. This function generates\n",
      "reaction and compound information by utilizing the kegg\n",
      "REST api to download the reaction information and uses psamm\n",
      "functions to parse out the kegg data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "501T\n",
      "Function to sort the downloaded     kegg object into a format\n",
      "that is     compatible with the psamm api for storage in\n",
      "a compounds.yaml file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "502T\n",
      "Function for checking if the compound formulation is\n",
      "compatible with psamm. generalized rules for this are\n",
      "that compounds must have a formula, the formula cannot\n",
      "be variable (e.g. presence of X), and R groups are\n",
      "generally discouraged.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "503T\n",
      "Function to sort the downloaded     kegg object into a format\n",
      "that is     compatible with the psamm api for storage in\n",
      "a generic_compounds.yaml file. This function contains\n",
      "special error handling for improperly formatted compounds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "504T\n",
      "Downloads the kegg entry associated with a reaction or\n",
      "a compound and stores each line in an object that can\n",
      "be parsed as a reaction or a compound, depending on the\n",
      "input\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "505T\n",
      "Functions converts gene associations to EC into gene\n",
      "associations for reaction IDs. Returns a dictionary\n",
      "of Reaction IDs to genes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "506T\n",
      "Functions converts gene associations to EC into gene\n",
      "associations for reaction IDs. Returns a dictionary\n",
      "of Reaction IDs to genes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "507T\n",
      "Try to assign a positive mass to each compound\n",
      "\n",
      "Return True if successful. The masses are simply constrained by m_i > 1 and\n",
      "finding a solution under these conditions proves that the database is mass\n",
      "consistent.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "508T\n",
      "Check inconsistent reactions by minimizing mass residuals\n",
      "\n",
      "Return a reaction iterable, and compound iterable. The reaction iterable\n",
      "yields reaction ids and mass residuals. The compound iterable yields\n",
      "compound ids and mass assignments.\n",
      "\n",
      "Each compound is assigned a mass of at least one, and the masses are\n",
      "balanced using the stoichiometric matrix. In addition, each reaction has a\n",
      "residual mass that is included in the mass balance equations. The L1-norm\n",
      "of the residuals is minimized. Reactions in the checked set are assumed to\n",
      "have been manually checked and therefore have the residual fixed at zero.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "509T\n",
      "Yield each compound in the database with assigned mass\n",
      "\n",
      "Each compound will be assigned a mass and the number of compounds having a\n",
      "positive mass will be approximately maximized.\n",
      "\n",
      "This is an implementation of the solution originally proposed by\n",
      "[Gevorgyan08]_  but using the new method proposed by [Thiele14]_ to avoid\n",
      "MILP constraints. This is similar to the way Fastcore avoids MILP\n",
      "contraints.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "510T\n",
      "Return weight of formula element.\n",
      "\n",
      "This implements the default weight proposed for MapMaker.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "511T\n",
      "Predict compound pairs of reaction using MapMaker.\n",
      "\n",
      "Yields all solutions as dictionaries with compound pairs as keys and\n",
      "formula objects as values.\n",
      "\n",
      "Args:\n",
      "    reaction: :class:`psamm.reaction.Reaction` object.\n",
      "    compound_formula: Dictionary mapping compound IDs to formulas. Formulas\n",
      "        must be flattened.\n",
      "    solver: LP solver (MILP).\n",
      "    epsilon: Threshold for rounding floating-point values to integers in\n",
      "        the predicted transfers.\n",
      "    alt_elements: Iterable of elements to consider for alternative\n",
      "          solutions. Only alternate solutions that have different transfers\n",
      "          of these elements will be returned (default=all elements).\n",
      "    weight_func: Optional function that returns a weight for a formula\n",
      "        element (should handle specific Atom and Radical objects). By\n",
      "        default, the standard MapMaker weights will be used\n",
      "        (H=0, R=40, N=0.4, O=0.4, P=0.4, *=1).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "512T\n",
      "Return a constant indicating the type of coupling.\n",
      "\n",
      "Depending on the type of coupling, one of the constants from\n",
      ":class:`.CouplingClass` is returned.\n",
      "\n",
      "Args:\n",
      "    coupling: Tuple of minimum and maximum flux ratio\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "513T\n",
      "Calculate the overall charge for the specified reaction.\n",
      "\n",
      "Args:\n",
      "    reaction: :class:`psamm.reaction.Reaction`.\n",
      "    compound_charge: a map from each compound to charge values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "514T\n",
      "Calculate the overall charge for all reactions in the model.\n",
      "\n",
      "Yield (reaction, charge) pairs.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm.datasource.native.NativeModel`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "515T\n",
      "Calculate formula compositions for both sides of the specified reaction.\n",
      "\n",
      "If the compounds in the reaction all have formula, then calculate and\n",
      "return the chemical compositions for both sides, otherwise return `None`.\n",
      "\n",
      "Args:\n",
      "    reaction: :class:`psamm.reaction.Reaction`.\n",
      "    compound_formula: a map from compound id to formula.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "516T\n",
      "Calculate formula compositions for each reaction.\n",
      "\n",
      "Call :func:`reaction_formula` for each reaction.\n",
      "Yield (reaction, result) pairs, where result has two formula compositions\n",
      "or `None`.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm.datasource.native.NativeModel`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "517T\n",
      "Add all reactions from database that occur in given compartments.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "518T\n",
      "Add all exchange reactions to database and to model.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "519T\n",
      "Add all transport reactions to database and to model.\n",
      "\n",
      "Add transport reactions for all boundaries. Boundaries are defined\n",
      "by pairs (2-tuples) of compartment IDs. Transport reactions are\n",
      "added for all compounds in the model, not just for compounds in the\n",
      "two boundary compartments.\n",
      "\n",
      "Args:\n",
      "    model: :class:`psamm.metabolicmodel.MetabolicModel`.\n",
      "    boundaries: Set of compartment boundary pairs.\n",
      "\n",
      "Returns:\n",
      "    Set of IDs of reactions that were added.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "521T\n",
      "Return a file object from an input file.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "522T\n",
      "Return sequence lines in FASTQ.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "523T\n",
      "Return quality score lines in FASTQ.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "524T\n",
      "Return sets of read records in FASTQ.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "525T\n",
      "Return an overlapping position between a pair of k-mers.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "526T\n",
      "Return a clean set of k-mers in tuple.\n",
      "\n",
      "Filter low-complexity and low-frequency kmers.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "527T\n",
      "Return assembled k-mers and the frequency in tuple.\n",
      "\n",
      "Assemble given k-mers by checking suffix-prefix matches.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "528T\n",
      "Return sorted k-mer frequency.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "529T\n",
      "Return a list of predicted adapters.\n",
      "\n",
      "Predict 3' adapter sequence with a combination of k and R.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "530T\n",
      "Return a list of predicted adapters.\n",
      "\n",
      "Iteratively predict 3' adapter sequence with different\n",
      "combinations of k and R.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "531T\n",
      "Remove temporary directory.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "532T\n",
      "Return adapter-clipped clean reads.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "533T\n",
      "Write FASTA containing clean reads, and return\n",
      "the number of the reads.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "534T\n",
      "Write FASTQ in the temporary directory, and retrun\n",
      "(subsampled) FASTQ name, the total read count,\n",
      "standard deviation of read lengths.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "535T\n",
      "Return the number of mapped reads to the genome.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "538T\n",
      "Return range of kmers or filtering ratios.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "539T\n",
      "Return options and required arguments.\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "540T\n",
      "Prepare fasta files for exonerate.\n",
      "\n",
      "In this iteration we are getting the contigs from the given fasta files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "541T\n",
      "Create the metadata table.\n",
      "\n",
      "Information used to tell how aTRAM was set up.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "542T\n",
      "Create the sequences index after we build the table.\n",
      "\n",
      "This speeds up the program significantly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "543T\n",
      "Assign processes to make the blast DBs.\n",
      "\n",
      "One process for each blast DB shard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "544T\n",
      "Assign processes to make the blast DBs.\n",
      "\n",
      "One process for each blast DB shard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "545T\n",
      "Create a blast DB from the shard.\n",
      "\n",
      "We fill a fasta file with the appropriate sequences and hand things off\n",
      "to the makeblastdb program.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "546T\n",
      "Fill the fasta file used as input into blast.\n",
      "\n",
      "Use sequences from the sqlite3 DB. We use the shard partitions passed in to\n",
      "determine which sequences to get for this shard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "547T\n",
      "Convert a number in a word.\n",
      "\n",
      "If this gets complex we will add the inflect module instead.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "548T\n",
      "Kill a process tree (including grandchildren etc.) with signal \"sig\".\n",
      "\n",
      "Return a (killed, alive) tuple. \"on_terminate\", if specified, is a callback\n",
      "function which is called as soon as a child terminates.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "549T\n",
      "Blast the query sequence against the contigs.\n",
      "\n",
      "The blast output will have the scores for later processing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "550T\n",
      "Find the next contig for the assembly.\n",
      "\n",
      "It's looking for the closest contig to the given beginning. The tiebreaker\n",
      "being the longer contig.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "551T\n",
      "Find the best overlapping contig for the assembly.\n",
      "\n",
      "Find an overlapping contig that starts anywhere between beg_lo & beg_hi.\n",
      "Is must also end somewhere after the given end marker. We want the contig\n",
      "that extends the stitched sequence by the longest amount so we ORDER BY\n",
      "end descending & choose the first one.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "552T\n",
      "Create a table to hold stitched genes & gap fillers.\n",
      "\n",
      "These overlaps are trimmed & the position in the assembled gene is noted.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "553T\n",
      "Prepare fasta files for exonerate.\n",
      "\n",
      "In this iteration we are getting all of the contigs from the first stitch\n",
      "and combining them into one long contig sequence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "554T\n",
      "Build one long contig that covers the reference gene.\n",
      "\n",
      "Build one long sequence from all of the non-overlapping contigs in the\n",
      "exonerate results. We want maximal coverage of the reference gene.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "556T\n",
      "Get all assembled contigs for the iteration.\n",
      "\n",
      "We will use them as the queries in the next atram iteration.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "557T\n",
      "Create query target for every query and query-split file.\n",
      "\n",
      "We put each query record into its own file for blast queries.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "558T\n",
      "Blast the query against the SRA databases.\n",
      "\n",
      "We're using a map-reduce strategy here. We map the blasting of the query\n",
      "sequences and reduce the output into one fasta file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "559T\n",
      "Get the shards we are using.\n",
      "\n",
      "We may not want the entire DB for highly redundant libraries.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "560T\n",
      "compute the distance (in number of gene between) between 2 hits\n",
      "\n",
      ":param :class:`macsypy.hit.ModelHit` h1: the first hit to compute inter hit distance\n",
      ":param :class:`macsypy.hit.ModelHit` h2: the second hit to compute inter hit distance\n",
      ":return: True if the 2 hits spaced by lesser or equal genes than inter_gene_max_space.\n",
      "         Managed circularity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "561T\n",
      "clusterize hit regarding the distance between them\n",
      "\n",
      ":param hits: the hits to clusterize\n",
      ":type hits: list of :class:`macsypy.model.ModelHit` objects\n",
      ":param model: the model to consider\n",
      ":type model: :class:`macsypy.model.Model` object\n",
      ":param hit_weights: the hit weight to compute the score\n",
      ":type hit_weights: :class:`macsypy.hit.HitWeight` object\n",
      ":type rep_info: :class:`macsypy.Indexes.RepliconInfo` object\n",
      "\n",
      ":return: the clusters\n",
      ":rtype: list of :class:`macsypy.cluster.Cluster` objects.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "562T\n",
      "We call a True Loner a Cluster composed of one or several hit related to the same gene tagged as loner\n",
      "(by opposition with hit representing a gene tagged loner but include in cluster with several other genes)\n",
      "\n",
      ":param clusters: the clusters\n",
      ":type clusters: list of :class:`macsypy.cluster.Cluster` objects.\n",
      ":return: tuple of 2 elts\n",
      "\n",
      "         * dict containing true clusters  {str func_name : :class:`macsypy.hit.Loner | :class:`macsypy.hit.LonerMultiSystem` object}\n",
      "         * list of :class:`macsypy.cluster.Cluster` objects\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "563T\n",
      "From a list of filtered hits, and replicon information (topology, length),\n",
      "build all lists of hits that satisfied the constraints:\n",
      "\n",
      "    * max_gene_inter_space\n",
      "    * loner\n",
      "    * multi_system\n",
      "\n",
      "If Yes create a cluster\n",
      "A cluster contains at least two hits separated by less or equal than max_gene_inter_space\n",
      "Except for loner genes which are allowed to be alone in a cluster\n",
      "\n",
      ":param hits: list of filtered hits\n",
      ":type hits: list of :class:`macsypy.hit.ModelHit` objects\n",
      ":param rep_info: the replicon to analyse\n",
      ":type rep_info: :class:`macsypy.Indexes.RepliconInfo` object\n",
      ":param model: the model to study\n",
      ":type model: :class:`macsypy.model.Model` object\n",
      ":return: list of regular clusters,\n",
      "         the special clusters (loners not in cluster and multi systems)\n",
      ":rtype: tuple with 2 elements\n",
      "\n",
      "        * true_clusters which is list of :class:`Cluster` objects\n",
      "        * true_loners: a dict { str function: :class:macsypy.hit.Loner | :class:macsypy.hit.LonerMultiSystem object}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "564T\n",
      ":param PN: The package name\n",
      ":param authors: the authors of the package\n",
      ":param cr_date: the date of the copyright (year)\n",
      ":param cr_holders: the holders of the copyright\n",
      ":param short_desc: One line description of the package\n",
      ":return: The preambule of the licence declaration\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "565T\n",
      "Create a text to put in the headers of all package file\n",
      "\n",
      ":param licence_name: The name of the license (accepted values are acronym for creative commons)\n",
      ":param PN: The program Name\n",
      ":param authors: the authors of the package\n",
      ":param cr_date: The date (year) of the copyright\n",
      ":param cr_holders: the holders of the copyright\n",
      ":param short_desc: One line description of the package\n",
      ":return: The text of the license to put on header of each package file\n",
      ":raise KeyError: when licence_name is not managed (not a CC licence)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "566T\n",
      ":param licence_name:\n",
      ":type licence_name:\n",
      ":return:\n",
      ":rtype:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "567T\n",
      "Compute the optimum number of worker and cpu per worker\n",
      "The number of worker is set by the user (1 by default 0 means all worker available)\n",
      "\n",
      "we use one worker per gene\n",
      "if number of workers is greater than number of genes then several cpu can be use by\n",
      "hmsearch to speed up the search step\n",
      "\n",
      ":param int genes_nb: the number of genes to search\n",
      ":param cfg: The macsyfinder configuration\n",
      ":type cfg: :class:`macsypy.config.Config` object\n",
      ":return: the number of worker and cpu_per_worker to use\n",
      ":rtype: tuple (int worker_nb, int cpu_per_worker)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "568T\n",
      "For each gene of the list, use the corresponding profile to perform an Hmmer search, and parse the output\n",
      "to generate a HMMReport that is saved in a file after CoreHit filtering.\n",
      "These tasks are performed in parallel using threads.\n",
      "The number of workers can be limited by worker_nb directive in the config object or\n",
      "in the command-line with the \"-w\" option.\n",
      "\n",
      ":param genes: the genes to search in the input sequence dataset\n",
      ":type genes: list of :class:`macsypy.gene.ModelGene` objects\n",
      ":param cfg: the configuration object\n",
      ":type cfg: :class:`macsypy.config.Config` object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "569T\n",
      "select the best Loner among several ones encoding for same function\n",
      "\n",
      "    * score\n",
      "    * i_evalue\n",
      "    * profile_coverage\n",
      "\n",
      ":param str function: the name of the function fulfill by the hits (all hits must have same function)\n",
      ":param hits: the hits to filter.\n",
      ":type hits: sequence of :class:`macsypy.hit.ModelHit` object\n",
      ":param str key: The criterion used to select the best hit 'score', i_evalue', 'profile_coverage'\n",
      ":return: the best hit\n",
      ":rtype: :class:`macsypy.hit.ModelHit` object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "570T\n",
      "Sort :class:`macsypy.hit.ModelHit` per function\n",
      "\n",
      ":param model_hits: a sequence of :class:`macsypy.hit.ModelHit`\n",
      ":return: dict {str function name: [model_hit, ...] }\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "571T\n",
      ":param ms_registry:\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "572T\n",
      "If several hits match the same protein, keep only the best match based either on\n",
      "\n",
      "    * score\n",
      "    * i_evalue\n",
      "    * profile_coverage\n",
      "\n",
      ":param hits: the hits to filter, all hits must match the same protein.\n",
      ":type hits: [ :class:`macsypy.hit.CoreHit` object, ...]\n",
      ":param str key: The criterion used to select the best hit 'score', i_evalue', 'profile_coverage'\n",
      ":return: the list of the best hits\n",
      ":rtype: [ :class:`macsypy.hit.CoreHit` object, ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "573T\n",
      ":param fasta_file: the file containing all input sequences in fasta format.\n",
      ":type fasta_file: file object\n",
      ":author: http://biostar.stackexchange.com/users/36/brentp\n",
      ":return: for a given fasta file, it returns an iterator which yields tuples\n",
      "         (string id, string comment, int sequence length)\n",
      ":rtype: iterator\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "574T\n",
      ":param fqn: the fully qualified de name of a DefinitionLocation object\n",
      "       the follow the schema model_name/<def_name>*/def_name\n",
      "       for instance CRISPR-Cas/typing/cas\n",
      ":type fqn: string\n",
      ":return: the list of components of the def path\n",
      "         ['CRISPR-Cas', 'typing', 'cas']\n",
      ":rtype: list of string\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "576T\n",
      ":param str models_dir: The path to the directory where are stored the models\n",
      ":param profile_suffix: the suffix of the hmm profiles\n",
      ":param relative_path: True if models_dir is relative false otherwise\n",
      ":return: the list of models in models_dir\n",
      ":rtype: [:class:`macsypy.registries.ModelLocation`, ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "577T\n",
      ":param models: the list of models to detect as returned by config.models.\n",
      ":type models: list of tuple with the following structure:\n",
      "              [('model_fqn', ('def1, def2, ...)), ('model_2', ('def1', ...)), ...]\n",
      ":param model_registry: the models registry for this run.\n",
      ":type model_registry: :class:`macsypy.registries.ModelRegistry` object.\n",
      ":return: the definitions to parse\n",
      ":rtype: list of :class:`macsypy.registries.DefinitionLocation` objects\n",
      ":raise ValueError: if a model name provided in models is not in model_registry.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "578T\n",
      "parse gembase file and get the list of replicon identifiers\n",
      "\n",
      ":param str genome_path: The path to a file containing sequence in **gembase** format\n",
      ":return: the list of replicon identifiers\n",
      ":rtype: list of str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "579T\n",
      ":return: The maximal number of threads available.\n",
      "         It's nice with cluster scheduler or linux.\n",
      "         On Mac it use the number of physical cores\n",
      ":rtype: int\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "580T\n",
      "xml.etree.ElementTree implement ident only from python 3.9\n",
      "below the code from python 3.9 to inject it in ET at runtime\n",
      "\n",
      ":param ElementTree: ElementTree class\n",
      ":type ElementTree: class\n",
      ":return: function indent\n",
      ":rtype: function\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "582T\n",
      "Among the systems choose the combination of systems which does not share :class:`macsypy.hit.CoreHit`\n",
      "and maximize the sum of systems scores\n",
      "\n",
      ":param systems: the systems to analyse\n",
      ":type systems: list of :class:`macsypy.system.System` object\n",
      ":return: the list of list of systems which represent one best solution and the it's score\n",
      ":rtype: tuple of 2 elements the best solution and it's score\n",
      "        ([[:class:`macsypy.system.System`, ...], [:class:`macsypy.system.System`, ...]], float score)\n",
      "        The inner list represent a best solution\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "583T\n",
      "generate the combinations of clusters, with loners and multi systems\n",
      "\n",
      ":param clusters: the clusters to combines\n",
      ":type clusters: list of :class:`macsypy.cluster.Cluster` object\n",
      ":param true_loners: the multi-systems hits\n",
      ":type true_loners: dict the name of the function code by hit gene_ref.alternate_of as key\n",
      "                          and 1 :class:`macsypy.cluster.Cluster` with the best a\n",
      "                          :class:`macsypy.hit.Loner` or\n",
      "                          :class:`macsypy.hit.LonerMultiSystem` hit  as value\n",
      ":param bool multi_loci: True if the model is multi_loci false otherwise\n",
      ":return: all available combination of clusters\n",
      ":rtype: List of combination. a combination is a tuple of :class:`macsypy.cluster.Cluster` objects\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "585T\n",
      ":param str path: the path to the archive\n",
      ":return: the name of the package and it's version\n",
      ":rtype: tuple\n",
      ":raise ValueError: if the extension of the package is neither '.tar.gz' nor '.tgz'\n",
      "                   or if the package does not seem to include version 'pack_name-<vers>.ext'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "586T\n",
      "Copy file from fh_in to out from position start to stop\n",
      "\n",
      ":param fh_in: the source file\n",
      ":type fh_in: file like object\n",
      ":param str out: the destination file name\n",
      ":param int start: the position to start the copy\n",
      ":param stop: the position to end the copy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "590T\n",
      ":return: the long description of the macsyfinder version\n",
      ":rtype: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "592T\n",
      "Search macsy-models for Model in a remote index.\n",
      "by default search in package name,\n",
      "if option -S is set search also in description\n",
      "by default the search is case insensitive except if\n",
      "option --match-case is set.\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "593T\n",
      ":param pattern: the substring to search packages names\n",
      ":param remote: the uri of the macsy-models index\n",
      ":param packages: list of packages to search in\n",
      ":param match_case: True if the search is case sensitive, False otherwise\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "594T\n",
      ":param pattern: the substring to search packages descriptions\n",
      ":param remote: the uri of the macsy-models index\n",
      ":param packages: list of packages to search in\n",
      ":param match_case: True if the search is case sensitive, False otherwise\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "595T\n",
      "Download tarball from remote models repository.\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "597T\n",
      "Install new models in macsyfinder local models repository.\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "598T\n",
      "Remove models from macsyfinder local models repository.\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "599T\n",
      "Show information about installed model.\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "600T\n",
      "List installed models.\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "601T\n",
      "How to cite an installed model.\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "602T\n",
      "Display on stdout the content of readme file\n",
      "if the readme file does nopt exists display a message to the user see :meth:`macsypy.package.help`\n",
      "\n",
      ":param args: the arguments passed on the command line (the package name)\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":return: None\n",
      ":raise ValueError: if the package name is not known.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "603T\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "604T\n",
      "display on stdout the definition if only a package or sub-package is specified\n",
      "display all model definitions in the corresponding package or subpackage\n",
      "\n",
      "for instance\n",
      "\n",
      "`TXSS+/bacterial T6SSii T6SSiii`\n",
      "\n",
      "display models *TXSS+/bacterial/T6SSii* and *TXSS+/bacterial/T6SSiii*\n",
      "\n",
      "`TXSS+/bacterial all` or `TXSS+/bacterial`\n",
      "\n",
      "display all models contains in *TXSS+/bacterial subpackage*\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "605T\n",
      "Create a template for data package\n",
      "\n",
      "    - skeleton for metadata.yml\n",
      "    - definitions directory with a skeleton of models.xml\n",
      "    - profiles directory\n",
      "    - skeleton for README.md file\n",
      "    - COPYRIGHT file (if holders option is set)\n",
      "    - LICENSE file (if license option is set)\n",
      "\n",
      ":param args: The parsed commandline subcommand arguments\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "606T\n",
      "Build argument parser.\n",
      "\n",
      ":rtype: :class:`argparse.ArgumentParser` object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "607T\n",
      "Return the name of the command being executed\n",
      "(scriptname + operation).\n",
      "\n",
      "Example\n",
      "    macsydata uninstall\n",
      "\n",
      ":param args: the arguments passed on the command line\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":rtype: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "608T\n",
      ":param level: The logger threshold could be a positive int or string\n",
      "              among: 'CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG'\n",
      ":param out: if the log message must be displayed\n",
      ":return: logger\n",
      ":rtype: :class:`logging.Logger` instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "610T\n",
      ":param path: the path of the result file to parse\n",
      ":type path: str\n",
      ":return: the list of warning in the header\n",
      ":rtype: list of str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "611T\n",
      ":param files: the list of files to merge\n",
      ":type files: list of str\n",
      ":param str out: the path to the merged file\n",
      ":param str ignore: a string which start the lines to ignore\n",
      ":param str keep_first: a string which start the line which must be keep\n",
      "                   only the first time\n",
      ":param skip_until: skip all lines until the condition is True\n",
      ":type skip_until: a fonction which test the line\n",
      ":param str header: The header of the merged file\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "612T\n",
      "merge all_best_solutions and reindex the sol_id column\n",
      "\n",
      ":param files: the list of files to merge\n",
      ":type files: list of str\n",
      ":param str out: the path to the merged file\n",
      ":param str ignore: a string which start the lines to ignore\n",
      ":param str header: The header of the merged file\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "613T\n",
      ":param files: the list of files to merge\n",
      ":param str out: the path to the merged file\n",
      ":param str header: The header of the merged file\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "614T\n",
      ":param results_dirs: The list of macsyfinder results directories to merge\n",
      ":type results_dirs: list of str\n",
      ":param str out_dir: the path to the directory where to store the merged files\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "615T\n",
      ":param args: the arguments passed on the command line without the first elemnet\n",
      ":type args: list of str\n",
      ":return: the command line and options arguments parsed\n",
      ":rtype: :class:`argparse.Namespace` object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "616T\n",
      ":return: the long description of the macsyfinder version\n",
      ":rtype: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "617T\n",
      "Parse the HMM profile to extract the length and the presence of GA bit threshold\n",
      "\n",
      ":param str path: The path to the hmm profile used to produced the hmm search output to analyse\n",
      ":return: the length, presence of ga bit threshold\n",
      ":rtype: tuple(int length, bool ga_threshold)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "618T\n",
      ":param str path: The path to the hmm output to analyse\n",
      ":param str suffix: the suffix of the hmm output file\n",
      ":return: the name of the analysed gene\n",
      ":rtype: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "620T\n",
      ":param level: The logger threshold could be a positive int or string\n",
      "              among: 'CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG'\n",
      ":param out: if the log message must be displayed\n",
      ":return: logger\n",
      ":rtype: :class:`logging.Logger` instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "621T\n",
      "transform the number of -v option in loglevel\n",
      ":param int verbosity: number of -v option on the command line\n",
      ":return: an int corresponding to a logging level\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "622T\n",
      ":param args: The arguments provided on the command line\n",
      ":type args: List of strings [without the program name]\n",
      ":return: The arguments parsed\n",
      ":rtype: :class:`aprgparse.Namespace` object.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "623T\n",
      ":return: the long description of the macsyfinder version\n",
      ":rtype: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "624T\n",
      ":param args: The command line argument once parsed\n",
      ":type args: :class:`argparse.Namespace` object\n",
      ":return: a string representation of all models and submodels installed.\n",
      ":rtype: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "626T\n",
      "Do the job, this function is the orchestrator of all the macsyfinder mechanics\n",
      "at the end several files are produced containing the results\n",
      "\n",
      "  - macsyfinder.conf: The set of variables used to runt this job\n",
      "  - macsyfinder.systems: The list of the potential systems\n",
      "  - macsyfinder.rejected_cluster: The list of all clusters and clustrs combination\n",
      "                                  which has been rejected and the reason\n",
      "  - macsyfinder.log: the copy of the standard output\n",
      "\n",
      ":param config: The MacSyFinder Configuration\n",
      ":type config: :class:`macsypy.config.Config` object\n",
      ":param model_registry: the registry of all models\n",
      ":type model_registry: :class:`macsypy.registries.ModelRegistry` object\n",
      ":param models_def_to_detect: the definitions to detect\n",
      ":type models_def_to_detect: list of :class:`macsypy.registries.DefinitionLocation` objects\n",
      ":param logger: The logger use to display information to the user.\n",
      "               It must be initialized. see :func:`macsypy.init_logger`\n",
      ":type logger: :class:`colorlog.Logger` object\n",
      ":return: the systems and rejected clusters found\n",
      ":rtype: ([:class:`macsypy.system.System`, ...], [:class:`macsypy.cluster.RejectedCAndidate`, ...])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "627T\n",
      ":param hits_by_replicon:\n",
      ":param models_to_detect:\n",
      ":param config:\n",
      ":param logger:\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "628T\n",
      ":param hits_by_replicon:\n",
      ":param models_to_detect:\n",
      ":param logger:\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "629T\n",
      ":return: The 2 first lines of each result file\n",
      ":rtype: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "632T\n",
      "print solution in a file in tabulated format\n",
      "A solution is a set of systems which represents an optimal combination of\n",
      "systems to maximize the score.\n",
      "\n",
      ":param solutions: list of systems found\n",
      ":type solutions: list of list of :class:`macsypy.system.System` objects\n",
      ":param hit_system_tracker: a filled HitSystemTracker.\n",
      ":type hit_system_tracker: :class:`macsypy.system.HitSystemTracker` object\n",
      ":param sys_file: The file where to write down the systems occurrences\n",
      ":type sys_file: file object\n",
      ":param skipped_replicons: the replicons name for which msf reach the timeout\n",
      ":type skipped_replicons: list of str\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "633T\n",
      ":param systems: sequence of systems\n",
      ":return: warning for loner which have less occurrences than systems occurrences in which this lone is used\n",
      "         except if the loner is also multi system\n",
      ":rtype: list of string\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "634T\n",
      "do a summary of best_solution in best_solution_path and write it on out_path\n",
      "a summary compute the number of system occurrence for each model and each replicon\n",
      ".. code-block:: text\n",
      "\n",
      "    replicon        model_fqn_1  model_fqn_2  ....\n",
      "    rep_name_1           1           2\n",
      "    rep_name_2           2           0\n",
      "\n",
      "columns are separated by     character\n",
      "\n",
      ":param str best_solution_path: the path to the best_solution file in tsv format\n",
      ":param sys_file: the file where to save the summary\n",
      ":param models_fqn: the fully qualified names of the models\n",
      ":type models_fqn: list of string\n",
      ":param replicon_names: the name of the replicons used\n",
      ":type replicon_names: list of string\n",
      ":param skipped_replicons: the replicons name for which msf reach the timeout\n",
      ":type skipped_replicons: list of str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "635T\n",
      "get loners from valid systems and save them on file\n",
      "\n",
      ":param systems: the systems from which the loners are extract\n",
      ":type systems: list of :class:`macsypy.system.System` object\n",
      ":param sys_file: the file where loners are saved\n",
      ":type sys_file: file object open in write mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "636T\n",
      "get multisystems from valid systems and save them on file\n",
      "\n",
      ":param systems: the systems from which the loners are extract\n",
      ":type systems: list of :class:`macsypy.system.System` object\n",
      ":param sys_file: the file where multisystems are saved\n",
      ":type sys_file: file object open in write mode\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "637T\n",
      "print rejected clusters in a file\n",
      "\n",
      ":param rejected_candidates: list of candidates which does not contitute a system\n",
      ":type rejected_candidates: list of :class:`macsypy.system.RejectedCandidate` objects\n",
      ":param cand_file: The file where to write down the rejected candidates\n",
      ":type cand_file: file object\n",
      ":param skipped_replicons: the replicons name for which msf reach the timeout\n",
      ":type skipped_replicons: list of str\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "638T\n",
      "print rejected clusters in a file\n",
      "\n",
      ":param rejected_candidates: list of candidates which does not contitute a system\n",
      ":type rejected_candidates: list of :class:`macsypy.system.RejectedCandidate` objects\n",
      ":param cand_file: The file where to write down the rejected candidates\n",
      ":type cand_file: file object\n",
      ":param skipped_replicons: the replicons name for which msf reach the timeout\n",
      ":type skipped_replicons: list of str\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "639T\n",
      "print likely systems occurrences (from unordered replicon)\n",
      "in a file in text human readable format\n",
      ":param likely_systems: list of systems found\n",
      ":type likely_systems: list of :class:`macsypy.system.LikelySystem` objects\n",
      ":param hit_system_tracker: a filled HitSystemTracker.\n",
      ":type hit_system_tracker: :class:`macsypy.system.HitSystemTracker` object\n",
      ":param sys_file: file object\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "640T\n",
      "print likely systems occurrences (from unordered replicon)\n",
      "in a file in tabulated separeted value (tsv) format\n",
      "\n",
      ":param likely_systems: list of systems found\n",
      ":type likely_systems: list of :class:`macsypy.system.LikelySystem` objects\n",
      ":param hit_system_tracker: a filled HitSystemTracker.\n",
      ":type hit_system_tracker: :class:`macsypy.system.HitSystemTracker` object\n",
      ":param sys_file: The file where to write down the systems occurrences\n",
      ":type sys_file: file object\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "641T\n",
      "print hits (from unordered replicon) which probably does not make a system occurrences\n",
      "in a file in human readable format\n",
      "\n",
      ":param unlikely_systems: list of :class:`macsypy.system.UnLikelySystem` objects\n",
      ":param sys_file: The file where to write down the systems occurrences\n",
      ":type sys_file: file object\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "642T\n",
      "Check if value point to an executable\n",
      "\n",
      ":param str raw: the value return by the user\n",
      ":param str default: the default value for the option\n",
      ":param expected: not used here to have the same signature for all check_xxx functions\n",
      ":return: value\n",
      ":raise MacsypyError: if the value cannot be cast in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "643T\n",
      "Check if value can be cast in integer >=0\n",
      "\n",
      ":param str raw: the value return by the user\n",
      ":param int default: the default value for the option\n",
      ":param expected: not used here to have the same signature for all check_xxx functions\n",
      ":return: value\n",
      ":raise MacsypyError: if the value cannot be cast in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "644T\n",
      "Check if value can be cast in float\n",
      "\n",
      ":param str raw: the value return by the user\n",
      ":param float default: the default value for the option\n",
      ":param expected: not used here to have the same signature for all check_xxx functions\n",
      ":return: value\n",
      ":raise MacsypyError: if the value cannot be cast in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "645T\n",
      "Check if value can be cast in str\n",
      "\n",
      ":param str raw: the value return by the user\n",
      ":param str default: the default value for the option\n",
      ":param expected: not used here to have the same signature for all check_xxx functions\n",
      ":return: value\n",
      ":raise MacsypyError: if the value cannot be cast in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "646T\n",
      "Check if value can be cast in str\n",
      "\n",
      ":param str raw: the value return by the user\n",
      ":param str default: the default value for the option\n",
      ":param expected: not used here to have the same signature for all check_xxx functions\n",
      ":return: value\n",
      ":raise MacsypyError: if the value cannot be cast in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "647T\n",
      "Check if value point to a directory\n",
      "\n",
      ":param str raw: the value return by the user\n",
      ":param str default: the default value for the option\n",
      ":param expected: not used here to have the same signature for all check_xxx functions\n",
      ":return: value\n",
      ":raise MacsypyError: if the value cannot be cast in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "648T\n",
      "Check if value point to a file\n",
      "\n",
      ":param str raw: the value return by the user\n",
      ":param str default: the default value for the option\n",
      ":param expected: not used here to have the same signature for all check_xxx functions\n",
      ":return: value\n",
      ":raise MacsypyError: if the value cannot be cast in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "649T\n",
      "Check if value is in list of expected values\n",
      "\n",
      ":param str raw: the value return by the user\n",
      ":param str default: the default value for the option\n",
      ":param expected: the allowed vlaues for this option\n",
      ":return: value\n",
      ":raise MacsypyError: if the value cannot be cast in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "650T\n",
      "ask a question on the terminal and return the user response\n",
      "check if the user response is allowed (right type, among allowed values, ...)\n",
      "\n",
      ":param str question: The question to prompt to the user on the terminal\n",
      ":param validator: what validator to be used to check the user response\n",
      ":type validator: a function define in this module starting by check\\_\n",
      ":param default: the default value\n",
      ":param expected: the values allowed (can be a list of value\n",
      ":param str explanation: some explanation about the option\n",
      ":param bool sequence: True if the parameter accept a sequence of value (comma separated values)\n",
      ":param question_color: the color of the question display to the user\n",
      ":type question_color: an attribute of :class:`macsypy.scripts.macsyconfig.Theme`\n",
      ":param int retry: The number of time to repeat the question if the response is rejected\n",
      ":return: the value casted in right type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "651T\n",
      "iter over options of a section\n",
      "ask question for each option\n",
      "and set this option in the config\n",
      "\n",
      ":param str sec_name: the name of the section\n",
      ":param dict options: a dictionnary with the options to set up for this section\n",
      ":param config: The config to fill in.\n",
      ":type config: :class:`ConfigParserWithComments` object\n",
      ":param defaults: the macsyfinder defaults values\n",
      ":type defaults: :class:`macsypy.config.MacsyDefaults` object\n",
      ":param bool use_defaults: The user skip this section so use defaults to set in config object\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "652T\n",
      "Options for directories section\n",
      "\n",
      ":param config: The config to setup\n",
      ":type config: :class:`ConfigParserWithComments` object\n",
      ":param defaults: the macsyfinder defaults values\n",
      ":type defaults: :class:`macsypy.config.MacsyDefaults` object\n",
      ":param bool use_defaults: If True do not ask any question use the defaults values\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "653T\n",
      "Options for hmmer section\n",
      "\n",
      ":param config: The config to setup\n",
      ":type config: :class:`ConfigParserWithComments` object\n",
      ":param defaults: the macsyfinder defaults values\n",
      ":type defaults: :class:`macsypy.config.MacsyDefaults` object\n",
      ":param bool use_defaults: If True do not ask any question use the defaults values\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "654T\n",
      "Options for general section\n",
      "\n",
      ":param config: The config to setup\n",
      ":type config: :class:`ConfigParserWithComments` object\n",
      ":param defaults: the macsyfinder defaults values\n",
      ":type defaults: :class:`macsypy.config.MacsyDefaults` object\n",
      ":param bool use_defaults: If True do not ask any question use the defaults values\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "655T\n",
      "Options for scoring section\n",
      "\n",
      ":param config: The config to setup\n",
      ":type config: :class:`ConfigParserWithComments` object\n",
      ":param defaults: the macsyfinder defaults values\n",
      ":type defaults: :class:`macsypy.config.MacsyDefaults` object\n",
      ":param bool use_defaults: If True do not ask any question use the defaults values\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "656T\n",
      "Options for base section\n",
      "\n",
      ":param config: The config to setup\n",
      ":type config: :class:`ConfigParserWithComments` object\n",
      ":param defaults: the macsyfinder defaults values\n",
      ":type defaults: :class:`macsypy.config.MacsyDefaults` object\n",
      ":param bool use_defaults: If True do not ask any question use the defaults values\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "657T\n",
      "save the configuration on file\n",
      "\n",
      ":param config: the config to save\n",
      ":type config: :class:`ConfigParserWithComments` object\n",
      ":param str path: where to store the configuration\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "658T\n",
      "parse command line\n",
      "\n",
      ":param args: the command line arguments\n",
      ":type args: list of string\n",
      ":return:\n",
      ":rtype: :class:`argparse.Namespace` object\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "659T\n",
      "Check types and return inputs for MBAR calculations.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn or q_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies or unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "Returns\n",
      "-------\n",
      "u_kn or q_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies or unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The number of samples in each state.  Converted to float because this cast is required when log is calculated.\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "660T\n",
      "Return an improved guess for the dimensionless free energies\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "Returns\n",
      "-------\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    Updated estimate of f_k\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Equation C3 in MBAR JCP paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "661T\n",
      "JAX version of self_consistent update.  For parameters, see self_consistent_update.\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "662T\n",
      "JAX version of self_consistent update.  For parameters, see self_consistent_update.\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "663T\n",
      "Gradient of MBAR objective function.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "Returns\n",
      "-------\n",
      "grad : np.ndarray, dtype=float, shape=(n_states)\n",
      "    Gradient of mbar_objective\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This is equation C6 in the JCP MBAR paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "665T\n",
      "Calculates objective function for MBAR.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "\n",
      "Returns\n",
      "-------\n",
      "obj : float\n",
      "    Objective function\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This objective function is essentially a doubly-summed partition function and is\n",
      "quite sensitive to precision loss from both overflow and underflow. For optimal\n",
      "results, u_kn can be preconditioned by subtracting out a `n` dependent\n",
      "vector.\n",
      "\n",
      "More optimal precision, the objective function uses math.fsum for the\n",
      "outermost sum and logsumexp for the inner sum.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "666T\n",
      "JAX version of mbar_objective.\n",
      "For parameters, mbar_objective_and_Gradient\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "667T\n",
      "JAX version of mbar_objective_and_gradient.\n",
      "For parameters, mbar_objective_and_Gradient\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "668T\n",
      "Calculates both objective function and gradient for MBAR.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "\n",
      "Returns\n",
      "-------\n",
      "obj : float\n",
      "    Objective function\n",
      "grad : np.ndarray, dtype=float, shape=(n_states)\n",
      "    Gradient of objective function\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This objective function is essentially a doubly-summed partition function and is\n",
      "quite sensitive to precision loss from both overflow and underflow. For optimal\n",
      "results, u_kn can be preconditioned by subtracting out a `n` dependent\n",
      "vector.\n",
      "\n",
      "More optimal precision, the objective function uses math.fsum for the\n",
      "outermost sum and logsumexp for the inner sum.\n",
      "\n",
      "The gradient is equation C6 in the JCP MBAR paper; the objective\n",
      "function is its integral.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "669T\n",
      "JAX version of mbar_hessian.\n",
      "For parameters, see mbar_hessian\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "670T\n",
      "Hessian of MBAR objective function.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "Returns\n",
      "-------\n",
      "H : np.ndarray, dtype=float, shape=(n_states, n_states)\n",
      "    Hessian of mbar objective function.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Equation (C9) in JCP MBAR paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "671T\n",
      "JAX version of mbar_log_W_nk.\n",
      "For parameters, see mbar_log_W_nk\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "672T\n",
      "Calculate the log weight matrix.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "Returns\n",
      "-------\n",
      "logW_nk : np.ndarray, dtype='float', shape=(n_samples, n_states)\n",
      "    The normalized log weights.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Equation (9) in JCP MBAR paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "673T\n",
      "JAX version of mbar_W_nk.\n",
      "For parameters, see mbar_W_nk\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "674T\n",
      "Calculate the weight matrix.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "Returns\n",
      "-------\n",
      "W_nk : np.ndarray, dtype='float', shape=(n_samples, n_states)\n",
      "    The normalized weights.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Equation (9) in JCP MBAR paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "675T\n",
      "Determine dimensionless free energies by a combination of Newton-Raphson iteration and self-consistent iteration.\n",
      "Picks whichever method gives the lowest gradient.\n",
      "Is slower than NR since it calculates the log norms twice each iteration.\n",
      "\n",
      "OPTIONAL ARGUMENTS\n",
      "tol (float between 0 and 1) - relative tolerance for convergence (default 1.0e-12)\n",
      "\n",
      "options : dictionary of options\n",
      "    gamma (float between 0 and 1) - incrementor for NR iterations (default 1.0).  Usually not changed now, since adaptively switch.\n",
      "    maxiter (int) - maximum number of Newton-Raphson iterations (default 10000: either NR converges or doesn't, pretty quickly)\n",
      "    verbose (boolean) - verbosity level for debug output\n",
      "\n",
      "NOTES\n",
      "\n",
      "This method determines the dimensionless free energies by\n",
      "minimizing a convex function whose solution is the desired\n",
      "estimator.  The original idea came from the construction of a\n",
      "likelihood function that independently reproduced the work of\n",
      "Geyer (see [1] and Section 6 of [2]).  This can alternatively be\n",
      "formulated as a root-finding algorithm for the Z-estimator.  More\n",
      "details of this procedure will follow in a subsequent paper.  Only\n",
      "those states with nonzero counts are include in the estimation\n",
      "procedure.\n",
      "\n",
      "REFERENCES\n",
      "See Appendix C.2 of [1].\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "676T\n",
      "JAX version of adaptive inner loop.\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "677T\n",
      "JAX version of precondition_u_kn\n",
      "for parameters, see precondition_u_kn\n",
      "N_k must be float (should be cast at a higher level)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "678T\n",
      "Subtract a sample-dependent constant from u_kn to improve precision\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "\n",
      "Returns\n",
      "-------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Returns u_kn - x_n, where x_n is based on the current estimate of f_k.\n",
      "Upon subtraction of x_n, the MBAR objective function changes by an\n",
      "additive constant, but its derivatives remain unchanged.  We choose\n",
      "x_n such that the current objective function value is zero, which\n",
      "should give maximum precision in the objective function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "679T\n",
      "Solve MBAR self-consistent equations using some form of equation solver.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn_nonzero : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "    for the nonempty states\n",
      "N_k_nonzero : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state for the nonempty states\n",
      "f_k_nonzero : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies for the nonempty states\n",
      "method : str, optional, default=\"hybr\"\n",
      "    The optimization routine to use.  This can be any of the methods\n",
      "    available via scipy.optimize.minimize() or scipy.optimize.root().\n",
      "tol : float, optional, default=1E-14\n",
      "    The convergance tolerance for minimize() or root()\n",
      "verbose: bool\n",
      "    Whether to print information about the solution method.\n",
      "options: dict, optional, default=None\n",
      "    Optional dictionary of algorithm-specific parameters.  See\n",
      "    scipy.optimize.root or scipy.optimize.minimize for details.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "f_k : np.ndarray\n",
      "    The converged reduced free energies.\n",
      "results : dict\n",
      "    Dictionary containing entire results of optimization routine, may\n",
      "    be useful when debugging convergence.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This function requires that N_k_nonzero > 0--that is, you should have\n",
      "already dropped all the states for which you have no samples.\n",
      "Internally, this function works in a reduced coordinate system defined\n",
      "by subtracting off the first component of f_k and fixing that component\n",
      "to be zero.\n",
      "\n",
      "For fast but precise convergence, we recommend calling this function\n",
      "multiple times to polish the result.  `solve_mbar()` facilitates this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "680T\n",
      "Solve MBAR self-consistent equations using some sequence of equation solvers.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn_nonzero : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "    for the nonempty states\n",
      "N_k_nonzero : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state for the nonempty states\n",
      "f_k_nonzero : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies for the nonempty states\n",
      "solver_protocol : tuple(dict()), optional, default=None\n",
      "    Optional list of dictionaries of steps in solver protocol.\n",
      "    If None, a default protocol will be used.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "f_k : np.ndarray\n",
      "    The converged reduced free energies.\n",
      "all_results : list(dict())\n",
      "    List of results from each step of solver_protocol.  Each element in\n",
      "    list contains the results dictionary from solve_mbar_once()\n",
      "    for the corresponding step.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This function requires that N_k_nonzero > 0--that is, you should have\n",
      "already dropped all the states for which you have no samples.\n",
      "Internally, this function works in a reduced coordinate system defined\n",
      "by subtracting off the first component of f_k and fixing that component\n",
      "to be zero.\n",
      "\n",
      "This function calls `solve_mbar_once()` multiple times to achieve\n",
      "converged results.  Generally, a single call to solve_mbar_once()\n",
      "will not give fully converged answers because of limited numerical precision.\n",
      "Each call to `solve_mbar_once()` re-conditions the nonlinear\n",
      "equations using the current guess.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "681T\n",
      "Solve for free energies of states with samples, then calculate for\n",
      "empty states.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, shape=(n_states, n_samples), dtype='float'\n",
      "    The reduced potential energies, i.e. -log unnormalized probabilities\n",
      "N_k : np.ndarray, shape=(n_states), dtype='int'\n",
      "    The number of samples in each state\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The reduced free energies of each state\n",
      "solver_protocol : tuple(dict()), optional, default=None\n",
      "    Sequence of dictionaries of steps in solver protocol for final\n",
      "    stage of refinement.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "f_k : np.ndarray, shape=(n_states), dtype='float'\n",
      "    The free energies of states\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "682T\n",
      "TODO: Add description for this function and types for parameters\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "replicates:\n",
      "    An array of replicates, and the size of the data.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "np.array\n",
      "    a Nxdims array of the data in the replicates, normalized by the standard deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "683T\n",
      "TODO: Description here\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "    replicates: list of replicates\n",
      "    K: number of replicates\n",
      "\n",
      "Returns\n",
      "-------\n",
      "type\n",
      "    Anderson-Darling statistics. See:\n",
      "    http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Since both sigma and mu are known (mu exactly, sigma as an estimate from mbar),\n",
      "we can apply the case 1 test.\n",
      "\n",
      "Because sigma is not precise, we should accept a higher threshold than the 1%\n",
      "threshold listed below to throw an error:\n",
      "\n",
      "    15%  1.610\n",
      "    10%  1.933\n",
      "    5%   2.492\n",
      "    2.5% 3.070\n",
      "    1%   3.857\n",
      "\n",
      "So we choose something like 4.5.  Note that for lower numbers of\n",
      "samples, it's more likely.  2000 samples for each of the\n",
      "harmonic_oscillators_distributions.py seems to give good\n",
      "results.\n",
      "\n",
      "For now, the standard deviation we use is the one from the\n",
      "_first_ replicate.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "684T\n",
      "TODO: Description here\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "replicates : list\n",
      "    TODO: type and description\n",
      "K : int\n",
      "    TODO: type and description\n",
      "title : str, optional=\"Generic Q-Q plot\"\n",
      "    Plot title\n",
      "filename : str, optional=\"qq.pdf\"\n",
      "    Output path to generated PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "685T\n",
      "Parameters\n",
      "----------\n",
      "replicates: list\n",
      "    list of replicates\n",
      "K: int\n",
      "    number of replicates\n",
      "\n",
      "Returns\n",
      "-------\n",
      "alpha_values\n",
      "    TODO: Description and type\n",
      "Pobs\n",
      "    TODO: Description and type\n",
      "Plow\n",
      "    TODO: Description and type\n",
      "Phigh\n",
      "    TODO: Description and type\n",
      "dPobs\n",
      "    TODO: Description and type\n",
      "Pnorm\n",
      "    TODO: Description and type\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Analyze data.\n",
      "\n",
      "By Chebyshev's inequality, we should have\n",
      "  P(error >= alpha sigma) <= 1 / alpha^2\n",
      "so that a lower bound will be\n",
      "  P(error < alpha sigma) > 1 - 1 / alpha^2\n",
      "for any real multiplier 'k', where 'sigma' represents the computed uncertainty (as one standard deviation).\n",
      "\n",
      "If the error is normal, we should have\n",
      "  P(error < alpha sigma) = erf(alpha / sqrt(2))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "686T\n",
      "Compute the (cross) statistical inefficiency of (two) timeseries.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "A_n : np.ndarray, float\n",
      "    A_n[n] is nth value of timeseries A.  Length is deduced from vector.\n",
      "B_n : np.ndarray, float, optional, default=None\n",
      "    B_n[n] is nth value of timeseries B.  Length is deduced from vector.\n",
      "    If supplied, the cross-correlation of timeseries A and B will be estimated instead of the\n",
      "    autocorrelation of timeseries A.\n",
      "fast : bool, optional, default=False\n",
      "    f True, will use faster (but less accurate) method to estimate correlation\n",
      "    time, described in Ref. [1] (default: False).  This is ignored\n",
      "    when B_n=None and fft=True.\n",
      "mintime : int, optional, default=3\n",
      "    minimum amount of correlation function to compute (default: 3)\n",
      "    The algorithm terminates after computing the correlation time out to mintime when the\n",
      "    correlation function first goes negative.  Note that this time may need to be increased\n",
      "    if there is a strong initial negative peak in the correlation function.\n",
      "fft : bool, optional, default=False\n",
      "    If fft=True and B_n=None, then use the fft based approach, as\n",
      "    implemented in statistical_inefficiency_fft().\n",
      "\n",
      "Returns\n",
      "-------\n",
      "g : np.ndarray,\n",
      "    g is the estimated statistical inefficiency (equal to 1 + 2 tau, where tau is the correlation time).\n",
      "    We enforce g >= 1.0.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The same timeseries can be used for both A_n and B_n to get the autocorrelation statistical inefficiency.\n",
      "The fast method described in Ref [1] is used to compute g.\n",
      "\n",
      "References\n",
      "----------\n",
      "[1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n",
      "histogram analysis method for the analysis of simulated and parallel tempering simulations.\n",
      "JCTC 3(1):26-41, 2007.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Compute statistical inefficiency of timeseries data with known correlation time.\n",
      "\n",
      ">>> from pymbar.testsystems import correlated_timeseries_example\n",
      ">>> A_n = correlated_timeseries_example(N=100000, tau=5.0)\n",
      ">>> g = statistical_inefficiency(A_n, fast=True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "687T\n",
      "Estimate the statistical inefficiency from multiple stationary timeseries (of potentially differing lengths).\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "A_kn : list of np.ndarrays\n",
      "    A_kn[k] is the kth timeseries, and A_kn[k][n] is nth value of timeseries k.  Length is deduced from arrays.\n",
      "\n",
      "fast : bool, optional, default=False\n",
      "    f True, will use faster (but less accurate) method to estimate correlation\n",
      "    time, described in Ref. [1] (default: False)\n",
      "return_correlation_function : bool, optional, default=False\n",
      "    if True, will also return estimates of normalized fluctuation correlation function that were computed (default: False)\n",
      "\n",
      "Returns\n",
      "-------\n",
      "g : np.ndarray,\n",
      "    g is the estimated statistical inefficiency (equal to 1 + 2 tau, where tau is the correlation time).\n",
      "    We enforce g >= 1.0.\n",
      "Ct : list (of tuples)\n",
      "    Ct[n] = (t, C) with time t and normalized correlation function estimate C is returned as well if return_correlation_function is set to True\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The autocorrelation of the timeseries is used to compute the statistical inefficiency.\n",
      "The normalized fluctuation autocorrelation function is computed by averaging the unnormalized raw correlation functions.\n",
      "The fast method described in Ref [1] is used to compute g.\n",
      "\n",
      "References\n",
      "----------\n",
      "[1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n",
      "    histogram analysis method for the analysis of simulated and parallel tempering simulations.\n",
      "    JCTC 3(1):26-41, 2007.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Estimate statistical efficiency from multiple timeseries of different lengths.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> N_k = [1000, 2000, 3000, 4000, 5000]\n",
      ">>> tau = 5.0 # exponential relaxation time\n",
      ">>> A_kn = [ testsystems.correlated_timeseries_example(N=N, tau=tau) for N in N_k ]\n",
      ">>> g = statistical_inefficiency_multiple(A_kn)\n",
      "\n",
      "Also return the values of the normalized fluctuation autocorrelation function that were computed.\n",
      "\n",
      ">>> [g, Ct] = statistical_inefficiency_multiple(A_kn, return_correlation_function=True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "688T\n",
      "Estimate the integrated autocorrelation time.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "statisticalInefficiency\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "689T\n",
      "Estimate the integrated autocorrelation time from multiple timeseries.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "statistical_inefficiency_multiple\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "690T\n",
      "Compute the normalized fluctuation (cross) correlation function of (two) stationary timeseries.\n",
      "\n",
      "C(t) = (<A(t) B(t)> - <A><B>) / (<AB> - <A><B>)\n",
      "\n",
      "This may be useful in diagnosing odd time-correlations in timeseries data.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "A_n : np.ndarray\n",
      "    A_n[n] is nth value of timeseries A.  Length is deduced from vector.\n",
      "B_n : np.ndarray\n",
      "    B_n[n] is nth value of timeseries B.  Length is deduced from vector.\n",
      "N_max : int, default=None\n",
      "    if specified, will only compute correlation function out to time lag of N_max\n",
      "norm: bool, optional, default=True\n",
      "    if False will return the unnormalized correlation function D(t) = <A(t) B(t)>\n",
      "\n",
      "Returns\n",
      "-------\n",
      "C_n : np.ndarray\n",
      "    C_n[n] is the normalized fluctuation auto- or cross-correlation function for timeseries A(t) and B(t).\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The same timeseries can be used for both A_n and B_n to get the autocorrelation statistical inefficiency.\n",
      "This procedure may be slow.\n",
      "The statistical error in C_n[n] will grow with increasing n.  No effort is made here to estimate the uncertainty.\n",
      "\n",
      "References\n",
      "----------\n",
      "[1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n",
      "histogram analysis method for the analysis of simulated and parallel tempering simulations.\n",
      "JCTC 3(1):26-41, 2007.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Estimate normalized fluctuation correlation function.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> A_t = testsystems.correlated_timeseries_example(N=10000, tau=5.0)\n",
      ">>> C_t = normalized_fluctuation_correlation_function(A_t, N_max=25)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "691T\n",
      "Compute the normalized fluctuation (cross) correlation function of (two) timeseries from multiple timeseries samples.\n",
      "\n",
      "C(t) = (<A(t) B(t)> - <A><B>) / (<AB> - <A><B>)\n",
      "This may be useful in diagnosing odd time-correlations in timeseries data.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "A_kn : Python list of numpy arrays\n",
      "    A_kn[k] is the kth timeseries, and A_kn[k][n] is nth value of timeseries k.  Length is deduced from arrays.\n",
      "B_kn : Python list of numpy arrays\n",
      "    B_kn[k] is the kth timeseries, and B_kn[k][n] is nth value of timeseries k.  B_kn[k] must have same length as A_kn[k]\n",
      "N_max : int, optional, default=None\n",
      "    if specified, will only compute correlation function out to time lag of N_max\n",
      "norm: bool, optional, default=True\n",
      "    if False, will return unnormalized D(t) = <A(t) B(t)>\n",
      "truncate: bool, optional, default=False\n",
      "    if True, will stop calculating the correlation function when it goes below 0\n",
      "\n",
      "Returns\n",
      "-------\n",
      "C_n[n] : np.ndarray\n",
      "    The normalized fluctuation auto- or cross-correlation function for timeseries A(t) and B(t).\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The same timeseries can be used for both A_n and B_n to get the autocorrelation statistical inefficiency.\n",
      "This procedure may be slow.\n",
      "The statistical error in C_n[n] will grow with increasing n.  No effort is made here to estimate the uncertainty.\n",
      "\n",
      "References\n",
      "----------\n",
      "[1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n",
      "histogram analysis method for the analysis of simulated and parallel tempering simulations.\n",
      "JCTC 3(1):26-41, 2007.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Estimate a portion of the normalized fluctuation autocorrelation function from multiple timeseries of different length.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> N_k = [1000, 2000, 3000, 4000, 5000]\n",
      ">>> tau = 5.0 # exponential relaxation time\n",
      ">>> A_kn = [ testsystems.correlated_timeseries_example(N=N, tau=tau) for N in N_k ]\n",
      ">>> C_n = normalized_fluctuation_correlation_function_multiple(A_kn, N_max=25)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "692T\n",
      "Determine the indices of an uncorrelated subsample of the data.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "A_t : np.ndarray\n",
      "    A_t[t] is the t-th value of timeseries A(t).  Length is deduced from vector.\n",
      "g : float, optional\n",
      "    if provided, the statistical inefficiency g is used to subsample the timeseries -- otherwise it will be computed (default: None)\n",
      "fast : bool, optional, default=False\n",
      "    fast can be set to True to give a less accurate but very quick estimate (default: False)\n",
      "conservative : bool, optional, default=False\n",
      "    if set to True, uniformly-spaced indices are chosen with interval ceil(g), where\n",
      "    g is the statistical inefficiency.  Otherwise, indices are chosen non-uniformly with interval of\n",
      "    approximately g in order to end up with approximately T/g total indices\n",
      "verbose : bool, optional, default=False\n",
      "    if True, some output is printed\n",
      "\n",
      "Returns\n",
      "-------\n",
      "indices : list of int\n",
      "    the indices of an uncorrelated subsample of the data\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The statistical inefficiency is computed with the function computeStatisticalInefficiency().\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Subsample a correlated timeseries to extract an effectively uncorrelated dataset.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> A_t = testsystems.correlated_timeseries_example(N=10000, tau=5.0) # generate a test correlated timeseries\n",
      ">>> indices = subsample_correlated_data(A_t) # compute indices of uncorrelated timeseries\n",
      ">>> A_n = A_t[indices] # extract uncorrelated samples\n",
      "\n",
      "Extract uncorrelated samples from multiple timeseries data from the same process.\n",
      "\n",
      ">>> # Generate multiple correlated timeseries data of different lengths.\n",
      ">>> T_k = [1000, 2000, 3000, 4000, 5000]\n",
      ">>> K = len(T_k) # number of timeseries\n",
      ">>> tau = 5.0 # exponential relaxation time\n",
      ">>> A_kt = [ testsystems.correlated_timeseries_example(N=T, tau=tau) for T in T_k ] # A_kt[k] is correlated timeseries k\n",
      ">>> # Estimate statistical inefficiency from all timeseries data.\n",
      ">>> g = statistical_inefficiency_multiple(A_kt)\n",
      ">>> # Count number of uncorrelated samples in each timeseries.\n",
      ">>> N_k = np.array([ len(subsample_correlated_data(A_t, g=g)) for A_t in A_kt ]) # N_k[k] is the number of uncorrelated samples in timeseries k\n",
      ">>> N = N_k.sum() # total number of uncorrelated samples\n",
      ">>> # Subsample all trajectories to produce uncorrelated samples\n",
      ">>> A_kn = [ A_t[subsample_correlated_data(A_t, g=g)] for A_t in A_kt ] # A_kn[k] is uncorrelated subset of trajectory A_kt[t]\n",
      ">>> # Concatenate data into one timeseries.\n",
      ">>> A_n = np.zeros([N], np.float32) # A_n[n] is nth sample in concatenated set of uncorrelated samples\n",
      ">>> A_n[0:N_k[0]] = A_kn[0]\n",
      ">>> for k in range(1,K): A_n[N_k[0:k].sum():N_k[0:k+1].sum()] = A_kn[k]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "693T\n",
      "Automatically detect equilibrated region of a dataset using a heuristic that maximizes number of effectively uncorrelated samples.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "A_t : np.ndarray\n",
      "    timeseries\n",
      "nskip : int, optional, default=1\n",
      "    number of samples to sparsify data by in order to speed equilibration detection\n",
      "\n",
      "Returns\n",
      "-------\n",
      "t : int\n",
      "    start of equilibrated data\n",
      "g : float\n",
      "    statistical inefficiency of equilibrated data\n",
      "Neff_max : float\n",
      "    number of uncorrelated samples\n",
      "\n",
      "Notes\n",
      "-----\n",
      "If your input consists of some period of equilibration followed by\n",
      "a constant sequence, this function treats the trailing constant sequence\n",
      "as having Neff = 1.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Determine start of equilibrated data for a correlated timeseries.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> A_t = testsystems.correlated_timeseries_example(N=1000, tau=5.0) # generate a test correlated timeseries\n",
      ">>> [t, g, Neff_max] = detect_equilibration(A_t) # compute indices of uncorrelated timeseries\n",
      "\n",
      "Determine start of equilibrated data for a correlated timeseries with a shift.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> A_t = testsystems.correlated_timeseries_example(N=1000, tau=5.0) + 2.0 # generate a test correlated timeseries\n",
      ">>> B_t = testsystems.correlated_timeseries_example(N=10000, tau=5.0) # generate a test correlated timeseries\n",
      ">>> C_t = np.concatenate([A_t, B_t])\n",
      ">>> [t, g, Neff_max] = detect_equilibration(C_t, nskip=50) # compute indices of uncorrelated timeseries\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "694T\n",
      "Compute the (cross) statistical inefficiency of (two) timeseries.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "A_n : np.ndarray, float\n",
      "    A_n[n] is nth value of timeseries A.  Length is deduced from vector.\n",
      "mintime : int, optional, default=3\n",
      "    minimum amount of correlation function to compute (default: 3)\n",
      "    The algorithm terminates after computing the correlation time out to mintime when the\n",
      "    correlation function first goes negative.  Note that this time may need to be increased\n",
      "    if there is a strong initial negative peak in the correlation function.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "g : np.ndarray,\n",
      "    g is the estimated statistical inefficiency (equal to 1 + 2 tau, where tau is the correlation time).\n",
      "    We enforce g >= 1.0.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The same timeseries can be used for both A_n and B_n to get the autocorrelation statistical inefficiency.\n",
      "The fast method described in Ref [1] is used to compute g.\n",
      "\n",
      "References\n",
      "----------\n",
      "[1] J. D. Chodera, W. C. Swope, J. W. Pitera, C. Seok, and K. A. Dill. Use of the weighted\n",
      "    histogram analysis method for the analysis of simulated and parallel tempering simulations.\n",
      "    JCTC 3(1):26-41, 2007.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "695T\n",
      "Automatically detect equilibrated region of a dataset using a heuristic that maximizes number of effectively uncorrelated samples.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "A_t : np.ndarray\n",
      "    timeseries\n",
      "\n",
      "bs_nodes : int > 4\n",
      "    number of geometrically distributed binary search nodes\n",
      "\n",
      "Returns\n",
      "-------\n",
      "t : int\n",
      "    start of equilibrated data\n",
      "g : float\n",
      "    statistical inefficiency of equilibrated data\n",
      "Neff_max : float\n",
      "    number of uncorrelated samples\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Finds the discard region (t) by a binary search on the range of\n",
      "possible lagtime values, with logarithmic spacings.  This will give\n",
      "a local maximum.  The global maximum is not guaranteed, but will\n",
      "likely be found if the N_eff[t] varies smoothly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "696T\n",
      "Convert KxKxN_max array to KxN max array\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kln : np.ndarray, float, shape=(KxLxN_max)\n",
      "N_k : np.array, optional\n",
      "    the N_k matrix from the previous formatting form\n",
      "cleanup : bool, optional\n",
      "    optional command to clean up, since u_kln can get very large\n",
      "\n",
      "Returns\n",
      "-------\n",
      "u_kn : np.ndarray, float, shape=(LxN)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "697T\n",
      "Convert KxN_max array to N array\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "u_kn : np.ndarray, float, shape=(KxN_max)\n",
      "N_k : np.array, optional\n",
      "    the N_k matrix from the previous formatting form\n",
      "cleanup : bool, optional\n",
      "    optional command to clean up, since u_kln can get very large\n",
      "\n",
      "Returns\n",
      "-------\n",
      "u_n : np.ndarray, float, shape=(N)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "698T\n",
      "Typecheck the size, shape and dtype of a numpy array, with optional\n",
      "casting.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "val : {np.ndaraay, None}\n",
      "    The array to check\n",
      "dtype : {nd.dtype, str}\n",
      "    The dtype you'd like the array to have\n",
      "ndim : int\n",
      "    The number of dimensions you'd like the array to have\n",
      "name : str\n",
      "    name of the array. This is used when throwing exceptions, so that\n",
      "    we can describe to the user which array is messed up.\n",
      "length : int, optional\n",
      "    How long should the array be?\n",
      "can_be_none : bool\n",
      "    Is ``val == None`` acceptable?\n",
      "shape : tuple, optional\n",
      "    What should be shape of the array be? If the provided tuple has\n",
      "    Nones in it, those will be semantically interpreted as matching\n",
      "    any length in that dimension. So, for example, using the shape\n",
      "    spec ``(None, None, 3)`` will ensure that the last dimension is of\n",
      "    length three without constraining the first two dimensions\n",
      "warn_on_cast : bool, default=True\n",
      "    Raise a warning when the dtypes don't match and a cast is done.\n",
      "add_newaxis_on_deficient_ndim : bool, default=True\n",
      "    Add a new axis to the beginining of the array if the number of\n",
      "    dimensions is deficient by one compared to your specification. For\n",
      "    instance, if you're trying to get out an array of ``ndim == 3``,\n",
      "    but the user provides an array of ``shape == (10, 10)``, a new axis will\n",
      "    be created with length 1 in front, so that the return value is of\n",
      "    shape ``(1, 10, 10)``.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The returned value will always be C-contiguous.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "typechecked_val : np.ndarray, None\n",
      "    If `val=None` and `can_be_none=True`, then this will return None.\n",
      "    Otherwise, it will return val (or a copy of val). If the dtype wasn't right,\n",
      "    it'll be casted to the right shape. If the array was not C-contiguous, it'll\n",
      "    be copied as well.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "699T\n",
      "Compute the log of a sum of exponentiated terms exp(a_n) in a numerically-stable manner.\n",
      "\n",
      "NOTE: this function has been deprecated in favor of logsumexp.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a_n : np.ndarray, shape=(n_samples)\n",
      "    a_n[n] is the nth exponential argument\n",
      "\n",
      "Returns\n",
      "-------\n",
      "a_n : np.ndarray, shape=(n_samples)\n",
      "    a_n[n] is the nth exponential argument\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "_logsum a_n = max_arg + \\log \\sum_{n=1}^N \\exp[a_n - max_arg]\n",
      "\n",
      "where max_arg = max_n a_n.  This is mathematically (but not numerically) equivalent to\n",
      "\n",
      "_logsum a_n = \\log \\sum_{n=1}^N \\exp[a_n]\n",
      "\n",
      "\n",
      "Example\n",
      "-------\n",
      ">>> a_n = np.array([0.0, 1.0, 1.2], np.float64)\n",
      ">>> print('{:.3e}'.format(_logsum(a_n)))\n",
      "1.951e+00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "700T\n",
      "Compute the log of the sum of exponentials of input elements.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a : array_like\n",
      "    Input array.\n",
      "axis : None or int, optional, default=None\n",
      "    Axis or axes over which the sum is taken. By default `axis` is None,\n",
      "    and all elements are summed.\n",
      "b : array-like, optional\n",
      "    Scaling factor for exp(`a`) must be of the same shape as `a` or\n",
      "    broadcastable to `a`.\n",
      "use_numexpr : bool, optional, default=True\n",
      "    If True, use the numexpr library to speed up the calculation, which\n",
      "    can give a 2-4X speedup when working with large arrays.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "res : ndarray\n",
      "    The result, ``log(sum(exp(a)))`` calculated in a numerically\n",
      "    more stable way. If `b` is given then ``log(sum(b*exp(a)))``\n",
      "    is returned.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "numpy.logaddexp, numpy.logaddexp2, scipy.special.logsumexp\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This is based on ``scipy.special.logsumexp`` but with optional numexpr\n",
      "support for improved performance.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "701T\n",
      "Check the weight matrix W is properly normalized. The sum over N should be 1, and the sum over k by N_k should aslo be 1\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "W : np.ndarray, shape=(N, K), dtype='float'\n",
      "    The normalized weight matrix for snapshots and states.\n",
      "    W[n, k] is the weight of snapshot n in state k.\n",
      "N_k : np.ndarray, shape=(K), dtype='int'\n",
      "    N_k[k] is the number of samples from state k.\n",
      "tolerance : float, optional, default=1.0e-4\n",
      "    Tolerance for checking equality of sums\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None : NoneType\n",
      "    Returns a None object if test passes\n",
      "\n",
      "Raises\n",
      "------\n",
      "ParameterError\n",
      "    Appropriate message if W is not normalized within tolerance.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "702T\n",
      "A function that when zeroed is equivalent to the solution of\n",
      "the Bennett acceptance ratio.\n",
      "\n",
      "from http://journals.aps.org/prl/pdf/10.1103/PhysRevLett.91.140601\n",
      "\n",
      "    D_F = M + w_F - Delta F\n",
      "    D_R = M + w_R - Delta F\n",
      "\n",
      "we want:\n",
      "\n",
      "    \\sum_N_F (1+exp(D_F))^-1 = \\sum N_R N_R <(1+exp(-D_R))^-1>\n",
      "    ln \\sum N_F (1+exp(D_F))^-1>_F = \\ln \\sum N_R exp((1+exp(-D_R))^(-1)>_R\n",
      "    ln \\sum N_F (1+exp(D_F))^-1>_F - \\ln \\sum N_R exp((1+exp(-D_R))^(-1)>_R = 0\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "w_F : np.ndarray\n",
      "    w_F[t] is the forward work value from snapshot t.\n",
      "    t = 0...(T_F-1)  Length T_F is deduced from vector.\n",
      "w_R : np.ndarray\n",
      "    w_R[t] is the reverse work value from snapshot t.\n",
      "    t = 0...(T_R-1)  Length T_R is deduced from vector.\n",
      "DeltaF : float\n",
      "    Our current guess\n",
      "\n",
      "Returns\n",
      "-------\n",
      "fzero : float\n",
      "    a variable that is zeroed when DeltaF satisfies bar.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Compute free energy difference between two specified samples of work values.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> [w_F, w_R] = testsystems.gaussian_work_example(mu_F=None, DeltaF=1.0, seed=0)\n",
      ">>> DeltaF = bar_zero(w_F, w_R, 0.0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "703T\n",
      "Compute free energy difference using the Bennett acceptance ratio (BAR) method.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "w_F : np.ndarray\n",
      "    w_F[t] is the forward work value from snapshot t.\n",
      "    t = 0...(T_F-1)  Length T_F is deduced from vector.\n",
      "w_R : np.ndarray\n",
      "    w_R[t] is the reverse work value from snapshot t.\n",
      "    t = 0...(T_R-1)  Length T_R is deduced from vector.\n",
      "DeltaF : float, optional, default=0.0\n",
      "    DeltaF can be set to initialize the free energy difference with a guess\n",
      "compute_uncertainty : bool, optional, default=True\n",
      "    if False, only the free energy is returned\n",
      "uncertainty_method : string, optional, default=''BAR''\n",
      "    There are two possible uncertainty estimates for BAR.  One agrees with MBAR for two states exactly,\n",
      "    and is indicated by \"MBAR\". The other estimator, which is the one originally derived for BAR, only\n",
      "    agrees with MBAR in the limit of good overlap, and is designated 'BAR'\n",
      "    See code comments below for derivations of the two methods.\n",
      "maximum_iterations : int, optional, default=500\n",
      "    Can be set to limit the maximum number of iterations performed\n",
      "relative_tolerance : float, optional, default=1E-11\n",
      "    Can be set to determine the relative tolerance convergence criteria (defailt 1.0e-11)\n",
      "verbose : bool\n",
      "    Should be set to True if verbse debug output is desired (default False)\n",
      "method: str, optional, defualt='false-position'\n",
      "    Choice of method to solve bar nonlinear equations: one of 'bisection', 'self-consistent-iteration' or 'false-position' (default : 'false-position').\n",
      "iterated_solution: bool, optional, default=True\n",
      "    whether to fully solve the optimized bar equation to consistency, or to stop after one step, to be\n",
      "    equivalent to transition matrix sampling.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "dict\n",
      "    'Delta_f' : float\n",
      "        Free energy difference\n",
      "    'dDelta_f' : float\n",
      "        Estimated standard deviation of free energy difference\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "[1] Shirts MR, Bair E, Hooker G, and Pande VS. Equilibrium free energies from nonequilibrium\n",
      "measurements using maximum-likelihood methods. PRL 91(14):140601, 2003.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The false position method is used to solve the implicit equation.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Compute free energy difference between two specified samples of work values.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> [w_F, w_R] = testsystems.gaussian_work_example(mu_F=None, DeltaF=1.0, seed=0)\n",
      ">>> results = bar(w_F, w_R)\n",
      ">>> print('Free energy difference is {:.3f} +- {:.3f} kT'.format(results['Delta_f'], results['dDelta_f']))\n",
      "Free energy difference is 1.088 +- 0.050 kT\n",
      "\n",
      "Test completion of various other schemes.\n",
      "\n",
      ">>> results = bar(w_F, w_R, method='self-consistent-iteration')\n",
      ">>> results = bar(w_F, w_R, method='false-position')\n",
      ">>> results = bar(w_F, w_R, method='bisection')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "704T\n",
      "Compute overlap between forward and backward ensembles (using MBAR definition of overlap)\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "w_F : np.ndarray\n",
      "    w_F[t] is the forward work value from snapshot t.\n",
      "    t = 0...(T_F-1)  Length T_F is deduced from vector.\n",
      "w_R : np.ndarray\n",
      "    w_R[t] is the reverse work value from snapshot t.\n",
      "    t = 0...(T_R-1)  Length T_R is deduced from vector.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "overlap : float\n",
      "    The overlap: 0 denotes no overlap, 1 denotes complete overlap\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "705T\n",
      "Estimate free energy difference using one-sided (unidirectional) exponential averaging (EXP).\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "w_F : np.ndarray, float\n",
      "    w_F[t] is the forward work value from snapshot t.  t = 0...(T-1)  Length T is deduced from vector.\n",
      "compute_uncertainty : bool, optional, default=True\n",
      "    if False, will disable computation of the statistical uncertainty (default: True)\n",
      "is_timeseries : bool, default=False\n",
      "    if True, correlation in data is corrected for by estimation of statisitcal inefficiency (default: False)\n",
      "    Use this option if you are providing correlated timeseries data and have not subsampled the data to produce uncorrelated samples.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "dict_vals: dict[float]\n",
      "    Dictionary with keys `Delta_f` and `dDelta_f` for the free energy difference and its\n",
      "    esimated deviation, respectively.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "If you are prodividing correlated timeseries data, be sure to set the 'timeseries' flag to True\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "Compute the free energy difference given a sample of forward work values.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> [w_F, w_R] = testsystems.gaussian_work_example(mu_F=None, DeltaF=1.0, seed=0)\n",
      ">>> results = exp(w_F)\n",
      ">>> print('Forward free energy difference is {:.3f} +- {:.3f} kT'.format(results['Delta_f'], results['dDelta_f']))\n",
      "Forward free energy difference is 1.088 +- 0.076 kT\n",
      ">>> results = exp(w_R)\n",
      ">>> print('Reverse free energy difference is {:.3f} +- {:.3f} kT'.format(results['Delta_f'], results['dDelta_f']))\n",
      "Reverse free energy difference is -1.073 +- 0.082 kT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "706T\n",
      "Estimate free energy difference using gaussian approximation to one-sided (unidirectional) exponential averaging.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "w_F : np.ndarray, float\n",
      "    w_F[t] is the forward work value from snapshot t.  t = 0...(T-1)  Length T is deduced from vector.\n",
      "compute_uncertainty : bool, optional, default=True\n",
      "    if False, will disable computation of the statistical uncertainty (default: True)\n",
      "is_timeseries : bool, default=False\n",
      "    if True, correlation in data is corrected for by estimation of statisitcal inefficiency (default: False)\n",
      "    Use this option if you are providing correlated timeseries data and have not subsampled the data to\n",
      "    produce uncorrelated samples.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "Results dictionary with keys:\n",
      "    'Delta_f' : float\n",
      "        Free energy difference between the two states\n",
      "    'dDelta_f' : float\n",
      "        Estimated standard deviation of free energy difference between the two states\n",
      "\n",
      "Notes\n",
      "-----\n",
      "If you are providing correlated timeseries data, be sure to set the 'timeseries' flag to ``True``\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Compute the free energy difference given a sample of forward work values.\n",
      "\n",
      ">>> from pymbar import testsystems\n",
      ">>> [w_F, w_R] = testsystems.gaussian_work_example(mu_F=None, DeltaF=1.0, seed=0)\n",
      ">>> results = exp_gauss(w_F)\n",
      ">>> print('Forward Gaussian approximated free energy difference is {:.3f} +- {:.3f} kT'.format(results['Delta_f'], results['dDelta_f']))\n",
      "Forward Gaussian approximated free energy difference is 1.049 +- 0.089 kT\n",
      ">>> results = exp_gauss(w_R)\n",
      ">>> print('Reverse Gaussian approximated free energy difference is {:.3f} +- {:.3f} kT'.format(results['Delta_f'], results['dDelta_f']))\n",
      "Reverse Gaussian approximated free energy difference is -1.073 +- 0.080 kT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "707T\n",
      "Helper function to compute unique transcripts UMIs from\n",
      "a given list of transcripts. The function using an offset (genomic coordinates) \n",
      "where all UMIs will be grouped together by a grouping function and with a certain\n",
      "number of mismatches allowed (hamming distance)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "708T\n",
      "The functions parses the reads in BAM format\n",
      "that have been annotated and demultiplexed (containing spatial barcode).\n",
      "It then groups them by gene-barcode to count reads accounting for duplicates\n",
      "using the UMIs (clustering them suing the strand and start position). \n",
      "It outputs the records in a matrix of counts in TSV format and BED format and it also \n",
      "writes out some statistics.\n",
      ":param input_file: the file with the annotated-demultiplexed records in BAM format\n",
      ":param qa_stats: the Stats object to add some stats (THIS IS PASSED BY REFERENCE)\n",
      ":param gff_filename: the annotation reference file\n",
      ":param umi_cluster_algorithm: the clustering algorithm to cluster UMIs\n",
      ":param umi_allowed_mismatches: the number of miss matches allowed to remove\n",
      "                              duplicates by UMIs\n",
      ":param umi_counting_offset: the number of bases allowed as offset (start position) when counting UMIs\n",
      ":param diable_umi: when True the UMI filtering step will not be performed\n",
      ":param output_folder: path to place the output files\n",
      ":param output_template: the name of the dataset\n",
      ":param verbose: True if we can to collect the stats in the logger\n",
      ":type input_file: str\n",
      ":type gff_filename: str\n",
      ":type umi_cluster_algorithm: str\n",
      ":type umi_allowed_mismatches: boolean\n",
      ":type umi_counting_offset: integer\n",
      ":type diable_umi: bool\n",
      ":type output_folder: str\n",
      ":type output_template: str\n",
      ":type verbose: bool\n",
      ":raises: RuntimeError,ValueError,OSError,CalledProcessError\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "709T\n",
      "Heng Li's fasta/fastq reader function.\n",
      "# https://github.com/lh3/readfq/blob/master/readfq.py\n",
      "# Unlicensed. \n",
      "Parses fastq records from a file using a generator approach.\n",
      ":param fp: opened file descriptor\n",
      ":returns an iterator over tuples (name,sequence,quality)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "710T\n",
      "Fastq writing generator sink.\n",
      "Send a (header, sequence, quality) triple to the instance to write it to\n",
      "the specified file pointer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "711T\n",
      "Function snippet and modified from CutAdapt \n",
      "https://github.com/marcelm/cutadapt/\n",
      "\n",
      "Copyright (c) 2010-2016 Marcel Martin <marcel.martin@scilifelab.se>\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in\n",
      "all copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN C\n",
      "\n",
      "Find the position at which to trim a low-quality end from a nucleotide sequence.\n",
      "\n",
      "Qualities are assumed to be ASCII-encoded as chr(qual + base).\n",
      "\n",
      "The algorithm is the same as the one used by BWA within the function\n",
      "'bwa_trim_read':\n",
      "- Subtract the cutoff value from all qualities.\n",
      "- Compute partial sums from all indices to the end of the sequence.\n",
      "- Trim sequence at the index at which the sum is minimal.\n",
      "\n",
      "This variant works on NextSeq data.\n",
      "With Illumina NextSeq, bases are encoded with two colors. 'No color' (a\n",
      "dark cycle) usually means that a 'G' was sequenced, but that also occurs\n",
      "when sequencing falls off the end of the fragment. The read then contains\n",
      "a run of high-quality G bases in the end.\n",
      "This routine works as the one above, but counts qualities belonging to 'G'\n",
      "bases as being equal to cutoff - 1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "712T\n",
      "Quality trims a fastq read using a BWA approach.\n",
      "It returns the trimmed record or None if the number of bases\n",
      "after trimming is below a minimum.\n",
      ":param sequence: the sequence of bases of the read\n",
      ":param quality: the quality scores of the read\n",
      ":param min_qual the quality threshold to trim (consider a base of bad quality)\n",
      ":param min_length: the minimum length of a valid read after trimming\n",
      ":param phred: the format of the quality string (33 or 64)\n",
      ":type sequence: str\n",
      ":type quality: str\n",
      ":type min_qual: integer\n",
      ":type min_length: integer\n",
      ":type phred: integer\n",
      ":return: A tuple (base, qualities) or (None,None)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "716T\n",
      "Tries to finds clusters of similar UMIs using a naive proximity\n",
      "approach where UMIs are sorted and the ones that are consecutive\n",
      "and has hamming distance below the given number of miss-matches will\n",
      "be clustered together.\n",
      "It returns a list with all the non clustered UMIs, for clusters of \n",
      "multiple UMIs a random one will be selected.\n",
      ":param molecular_barcodes: a list of UMIs\n",
      ":param allowed_mismatches: how much distance we allow between clusters\n",
      ":param method: the type of distance algorithm when clustering \n",
      "               (single more restrictive or complete less restrictive)\n",
      ":type allowed_mismatches: integer\n",
      ":type method: str \n",
      ":return: a list of unique UMIs\n",
      ":rtype: list\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "717T\n",
      "This function has been obtained from\n",
      "https://github.com/CGATOxford/UMI-tools\n",
      "The logic behind the algorithm to cluster UMIs using\n",
      "an adjacent distance matrix is described in \n",
      "http://genome.cshlp.org/content/early/2017/01/18/gr.209601.116.abstract\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "718T\n",
      "Removes the specified nodes from the cluster and returns\n",
      "the remaining nodes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "719T\n",
      "This function has been obtained from\n",
      "https://github.com/CGATOxford/UMI-tools\n",
      "The logic behind the algorithm to cluster UMIs using\n",
      "an adjacent distance matrix is described in \n",
      "http://genome.cshlp.org/content/early/2017/01/18/gr.209601.116.abstract\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "721T\n",
      "Tries to finds clusters of similar UMIs using an affinity based approach. \n",
      "It returns a list with all the non clustered UMIs, for clusters of \n",
      "multiple UMIs a random one will be selected.\n",
      ":param molecular_barcodes: a list of UMIs\n",
      ":return: a list of unique UMIs\n",
      ":rtype: list\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "722T\n",
      "Splits a BAM file in to parts with equal read counts.\n",
      "The number of parts to split the BAM File into equals the \n",
      "number of cores given as input.\n",
      ":param input_bamfile_name: path to the BAM file to be splitted\n",
      ":param temp_dir: path to the folder where to put the created files\n",
      ":param threads: the number of CPU cores to use\n",
      ":retuns: the list of splitted BAM files\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "723T\n",
      "This function converts the input variables \n",
      "(header,sequence,quality,barcode_sequence,umi_sequence)\n",
      "to a unaligned pysam.AlignedSegment with the umi and barcode \n",
      "informations as the following tags:\n",
      "    Tag  Value\n",
      "    \"B0\" barcode_sequence\n",
      "    \"B3\" umi_sequence\n",
      ":param header: string with the header information\n",
      ":param sequence: string with the DNA/RNA sequence\n",
      ":param quality: string with the base calling quality values\n",
      ":param barcode_sequence: string with the barcode sequence\n",
      ":param umi_sequence: string with the unique molecular identifier sequence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "724T\n",
      "Function for merging partial BAM files into one.\n",
      ":param merged_file_name: name of the merged output bam file\n",
      ":param files_to_merge: list with names of the partial bam files to merge\n",
      ":param ubam: indicates unaligned bam file (True or False, default False)\n",
      ":returns: the total number of records\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "725T\n",
      "It splits the input file up into sub-files containing\n",
      "random reads from the input file up to the saturation point. \n",
      "It then calls createDataset.py and retrieve some stats to\n",
      "compute the saturation points information that is then added\n",
      "to the log file.\n",
      ":param nreads: the number of reads present in the annotated_reads file\n",
      ":param annotated_reads: path to a BAM file with the annotated reads\n",
      ":param umi_cluster_algorithm: the clustering algorithm to cluster UMIs\n",
      ":param umi_allowed_mismatches: the number of miss matches allowed to remove\n",
      "                              duplicates by UMIs\n",
      ":param umi_counting_offset: the number of bases allowed as offset when couting UMIs\n",
      ":param diable_umi: when True the UMI filtering step will not be performed\n",
      ":param expName: the name of the dataset\n",
      ":param temp_folder: the path where to put the output files\n",
      ":param saturation_points: a list of saturation points to be used\n",
      ":type nreads: integer\n",
      ":type annotated_reads: str\n",
      ":type umi_cluster_algorithm: str\n",
      ":type umi_allowed_mismatches: boolean\n",
      ":type umi_counting_offset: integer\n",
      ":type diable_umi: bool\n",
      ":type expName: str\n",
      ":type temp_folder: str\n",
      ":type saturation_points: list\n",
      ":raises: RuntimeError\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "726T\n",
      "Checks that a program exists and is executable\n",
      ":param program: the program name\n",
      ":type program: str\n",
      ":return: The program name if the program is in the system and is executable\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "727T\n",
      "Safely remove a file\n",
      ":param filename: the path of the file\n",
      ":type filename: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "728T\n",
      "Safely opens a file\n",
      "For writing mode it removes the previous file if it exits\n",
      "For reading mode it check that the file exists\n",
      ":param filename: the path of the file\n",
      ":param atrib: the file open/write attribute\n",
      ":type filename: str\n",
      ":type atrib: str\n",
      ":return: the file descriptor\n",
      ":raises: IOError\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "729T\n",
      "Checks file exists and is not zero size\n",
      ":param file: a file name\n",
      ":return: True if the file is correct\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "730T\n",
      "Tries to find the STAR binary\n",
      "and makes a system call to get its\n",
      "version and return it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "731T\n",
      "Tries to find the Taggd binary\n",
      "and makes a system call to get its\n",
      "version and return it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "732T\n",
      "Tries to find the HTSeqCount binary\n",
      "and makes a system call to get its\n",
      "version and return it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "733T\n",
      "Checks if the file name is a FIFO\n",
      ":param file_name: a file name\n",
      ":return: True if the file is a FIFO\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "734T\n",
      "This function will perform a sequence alignment using STAR.\n",
      "Mapped and unmapped reads are written to the paths given as\n",
      "parameters. It needs the path of the STAR genome index.\n",
      "It allows to perform the 2-Pass mode.\n",
      "It needs the annotation file to use the on-the-fly mode.\n",
      ":param reverse_reads: file containing reverse reads in BAM format\n",
      ":param ref_map: a path to the genome/transcriptome STAR index\n",
      ":param outputFile: the name of the SAM/BAM output file to write the alignments to\n",
      ":param annotation: the annotation file in GTF\n",
      ":param outputFolder: the path of the output folder\n",
      ":param trimReverse: the number of bases to trim in the reverse reads (from 5')\n",
      ":param invTrimReverse: number of bases to trim from the 3'\n",
      ":param cores: the number of cores to use to speed up the alignment\n",
      ":param min_intron_size: min allowed intron size when spanning splice junctions\n",
      ":param max_intron size: max allowed intron size when spanning splice junctions \n",
      ":param disable_multimap: if True no multiple alignments will be allowed\n",
      ":param diable_softclipping: it True no local alignment allowed\n",
      ":param twopassMode: True to use the 2-pass mode\n",
      ":param min_length: the min allowed read length (mapped bases)\n",
      ":param include_non_mapped: True to include un-aligned reads in the output\n",
      ":param star_genome_loading: The type of genome sharing for STAR\n",
      ":param star_sort_mem_limit: The BAM sort memory limit for STAR\n",
      ":type reverse_reads: str\n",
      ":type ref_map: str\n",
      ":type outputFile: str\n",
      ":type annotation: str\n",
      ":type outputFolder: str\n",
      ":type trimReverse: int\n",
      ":type invTrimReverse: int\n",
      ":type cores: int\n",
      ":type min_intron_size: int\n",
      ":type max_intron: int\n",
      ":type disable_multimap: bool\n",
      ":type diable_softclipping: bool\n",
      ":type twopassMode: bool\n",
      ":type min_length: str\n",
      ":type include_non_mapped: bool\n",
      ":type star_genome_loading: str\n",
      ":type star_sort_mem_limit: int\n",
      ":raises: RuntimeError,ValueError,OSError,CalledProcessError\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "735T\n",
      "This functions performs a demultiplexing using Taggd. Input reads will be filtered\n",
      "out looking at their barcodes. Only the ones that contain a barcode\n",
      "that is matched in the barcodes files will be kept.\n",
      "Information about the barcode and the array coordinates will be added\n",
      "to the output file. \n",
      ":param reads: a file in FASTQ/BAM format containing reads with barcodes\n",
      ":param idFile: a tab delimited file (BARCODE - X - Y) containing all the barcodes\n",
      ":param mismatches: the number of allowed mismatches\n",
      ":param kmer: the kmer length\n",
      ":param over_hang: the number of bases to allow for overhang\n",
      ":param taggd_metric: the distance metric algorithm (Subglobal, Levensthein or Hamming)\n",
      ":param taggd_multiple_hits_keep_one: when True keep one random hit when multiple candidates\n",
      ":param taggd_trim_sequences: coordinates to trim in the barcode\n",
      ":param outputFilePrefix: location and prefix for the output files\n",
      ":param keep_discarded_files: if True files with the non demultiplexed reads will be generated\n",
      ":type reads: str\n",
      ":type idFile: str\n",
      ":type mismatches: int\n",
      ":type kmer: int\n",
      ":type over_hang: int\n",
      ":type taggd_metric: str\n",
      ":type taggd_multiple_hits_keep_one: bool\n",
      ":type taggd_trim_sequences: list\n",
      ":type outputFilePrefix: str\n",
      ":type keep_discarded_files: bool\n",
      ":raises: RuntimeError,ValueError,OSError,CalledProcessError\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "736T\n",
      "This is taken from the function count_reads_in_features() from the \n",
      "script htseq-count in the HTSeq package version 0.70 \n",
      "The reason to do so is to fix two really small bugs related to the SAM output.\n",
      "The code of the function is small and simple so for now we\n",
      "will use the patched function here. A patch request has been sent\n",
      "to the HTSeq team.\n",
      "The description of the parameters are the same as htseq-count.\n",
      "Two parameters were added to filter out what to write in the sam output\n",
      "\n",
      "The HTSEQ License\n",
      "HTSeq is free software: you can redistribute it and/or modify it under the terms of \n",
      "the GNU General Public License as published by the Free Software Foundation, \n",
      "either version 3 of the License, or (at your option) any later version.\n",
      "\n",
      "This program is distributed in the hope that it will be useful, \n",
      "but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY \n",
      "or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
      "\n",
      "The full text of the GNU General Public License, version 3, \n",
      "can be found here: http://www.gnu.org/licenses/gpl-3.0-standalone.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "737T\n",
      "Annotates a file with mapped reads (BAM) using a modified \n",
      "version of the htseq-count tool. It writes the annotated records to a file.\n",
      "It assumes the input reads (BAM) are single end and do not contain\n",
      "multiple alignments or un-annotated reads.\n",
      ":param mappedReads: path to a BAM file with mapped reads sorted by coordinate\n",
      ":param gtfFile: path to an annotation file in GTF format\n",
      ":param outputFile: where to write the annotated records (BAM)\n",
      ":param outputDiscarded: where to write the non-annotated records (BAM)\n",
      ":param mode: htseq-count overlapping mode (see htseq-count documentation)\n",
      ":param strandness: the type of strandness to use when annotating (yes, no or reverse)\n",
      ":param htseq_no_ambiguous: true if we want to discard ambiguous annotations\n",
      ":param include_non_annotated: true if we want to include \n",
      "non annotated reads as __no_feature in the output\n",
      ":param outputFile: the name/path to the output file\n",
      ":type mappedReads: str\n",
      ":type gtfFile: str\n",
      ":type outputFile: str\n",
      ":type outputDiscarded: str\n",
      ":type mode: str\n",
      ":type strandness: str\n",
      ":type htseq_no_ambiguos: boolean\n",
      ":type include_non_annotated: str\n",
      ":type outputFile: str\n",
      ":raises: RuntimeError, ValueError\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "738T\n",
      "Correct minor misalignments in retention time not detected by the\n",
      "pre-processing software.\n",
      "\n",
      "These misalignments might have become clearer during PeakFilter's\n",
      "pipeline.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "    means      -- perform the correction over mean columns instead\n",
      "                  of each sample replicate? [default: False]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "739T\n",
      "Correct retention time misalignment in the given feature cluster.\n",
      "\n",
      "Keyword Arguments:\n",
      "    featureCluster -- frames with the same feature cluster ID\n",
      "    parameters     -- LipidFinder's PeakFilter parameters instance\n",
      "    means          -- perform the correction over mean columns\n",
      "                      instead of each sample replicate?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "740T\n",
      "Correct retention time misalignment in the given sample.\n",
      "\n",
      "Keyword Arguments:\n",
      "    intensity    -- intensity per frame and sample's replicate\n",
      "    rtDiff       -- time differences between consecutive frames\n",
      "    parameters   -- LipidFinder's PeakFilter parameters instance\n",
      "    repsPerGroup -- number of replicates per sample\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "741T\n",
      "Combine the intensities of lower intensity features into the most\n",
      "intense feature within feature clusters.\n",
      "\n",
      "For all features in all replicates the intensities of lower\n",
      "intensity features (peak frames) within feature clusters are\n",
      "combined into the most intense feature (peak centre) where they are\n",
      "part of the same peak. Wide peaks and leading and trailing tails\n",
      "that are indicative of contaminants are also removed.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "742T\n",
      "Process the feature cluster.\n",
      "\n",
      "Keyword Arguments:\n",
      "    featureCluster -- feature cluster dataframe\n",
      "    parameters     -- LipidFinder's PeakFilter parameters instance\n",
      "    rtArray        -- array of retention times from source data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "743T\n",
      "Process the each sample replicate of the feature.\n",
      "\n",
      "Keyword Arguments:\n",
      "    repFeature -- sample replicate intensities\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "    rtArray    -- array of retention times from source data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "744T\n",
      "Analyse feature peak.\n",
      "\n",
      "Keyword Arguments:\n",
      "    parameters  -- LipidFinder's PeakFilter parameters instance\n",
      "    intensities -- array of feature peak intensities\n",
      "    repRT       -- array of retention times of the sample replicate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "745T\n",
      "Check for the low solvent range of the given feature.\n",
      "\n",
      "Keyword Arguments:\n",
      "    parameters      -- LipidFinder's PeakFilter parameters instance\n",
      "    peakCategory    -- frame category array\n",
      "    intensities     -- array of feature peak intensities\n",
      "    peakLowestIndex -- lowest index of the peak\n",
      "    lowestIndex     -- lowest index of the feature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "746T\n",
      "Check for the high solvent range of the given feature.\n",
      "\n",
      "Keyword Arguments:\n",
      "    parameters       -- LipidFinder's PeakFilter parameters instance\n",
      "    peakCategory     -- frame category array\n",
      "    intensities      -- array of feature peak intensities\n",
      "    peakHighestIndex -- highest index of the peak\n",
      "    highestIndex     -- highest index of the feature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "747T\n",
      "Remove isotopes of parent analytes.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "748T\n",
      "Return an array with the tagged parents and their corresponding\n",
      "isotopes in the same order as in the given sample array.\n",
      "\n",
      "Keyword Arguments:\n",
      "    array      -- array with m/z, retention time (RT), sample's\n",
      "                  intensity mean and index of the original dataframe\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "749T\n",
      "Removes outliers from a set of replicates on a row by row basis.\n",
      "\n",
      "All sample replicates may be discarded if the relative standard\n",
      "deviation (RSD) of the remaining replicates cannot be reduced below\n",
      "the established threshold.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data         -- LFDataFrame instance\n",
      "    parameters   -- LipidFinder's PeakFilter parameters instance\n",
      "    src          -- columns where to check for outliers: \"samples\"\n",
      "                    or \"blanks\" [default: \"samples\"]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "750T\n",
      "Return the mean of non-zero values of an array.\n",
      "\n",
      "Keyword Arguments:\n",
      "    inArray -- intensities of one frame\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "751T\n",
      "Return the standard deviation of non-zero values of an array.\n",
      "\n",
      "Keyword Arguments:\n",
      "    inArray -- intensities of one frame\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "752T\n",
      "Remove any value out of the parameters' thresholds.\n",
      "\n",
      "Keyword Arguments:\n",
      "    inArray    -- intensities of one frame\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "753T\n",
      "Remove in-source ion fragments.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "754T\n",
      "Return an index list corresponding to common in-source fragments\n",
      "in the given sample array.\n",
      "\n",
      "Return the index list of all 'array' features that match the m/z\n",
      "values provided in 'fragments' for which there is at least another\n",
      "feature above the given m/z cut-off at the same retention time (RT).\n",
      "All m/z and RT matching are computed within tolerance.\n",
      "\n",
      "Keyword arguments:\n",
      "    array      -- array with m/z, RT and index of the original\n",
      "                  dataframe\n",
      "    fragments  -- in-source fragments to be removed\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "755T\n",
      "Return an index list corresponding to the features in the given\n",
      "sample array that have been fragmented.\n",
      "\n",
      "Return the index list of all 'array' features that have lost one of\n",
      "the m/z in 'losses' and their complete counterpart is present in the\n",
      "data. The features to be removed must be higher than the given\n",
      "cut-off. All m/z and retention time (RT) matching are computed\n",
      "within tolerance.\n",
      "\n",
      "Keyword arguments:\n",
      "    array      -- array with m/z, RT and index of the original\n",
      "                  dataframe\n",
      "    losses     -- neutral losses to subtract in order to detect\n",
      "                  fragmented features\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "756T\n",
      "Cluster m/z artifacts that differ from each other by a mass less\n",
      "than the defined tolerance.\n",
      "\n",
      "Hierarchical clustering is employed to group the ions into the most\n",
      "appropriate groups. Mass clusters are assigned an arbitrary unique\n",
      "integer identifier.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "757T\n",
      "Cluster contiguous ions within the same mass cluster where each\n",
      "member is separated by a retention time difference of less than\n",
      "'maxRTDiffAdjFrame' (in 'parameters').\n",
      "\n",
      "Feature clusters are identified and each assigned an arbitrary\n",
      "unique integer identifier.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "758T\n",
      "Return ratio (%) of quality control (QC) samples with a relative\n",
      "standard deviation (RSD) lower than QCRSD's lower part to QC samples\n",
      "with RSD lower than QCRSD's upper part.\n",
      "\n",
      "The mean and RSD of the set of QC samples for each row is calculated\n",
      "and added at the end of the input dataframe.\n",
      "\"QCRSD\" is the key of one of the parameters in \"parameters\".\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "759T\n",
      "Remove the effect of solvent samples on biological samples.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "760T\n",
      "Discard features (rows) where intensity is below the threshold.\n",
      "\n",
      "The threshold is set by \"intenSignifCutOff\" parameter.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "761T\n",
      "Remove features identified as salt clusters based on m/z defect.\n",
      "\n",
      "Only take into account frames under the given retention time (RT)\n",
      "threshold, excluding m/z values in the inclusion list.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "762T\n",
      "Calculate and add the mean of the intensity of each sample\n",
      "replicates in the input dataframe.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "763T\n",
      "Remove straight m/z contaminants included in the contaminants CSV\n",
      "file from input data.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "764T\n",
      "Retain only the highest intensity adduct from those found in\n",
      "the input data.\n",
      "\n",
      "Adduct pairs for the appropriate mode are imported from the adducts\n",
      "CSV file. An offset is generated for each given pair based upon\n",
      "their mass difference. The m/z column in 'data' is searched for\n",
      "pairs differing by this offset with the same retention time (within\n",
      "tolerance).\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "765T\n",
      "Detect pairs of adducts in the given sample replicate and set to\n",
      "zero the lowest intensity of each pair.\n",
      "\n",
      "The m/z and retention time (RT) matches are done within a tolerance.\n",
      "\n",
      "Keyword Arguments:\n",
      "    replicate    -- replicate's intensities\n",
      "    pairs        -- list of paired adducts\n",
      "    adducts      -- adducts information\n",
      "    mzArray      -- sample replicate's m/z values\n",
      "    rtArray      -- sample replicate's rt values\n",
      "    parameters   -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "766T\n",
      "Detect lipid and contaminant stacks and delete all ions present\n",
      "(in lipid stacks the parent is retained).\n",
      "\n",
      "A stack is a series of ions differing in m/z by a user-defined fixed\n",
      "mass shift. Lipid stacks elute at same retention time (RT) whilst\n",
      "contaminant stacks increase their RT as overall m/z increases.\n",
      "Firstly, the m/z is checked for a lipid stack and if a stack is\n",
      "present, all ions except the parent are deleted and the next m/z is\n",
      "checked. If no lipid stack is found then the m/z is checked for\n",
      "contaminant stacks. If found, the whole stack including the parent\n",
      "is removed. The list of lipid and contaminant stack mass differences\n",
      "is imported from the stacks CSV file.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "767T\n",
      "Get every feature that matches the stack m/z difference and\n",
      "retention time gap from the previous feature to shape the\n",
      "contaminant stack.\n",
      "\n",
      "Keyword Arguments:\n",
      "    array      -- array with every feature m/z, retention time (RT)\n",
      "                  and index\n",
      "    index      -- index of the previous feature in 'array'\n",
      "    rtGap      -- RT difference between consecutive features\n",
      "    stackMZ    -- contaminant m/z difference\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "768T\n",
      "Remove ions that elute across a chromatogram for a particular m/z\n",
      "with similar intensities that are likely to be contaminants.\n",
      "\n",
      "The m/z matches are done within a tolerance.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "769T\n",
      "Process each sample mean of the same mass cluster independently.\n",
      "\n",
      "Keyword Arguments:\n",
      "    mzCluster  -- mass cluster dataframe with sample means\n",
      "    rtArray    -- array of retention times from source data\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "770T\n",
      "Remove contaminant from the given sample mean intensities.\n",
      "\n",
      "Keyword Arguments:\n",
      "    sample     -- sample mean series\n",
      "    rtArray    -- array of retention times (RT) from source data\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "771T\n",
      "Return outliers in the input array where values represented by\n",
      "input outliers are discarded.\n",
      "\n",
      "Keyword Arguments:\n",
      "    inArray    -- input array\n",
      "    outliers   -- indices to omit\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "772T\n",
      "Return high outliers in the input array where values represented\n",
      "by input outliers are discarded.\n",
      "\n",
      "Keyword Arguments:\n",
      "    inArray    -- input array\n",
      "    outliers   -- indices to omit\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "774T\n",
      "Return the standard deviation of values of input array excluding\n",
      "the indices in 'outliers'.\n",
      "\n",
      "The returned value has been rounded up to 3 decimal numbers.\n",
      "\n",
      "Keyword Arguments:\n",
      "    inArray  -- input array\n",
      "    outliers -- indices to omit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "775T\n",
      "Return an array of indices of the input array without the indices\n",
      "to omit.\n",
      "\n",
      "Keyword Arguments:\n",
      "    inArray -- input array\n",
      "    toOmit  -- indices to omit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "776T\n",
      "Return the False Discovery Rate (FDR) of the dataset following a\n",
      "target-decoy strategy.\n",
      "\n",
      "The value is calculated based on the number of m/z values of 'data'\n",
      "found in the COMP_DB database from LIPID MAPS, and the number of m/z\n",
      "values of 'data' found in a decoy database, created adding 0.5 Da to\n",
      "every m/z in COMP_DB (a very rare lipid mass defect). FDR is equal\n",
      "to the number of decoy hits divided by the number of target hits.\n",
      "\n",
      "Keyword arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "777T\n",
      "Return the number of matches for the selected database and\n",
      "parameters.\n",
      "\n",
      "Keyword Arguments:\n",
      "    db        -- LIPID MAPS' database\n",
      "    mzStr     -- string with one m/z per line (text file alike)\n",
      "    adducts   -- list of adducts separated by commas\n",
      "    tolerance -- mass tolerance in Daltons (+/- to each m/z)\n",
      "                 [default: 0.001]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "778T\n",
      "Create a summary CSV file containing only the mean sample\n",
      "intensity of each feature within the given retention time window.\n",
      "\n",
      "If 'dst' is not an absolute path, the current working directory will\n",
      "be used as starting point. If \"peakfilter_<polarity>_summary.csv\"\n",
      "file already exists, it will be overwritten without warning.\n",
      "\"<polarity>\" stands for \"positive\" or \"negative\", as stated in the\n",
      "parameters.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "    dst        -- destination directory where the file will be saved\n",
      "                  [default: current working directory]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "779T\n",
      "Assign each mass in either a mass cluster or feature cluster to\n",
      "the mass of the row containing the highest sample mean intensity.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame instance\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "780T\n",
      "Replace the m/z value of each row by the m/z with the highest\n",
      "sample mean intensity.\n",
      "\n",
      "Keyword Arguments:\n",
      "    groupMass  -- mass or feature cluster\n",
      "    parameters -- LipidFinder's PeakFilter parameters instance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "781T\n",
      "Create and export the lipid category scatter plot.\n",
      "\n",
      "Generates a scatter plot of the main lipid categories based on m/z\n",
      "vs retention time (RT) of the input dataframe, which must contain a\n",
      "column named \"Category\" (case sensitive). The plot is saved in the\n",
      "file format selected during the parameter configuration (PDF by\n",
      "default).\n",
      "\n",
      "Keyword arguments:\n",
      "    data       -- LFDataFrame or pandas.DataFrame instance\n",
      "    parameters -- LipidFinder's MS Search parameters instance\n",
      "    dst        -- destination directory where the figure will be\n",
      "                  exported\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "783T\n",
      "Create a summary XLSX file containing only one row per m/z and\n",
      "retention time with the most common lipid category.\n",
      "\n",
      "'data' must have, at least, m/z, retention time (RT), \"Main Class\"\n",
      "and \"Category\" columns.\n",
      "If two or more categories have the same number of matches, the first\n",
      "one to appear in 'data' is chosen, unless it is \"other metabolites\":\n",
      "in that case the second one will be selected. The same rule applies\n",
      "for the main class.\n",
      "If 'dst' is not an absolute path, the current working directory will\n",
      "be used as starting point. If \"mssearch_<db>_summary.xlsx\"\n",
      "file already exists, it will be overwritten without warning.\n",
      "\"<db>\" stands for the selected LIPID MAPS database.\n",
      "\n",
      "Keyword Arguments:\n",
      "    data       -- LFDataFrame or pandas.DataFrame instance\n",
      "    parameters -- LipidFinder's MS Search parameters instance\n",
      "    dst        -- destination directory where the XSLX file will be\n",
      "                  created [default: current working directory]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "784T\n",
      "Return LipidFinder v2.0 name for the selected adduct: some\n",
      "adducts have been renamed to follow the standard nomenclature.\n",
      "\n",
      "Keyword parameters:\n",
      "    adducts -- LipidFinder v1.0 adducts dataframe\n",
      "    index   -- row index\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "786T\n",
      "generate all possible nedit edits of a string. each item has the form\n",
      "((index1, index2), 'A', 'G')  for nedit=2\n",
      "index1 will be replaced by 'A', index2 by 'G'\n",
      "\n",
      "this covers all edits < nedit as well since some of the specified\n",
      "substitutions will not change the base\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "787T\n",
      "Make a directory if it doesn't exist, handling concurrent race conditions.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "788T\n",
      "Generator which gives all four lines if a fastq read as one string\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "789T\n",
      "detects the annotations present in a SAM file, inspecting either the\n",
      "tags or the query names and returns a set of annotations present\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "790T\n",
      "construct a regex that matches possible fields in a transformed file\n",
      "annotations is a set of which keys in BARCODEINFO are present in the file\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "791T\n",
      "Transform input reads to the tagcounts compatible read layout using\n",
      "regular expressions as defined in a transform file. Outputs new format to\n",
      "stdout.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "792T\n",
      "figure out what transform options should be by examining the provided\n",
      "regexes for keywords\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "793T\n",
      "Extract read numbers from old-style fastqs.\n",
      "\n",
      "Handles read 1 and 2 specifications where naming is\n",
      "readname/1 readname/2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "794T\n",
      "Count up evidence for tagged molecules\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "795T\n",
      "Count up evidence for tagged molecules, this implementation assumes the\n",
      "alignment file is coordinate sorted\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "796T\n",
      "Convert a CSV file to a sparse matrix with rows and column names\n",
      "saved as companion files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "797T\n",
      "Counts the number of reads for each cellular barcode\n",
      "\n",
      "Expects formatted fastq files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "798T\n",
      "Counts the number of reads for each UMI\n",
      "\n",
      "Expects formatted fastq files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "799T\n",
      "Returns a set of barcodes with a minimum number of reads\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "800T\n",
      "Guesses at an appropriate barcode cutoff\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "801T\n",
      "Filters reads with non-matching barcodes\n",
      "Expects formatted fastq files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "802T\n",
      "Filters reads with non-matching sample barcodes\n",
      "Expects formatted fastq files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "803T\n",
      "Filters umis with non-ACGT bases\n",
      "Expects formatted fastq files.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "805T\n",
      "Convert fastqtransformed file to output format compatible with\n",
      "kallisto.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "806T\n",
      "Convert a BAM/SAM with fastqtransformed read names to have UMI and\n",
      "cellular barcode tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "807T\n",
      "Demultiplex a fastqtransformed FASTQ file into a FASTQ file for\n",
      "each sample.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "809T\n",
      "Subset a SAM/BAM file, keeping only alignments from given\n",
      "cellular barcodes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "810T\n",
      "Index reference fasta\n",
      ":param reference_path: string path to the reference\n",
      ":return: reference index in list from\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "811T\n",
      "Find all non-N regions in reference sequence ahead of time, for computing jobs in parallel\n",
      "\n",
      ":param ref_path:\n",
      ":param ref_inds:\n",
      ":param n_handling:\n",
      ":param save_output:\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "812T\n",
      "Find which of the non-N regions are going to be used for this job\n",
      "\n",
      ":param in_regions:\n",
      ":param ref_inds:\n",
      ":param my_job:\n",
      ":param n_jobs:\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "813T\n",
      "Returns the index of the mean of a weighted list\n",
      "\n",
      ":param candidate_list: weighted list\n",
      ":return: index of mean\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "814T\n",
      "Return the reverse complement of a string from a DNA strand. Found this method that is slightly faster than\n",
      "biopython. Thanks to this stack exchange post:\n",
      "https://bioinformatics.stackexchange.com/questions/3583/what-is-the-fastest-way-to-get-the-reverse-complement-of-a-dna-sequence-in-pytho\n",
      ":param dna_string: string of DNA, either in string or Seq format\n",
      ":return: the reverse complement of the above string in either string or MutableSeq format\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "815T\n",
      "Finds the largest superset bin of region. Numeric values taken from hts-specs\n",
      "Note: description of this function taken from source code for bamnostic.bai\n",
      "    (https://bamnostic.readthedocs.io/en/latest/_modules/bamnostic/bai.html)\n",
      ":param beg: inclusive beginning position of region\n",
      ":param end: exclusive end position of region\n",
      ":return: distinct bin ID or largest superset bin of region\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "816T\n",
      "If required field variable_to_test is empty, issues an error. Otherwise this does nothing\n",
      "\n",
      ":param variable_to_test: Any input type\n",
      ":param err_string: A string with the error message\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "817T\n",
      "Checks that the filename is not empty and that it is indeed a  file\n",
      "\n",
      ":param filename: file name, string\n",
      ":param err_string: string of the error if it is not a file\n",
      ":param required: If not required, skips the check\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "818T\n",
      "Checks that directory exists and is a directory\n",
      ":param directory: string of the directory path\n",
      ":param err_string: string of the error in case it is not a directory or doesn't exist\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "819T\n",
      "Checks that value is between the lower bound and upper bound, and if not prints an error message\n",
      "(err_string) and exits the program.\n",
      "\n",
      ":param value: float for the value\n",
      ":param lower_bound: float for the upper bound\n",
      ":param upper_bound: float for the lower bound\n",
      ":param err_string: string of the error message to print if the value is out of range\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "820T\n",
      "Takes a parameter length and returns a randomly generated DNA string of that length\n",
      ":param lnth: how long of a string to generate\n",
      ":param seed: Optional seed to produce reproducibly random results\n",
      ":return: randomly generated string\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "821T\n",
      "Clusters a sorted list\n",
      ":param list_to_cluster: a sorted list\n",
      ":param delta: the value to compare list items to\n",
      ":return: a clustered list of values\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "822T\n",
      "Finds the median of a list of data. For this function, the data are expected to be a list of\n",
      "numbers, either float or int.\n",
      ":param datalist: the list of data to find the median of. This should be a set of numbers.\n",
      ":return: The median of the set\n",
      ">>> median([2])\n",
      "2\n",
      ">>> median([2183, 2292, 4064, 4795, 7471, 12766, 14603, 15182, 16803, 18704, 21504, 21677, 23347, 23586, 24612, 24878, 25310, 25993, 26448, 28018, 28352, 28373, 28786, 30037, 31659, 31786, 33487, 33531, 34442, 39138, 39718, 39815, 41518, 41934, 43301])\n",
      "25993\n",
      ">>> median([1,2,4,6,8,12,14,15,17,21])\n",
      "10.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "823T\n",
      "Calculates the absolute value of the median deviation from the median for each element of of a datalist. \n",
      "Then returns the median of these values.\n",
      ":param datalist: A list of data to find the MAD of\n",
      ":return: index of median of the deviations\n",
      ">>> median_absolute_deviation([2183, 2292, 4064, 4795, 7471, 12766, 14603, 15182, 16803, 18704, 21504, 21677, 23347, 23586, 24612, 24878, 25310, 25993, 26448, 28018, 28352, 28373, 28786, 30037, 31659, 31786, 33487, 33531, 34442, 39138, 39718, 39815, 41518, 41934, 43301])\n",
      "7494\n",
      ">>> median_absolute_deviation([1,2,4,6,8,12,14,15,17,21])\n",
      "5.5\n",
      ">>> median_absolute_deviation([0,2])\n",
      "1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "825T\n",
      "Computes the probabilities for fragments with at least 100 pairs supporting it and that are at least 10 median\n",
      "deviations from the median.\n",
      ":param datalist: A list of fragments with counts\n",
      ":return: A list of values that meet the criteria and a list of their associated probabilities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "826T\n",
      "Takes a fasta file, converts it into a dictionary of upper case sequences. Does some basic error checking,\n",
      "like the file is readable and the reference dictionary is not empty\n",
      ":param file: path to a fasta file\n",
      ":return: dictionary form of the sequences indexed by chromosome\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "827T\n",
      "Takes a genomecov file and converts it into a dictionary made up of 'window' sized sections \n",
      "that record the number of GCs and the coverage measure for each section.\n",
      ":param file: path to a genomecov file\n",
      ":param ref_dict: dictionary created from using the process_fasta function\n",
      ":param window: Length of each section of base pairs to count in the reference dictionary\n",
      ":return: dictionary form of genomecov file based on window size and ref_dict data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "828T\n",
      "Takes the dictionary created in process_genomecov and finds the average coverage value.\n",
      "Also ouputs the average coverage value for each window, along with the number of entries in that window.\n",
      ":param bin_dict: dictionary created from using the process_genomecov function\n",
      ":param window: Length of each section of base pairs to count, \n",
      "               should be the same as the window value in process_genomecov\n",
      ":return: Average coverage value for the whole sample, along with average coverage values for each window.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "830T\n",
      "The purpose of this function will be to cull the bacteria created in the model\n",
      ":param percentage: percentage of the population to eliminate\n",
      ":param population: the list of members to cull\n",
      ":return: The list of remaining members\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "831T\n",
      "The purpose of this function is to evolve the initial population of bacteria. All bacteria are stored as\n",
      "Bacterium objects.\n",
      ":param chrom_names: A list of contigs from the original fasta\n",
      ":param reference: string path to the reference fasta file\n",
      ":param pop_size: size of the population to initialize.\n",
      ":return population: returns a list of bacterium objects.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "832T\n",
      "This evolves an existing population by doubling them (binary fission), then introducing random mutation to\n",
      "each member of the population.\n",
      ":param generation: Helps determine the starting point of the numbering system so the bacteria have unique names\n",
      ":param population: A list of fasta files representing the bacteria.\n",
      ":return: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "834T\n",
      "This function attempts to extract the chromosome names from a fasta file\n",
      ":param reference: The fasta file to analyze\n",
      ":return: A list of chromosome names\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "835T\n",
      "Parse a newick phylogeny, provided either via a file or a string. The tree does not need to be bifurcating, and may be rooted or unrooted.\n",
      "Returns a Node object along which sequences may be evolved.  \n",
      "    \n",
      "Trees can either read from a file or given directly to ``read_tree`` as a string. One of these two keyword arguments is required.\n",
      "\n",
      "    1. **file**, the name of the file containing a newick tree for parsing. If this argument is provided in addition to tstring, the tree in the file will be used and tstring will be ignored.\n",
      "    2. **tree**, a newick tree string. If a file is additionally provided, the tstring argument will be ignored.   \n",
      "\n",
      "Optional keyword arguments:\n",
      "    1. **scale_tree** is a float value for scaling all branch lengths by a given multiplier. Default: 1.\n",
      "\n",
      "To implement branch (temporal) heterogeneity, place \"model flags\" at particular nodes within the tree. Model flags can be specified with either underscores (_) or hashtags (#), through one of two paradigms:\n",
      "    + Using trailing and leading symbols, e.g. _flagname_ or #flagname# . Specifying a model flag with this format will cause ALL descendents of that node to also follow this model, unless a new model flag is given downstream.\n",
      "    + Using *only a leading* symbol, e.g. _flagname or #flagname. Specifying a model flag with this format will cause ONLY that branch/edge to use the provided model. Descendent nodes will NOT inherit this model flag. Useful for changing model along a single branch, or towards a single leaf.\n",
      "\n",
      "Model flags may be repeated throughout the tree, but the model associated with each model flag will always be the same. Note that these model flag names **must** have correspondingly named model objects.\n",
      "\n",
      "**IMPORTANT**: Node names must be provided BEFORE a branch length, and model flags be provided AFTER a branch length. For example, this subtree is correct: \"...(taxon1:0.5, taxon2:0.2)<NODENAME>:<BL><MODEL FLAG>)...\". This subtree is *incorrect* and will raise a cryptic error: \"...(taxon1:0.5, taxon2:0.2):<BL><NODENAME><MODEL FLAG>)...\". \n",
      "\n",
      "\n",
      "Examples:\n",
      "    .. code-block:: python\n",
      "        \n",
      "       tree = read_tree(file = \"/path/to/tree/file.tre\")\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762):0.921):0.207);\")\n",
      "       \n",
      "       # Tree containing model flags named m1 and m2, both of which propagate to descendents.\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762_m1_):0.921)_m2_:0.207);\"\n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762#m1#):0.921)#m2#:0.207);\"\n",
      "\n",
      "\n",
      "       # Tree containing model flags named m1 and m2, each of which applies only to that branch.\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660_m1):0.762_m2):0.921):0.207);\"\n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660#m1):0.762#m2):0.921):0.207);\"\n",
      "\n",
      "\n",
      "       # Tree with a node demonstrating how to provide both a node name and model flag\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660)NODENAME:0.762_m1_):0.921):0.207);\" # propagating model flag\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762):0.921)NODENAME:0.207#m1);\" # non-propagating model flag\n",
      "\n",
      "\n",
      "       # Tree containing model flags named m1 and m2, where m1 is branch-specific but m2 is propagating.\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660#m1):0.762#m2#):0.921):0.207);\" \n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660_m1):0.762_m2_):0.921):0.207);\"\n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660_m1):0.762#m2#):0.921):0.207);\"\n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660#m1):0.762_m2_):0.921):0.207);\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "836T\n",
      "Prints a Node object in graphical, nested format. \n",
      "This function takes two arguments:\n",
      "    \n",
      "    1. **tree** is a Node object to print\n",
      "    2. **level** is used internally for printing. DO NOT PROVIDE THIS ARGUMENT.\n",
      "\n",
      "Each node in the tree is represented by a string in the format, \"name   branch.length   model.flag\", and levels are represented by indentation.\n",
      "Names for tree tips are taken directly from the provided tree, and internal node names are assigned automatically by the ``read_tree`` function.\n",
      "The node with a branch length of None will be the root node where sequence evolution will begin.\n",
      "Note that the model.flag field will be None under cases of branch homogeneity.       \n",
      "\n",
      "For example,\n",
      "    .. code-block:: python\n",
      "    \n",
      "       >>> my_tree = newick.read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762):0.921):0.207);\")\n",
      "       >>> print_tree(my_tree)\n",
      "            root None None\n",
      "                t4 0.785 None\n",
      "                    internalNode3 0.207 None\n",
      "                        t3 0.38 None\n",
      "                        internalNode2 0.921 None\n",
      "                            t2 0.806 None\n",
      "                            internalNode1 0.762 None\n",
      "                                t5 0.612 None\n",
      "                                t1 0.66 None\n",
      "    \n",
      "       >>> flagged_tree = newick.read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762_m1_):0.921_m2_):0.207);\")\n",
      "       >>> newick.print_tree(flagged_tree)  \n",
      "             root None None\n",
      "                t4 0.785 None\n",
      "                internalNode3 0.207 None\n",
      "                    t3 0.38 None\n",
      "                    internalNode2 0.921 m2\n",
      "                        t2 0.806 m2\n",
      "                        internalNode1 0.762 m1\n",
      "                            t5 0.612 m1\n",
      "                            t1 0.66 m1\n",
      "\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "838T\n",
      "Read a provided internal node name while parsing the tree from the function _parse_tree.\n",
      "Importantly, internal node names *MAY NOT* contain colons!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "839T\n",
      "Recursively parse a newick tree string and convert to a Node object. \n",
      "Uses the functions _read_branch_length(), _read_leaf(), _read_model_flag() during the recursion.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "840T\n",
      "We add the function fxn to the instance of a certain object.\n",
      "\n",
      "The function can access all attributes stored as self; the first argument\n",
      "of the function should be self like in all normal functions of a class.\n",
      "This is helpful if you want to add a function after creation. example\n",
      "\n",
      "def fxn(self, ...):\n",
      "    self.attribute\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "841T\n",
      "Compute distance matrix of all runs.\n",
      "\n",
      "Computes a n x n distance matrix between all runs of an experiment. The\n",
      "reported distance is 1 minus the Rsquared value (1-R^2) from the linear\n",
      "regression.\n",
      "\n",
      "Args:\n",
      "    exp(MRExperiment): a collection of runs\n",
      "    multipeptides(list(Multipeptide)): a list of\n",
      "        multipeptides containing the matching of precursors across runs.\n",
      "    initial_alignment_cutoff(float): a filtering cutoff (in q-value) to\n",
      "        specify which points should be used for the calculation of the\n",
      "        distance. In general, only identification which are very certain\n",
      "        should be used for this and a q-value of 0.0001 is recommended --\n",
      "        given that there are enough points.\n",
      "\n",
      "Returns:\n",
      "    numpy (n x n) matrix(float): distance matrix\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "842T\n",
      "Determine the optimal integration border by using the shortest path in the MST\n",
      "\n",
      "Args:\n",
      "    selected_pg(list(GeneralPeakGroup)): list of selected peakgroups (e.g. those passing the quality threshold)\n",
      "    target_run(String): run id of the target run (where value is missing)\n",
      "    transformation_collection_(:class:`.LightTransformationData`): structure to hold binary transformations between two different retention time spaces\n",
      "    tree(list(tuple)): a minimum spanning tree (MST) represented as list of edges (for example [('0', '1'), ('1', '2')] ). Node names need to correspond to run ids.\n",
      "\n",
      "Returns:\n",
      "    A tuple of (left_integration_border, right_integration_border)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "843T\n",
      "Determine the optimal integration border by using the shortest distance (direct transformation)\n",
      "\n",
      "Args:\n",
      "    selected_pg(list(GeneralPeakGroup)): list of selected peakgroups (e.g. those passing the quality threshold)\n",
      "    target_run(String): run id of the target run (where value is missing)\n",
      "    transformation_collection_(:class:`.LightTransformationData`): structure to hold binary transformations between two different retention time spaces\n",
      "    mat(numpy matrix): distance matrix for all runs (as returned by algorithms.alignment.AlignmentMST.getDistanceMatrix)\n",
      "    rmap(dict): mapping run ids to matrix columns (e.g. {\"run_0\" : 0, \"run_1\" : 1})\n",
      "\n",
      "Returns:\n",
      "    A tuple of (left_integration_border, right_integration_border)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "844T\n",
      "Determine the optimal integration border by taking the mean of all other peakgroup boundaries using a reference run.\n",
      "\n",
      "Args:\n",
      "    new_exp(AlignmentExperiment): experiment containing the aligned peakgroups\n",
      "    selected_pg(list(GeneralPeakGroup)): list of selected peakgroups (e.g. those passing the quality threshold)\n",
      "    rid(String): current run id\n",
      "    transformation_collection_(:class:`.TransformationCollection`): specifying how to transform between retention times of different runs\n",
      "    border_option(String): one of the following options (\"mean\", \"median\" \"max_width\"), determining how to aggregate multiple peak boundary information\n",
      "\n",
      "Returns:\n",
      "    A tuple of (left_integration_border, right_integration_border) in the retention time space of the _reference_ run\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "845T\n",
      "Return the minimum spanning tree of an undirected graph G.\n",
      "G should be represented in such a way that G[u][v] gives the\n",
      "length of edge u,v, and G[u][v] should always equal G[v][u].\n",
      "The tree is returned as a list of edges.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "846T\n",
      "Convert a tree into a adjacency list\n",
      "\n",
      "Args:\n",
      "    tree(list(tuple)): a tree represented as list of edges (for example [('0', '1'), ('1', '2')] ).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "847T\n",
      "Perform breadth-first-search (BFS) on a given tree\n",
      "\n",
      "Args:\n",
      "    tree(list(tuple)): a tree represented as list of edges (for example [('0', '1'), ('1', '2')] ).\n",
      "    start_node(str): starting node\n",
      "\n",
      "Yields:\n",
      "    node(str): current node during search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "848T\n",
      "Finds a path in an MST from start to one of the end elements\n",
      "\n",
      "The algorithm will look for the shortest path in a minimum spanning tree\n",
      "(MST) to one of the elements contained in end. It will do a breadth-first\n",
      "search (BFS) through the MST to find the first element in \"end\" which has\n",
      "minimal distance to start. If there are multiple elements in \"end\" with\n",
      "equal distance, whichever comes first in the BFS will be chosen.\n",
      "\n",
      "It will then return the path between this element and the start element. \n",
      "\n",
      "Args:\n",
      "    graph(list(tuple)): a graph represented as list of edges (for example [('0', '1'), ('1', '2')] ).\n",
      "    start(str): Starting node\n",
      "    end(list(str)): List of possible end nodes (closest will be chosen)\n",
      "\n",
      "Returns:\n",
      "    path (list(str)) : A path represented as list of nodes to be visited in that order\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "849T\n",
      "Factory function to get the correct writer depending on the file ending\n",
      "\n",
      "Args:\n",
      "    matrix_outfile(str): Filename of output - used to determine output format. Valid formats are .xlsx .xls .csv or .tsv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "850T\n",
      "Builds a peakgroup map based on OpenSWATH output data\n",
      "\n",
      "Compare with mapRow for construction of the key\n",
      "\n",
      "Creates a map of the PeptideName/Charge to the individual multipeptide\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "851T\n",
      "Populate mapping from a single row in the CSV file.\n",
      "\n",
      "Populate the precursors_mapping, sequences_mapping and protein_mapping\n",
      "based on the information in a row in a CSV file. Read the relationship\n",
      "between transition_ids, precursors, peptide sequences and proteins from the\n",
      "CSV input file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "852T\n",
      "Infers a mapping between raw chromatogram files (mzML) and processed feature TSV files\n",
      "\n",
      "Usually one feature file can contain multiple aligned runs and maps to\n",
      "multiple chromatogram files (mzML). This function will try to guess the\n",
      "original name of the mzML based on the align_origfilename column in the\n",
      "TSV. Note that both files have some typical endings that are _not_ shared,\n",
      "these are generally removed before comparison.\n",
      "\n",
      "Only an excact match is allowed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "853T\n",
      "provides an example with error rates (one per session)\n",
      "@note linear function verified in open office calc \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "854T\n",
      "Build deblur command for subprocess.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "params: list of str\n",
      "    parameter settings to pass to deblur CLI\n",
      "input_fp : str\n",
      "    name of the input fasta file to deblur\n",
      "\n",
      "Returns\n",
      "-------\n",
      "stdout: string\n",
      "    process output directed to standard output\n",
      "stderr: string\n",
      "    process output directed to standard error\n",
      "return_value: integer\n",
      "    return code from process\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "855T\n",
      "Given a functor, run it and return its result. We can use this with\n",
      "multiprocessing.map and map it over a list of job functors to do them.\n",
      "\n",
      "Handles getting more than multiprocessing's pitiful exception output\n",
      "\n",
      "This function was derived from:\n",
      "http://stackoverflow.com/a/16618842/19741\n",
      "\n",
      "This code was adopted from the American Gut project:\n",
      "https://github.com/biocore/American-Gut/blob/master/americangut/parallel.py\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "856T\n",
      "Dispatch execution over a pool of processors\n",
      "\n",
      "This code was adopted from the American Gut project:\n",
      "https://github.com/biocore/American-Gut/blob/master/americangut/parallel.py\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "inputs : iterable of str\n",
      "    File paths to input per-sample sequence files\n",
      "params : list of str\n",
      "    list of CLI parameters supplied to the deblur workflow\n",
      "    (argv - first 2 are 'deblur','workflow' and are ignored)\n",
      "pos_ref_db_fp : list of str\n",
      "    the indexed positive (16s) sortmerna database\n",
      "    (created in the main thread)\n",
      "neg_ref_db_fp : list of str\n",
      "    the indexed negative (artifacts) sortmerna database\n",
      "    (created in the main thread)\n",
      "jobs_to_start : int, optional\n",
      "    The number of processors on the local system to use\n",
      "\n",
      "Returns\n",
      "-------\n",
      "all_result_paths : list\n",
      "    list of expected output files\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "857T\n",
      "Return the default error profile for deblurring\n",
      "based on illumina run data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "858T\n",
      "Returns a list of Sequences\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input_seqs : iterable of (str, str)\n",
      "    The list of input sequences in (label, sequence) format\n",
      "\n",
      "Returns\n",
      "-------\n",
      "list of Sequence\n",
      "\n",
      "Raises\n",
      "------\n",
      "ValueError\n",
      "    If no sequences where found in `input_seqs`\n",
      "    If all the sequences do not have the same length either aligned or\n",
      "    unaligned.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "859T\n",
      "Deblur the reads\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input_seqs : iterable of (str, str)\n",
      "    The list of input sequences in (label, sequence) format. The label\n",
      "    should include the sequence count in the 'size=X' format.\n",
      "mean_error : float, optional\n",
      "    The mean illumina error, used for original sequence estimate.\n",
      "    Default: 0.005\n",
      "error_dist : list of float, optional\n",
      "    A list of error probabilities. The length of the list determines the\n",
      "    amount of hamming distances taken into account. Default: None, use\n",
      "    the default error profile (from get_default_error_profile() )\n",
      "indel_prob : float, optional\n",
      "    Indel probability (same for N indels). Default: 0.01\n",
      "indel_max : int, optional\n",
      "    The maximal number of indels expected by errors. Default: 3\n",
      "\n",
      "Results\n",
      "-------\n",
      "list of Sequence\n",
      "    The deblurred sequences\n",
      "\n",
      "Notes\n",
      "-----\n",
      "mean_error is used only for normalizing the peak height before deblurring.\n",
      "The array 'error_dist' represents the error distribution, where\n",
      "Xi = max frequency of error hamming. The length of this array - 1 limits\n",
      "the hamming distance taken into account, i.e. if the length if `error_dist`\n",
      "is 10, sequences up to 10 - 1 = 9 hamming distance will be taken into\n",
      "account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "860T\n",
      "Yield (id, sequence) from an input file\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input_fp : filepath\n",
      "    A filepath, which can be any valid fasta or fastq file within the\n",
      "    limitations of scikit-bio's IO registry.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The use of this method is a stopgap to replicate the existing `parse_fasta`\n",
      "functionality while at the same time allowing for fastq support.\n",
      "\n",
      "Raises\n",
      "------\n",
      "skbio.io.FormatIdentificationWarning\n",
      "    If the format of the input file cannot be determined.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "(str, str)\n",
      "    The ID and sequence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "861T\n",
      "Trim FASTA sequences to specified length.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input_seqs : iterable of (str, str)\n",
      "    The list of input sequences in (label, sequence) format\n",
      "trim_len : int\n",
      "    Sequence trimming length. Specify a value of -1 to disable trimming.\n",
      "left_trim_len : int\n",
      "    Sequence trimming from the 5' end. A value of 0 will disable this trim.\n",
      "\n",
      "\n",
      "Returns\n",
      "-------\n",
      "Generator of (str, str)\n",
      "    The trimmed sequences in (label, sequence) format\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "862T\n",
      "Dereplicate FASTA sequences and remove singletons using VSEARCH.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "seqs_fp : string\n",
      "    filepath to FASTA sequence file\n",
      "output_fp : string\n",
      "    file path to dereplicated sequences (FASTA format)\n",
      "min_size : integer, optional\n",
      "    discard sequences with an abundance value smaller\n",
      "    than integer\n",
      "use_log: boolean, optional\n",
      "    save the vsearch logfile as well (to output_fp.log)\n",
      "    default=False\n",
      "threads : int, optional\n",
      "    number of threads to use (0 for all available)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "863T\n",
      "Build a SortMeRNA index for all reference databases.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "ref_fp: tuple\n",
      "    filepaths to FASTA reference databases\n",
      "working_dir: string\n",
      "    working directory path where to store the indexed database\n",
      "\n",
      "Returns\n",
      "-------\n",
      "all_db: tuple\n",
      "    filepaths to SortMeRNA indexed reference databases\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "864T\n",
      "Filter samples from biom table that have less than\n",
      "minreads reads total\n",
      "\n",
      "Paraneters\n",
      "----------\n",
      "table : biom.Table\n",
      "    the biom table to filter\n",
      "minreads : int (optional)\n",
      "    the minimal number of reads in a sample in order to keep it\n",
      "inplace : bool (optional)\n",
      "    if True, filter the biom table in place, if false create a new copy\n",
      "\n",
      "Returns\n",
      "-------\n",
      "table : biom.Table\n",
      "    the filtered biom table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "865T\n",
      "Save sequences from a biom table to a fasta file\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "table : biom.Table\n",
      "    The biom table containing the sequences\n",
      "fasta_file_name : str\n",
      "    Name of the fasta output file\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "866T\n",
      "Remove artifacts from a biom table using SortMeRNA\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "table : str\n",
      "    name of the biom table file\n",
      "fasta_filename : str\n",
      "    the fasta file containing all the sequences of the biom table\n",
      "\n",
      "Returns\n",
      "-------\n",
      "tmp_files : list of str\n",
      "    The temp files created during the artifact removal step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "867T\n",
      "Remove artifacts from FASTA file using SortMeRNA.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "seqs_fp: string\n",
      "    file path to FASTA input sequence file\n",
      "ref_fp: tuple\n",
      "    file path(s) to FASTA database file\n",
      "working_dir: string\n",
      "    working directory path\n",
      "ref_db_fp: tuple\n",
      "    file path(s) to indexed FASTA database\n",
      "negate: boolean, optional\n",
      "    if True, discard all input sequences aligning\n",
      "    to reference database\n",
      "threads: integer, optional\n",
      "    number of threads to use for SortMeRNA\n",
      "verbose: boolean, optional\n",
      "    If true, output SortMeRNA errors\n",
      "sim_thresh: float, optional\n",
      "    The minimal similarity threshold (between 0 and 1)\n",
      "    for keeping the sequence\n",
      "    if None, the default values used are 0.65 for negate=False,\n",
      "    0.95 for negate=True\n",
      "coverage_thresh: float, optional\n",
      "    The minimal coverage threshold (between 0 and 1)\n",
      "    for alignments for keeping the sequence\n",
      "    if None, the default values used are 0.5 for negate=False,\n",
      "    0.95 for negate=True\n",
      "\n",
      "Returns\n",
      "-------\n",
      "output_fp : str\n",
      "    Name of the artifact removed fasta file\n",
      "okseqs : int\n",
      "    The number of sequences left after artifact removal\n",
      "tmp_files : list of str\n",
      "    Names of the tmp files created\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "868T\n",
      "Perform multiple sequence alignment on FASTA file using MAFFT.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "seqs_fp: string\n",
      "    filepath to FASTA file for multiple sequence alignment\n",
      "threads: integer, optional\n",
      "    number of threads to use. 0 to use all threads\n",
      "\n",
      "Returns\n",
      "-------\n",
      "msa_fp : str\n",
      "    name of output alignment file or None if error encountered\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "869T\n",
      "Remove chimeras de novo using UCHIME (VSEARCH implementation).\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "seqs_fp: string\n",
      "    file path to FASTA input sequence file\n",
      "output_fp: string\n",
      "    file path to store chimera-free results\n",
      "threads : int\n",
      "    number of threads (0 for all cores)\n",
      "\n",
      "Returns\n",
      "-------\n",
      "output_fp\n",
      "    the chimera removed fasta file name\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "872T\n",
      "Write BIOM table to file.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "table: biom.table\n",
      "    an instance of a BIOM table\n",
      "biom_fp: string\n",
      "    filepath to output BIOM table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "873T\n",
      "Get a list of files to add to the output table\n",
      "\n",
      "Parameters:\n",
      "-----------\n",
      "input_dir : string\n",
      "    name of the directory containing the deblurred fasta files\n",
      "file_end : string\n",
      "    the ending of all the fasta files to be added to the table\n",
      "    (default '.fasta.trim.derep.no_artifacts.msa.deblur.no_chimeras')\n",
      "\n",
      "Returns\n",
      "-------\n",
      "names : list of tuples of (string,string)\n",
      "    list of tuples of:\n",
      "        name of fasta files to be added to the biom table\n",
      "        sampleid (file names without the file_end and path)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "874T\n",
      "Create a biom table out of all files in a directory\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "output_fp : string\n",
      "    filepath to output BIOM table\n",
      "deblurred_list : list of (str, str)\n",
      "    list of file names (including path), sampleid of all deblurred\n",
      "    fasta files to add to the table\n",
      "outputfasta_fp : str, optional\n",
      "    name of output fasta file (of all sequences in the table) or None\n",
      "    to not write\n",
      "minreads : int, optional\n",
      "    minimal number of reads per bacterial sequence in order to write\n",
      "    it to the biom table and fasta file or 0 to write all\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "875T\n",
      "Launch full deblur workflow for a single post split-libraries fasta file\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "seqs_fp: string\n",
      "    a post split library fasta file for debluring\n",
      "working_dir: string\n",
      "    working directory path\n",
      "mean_error: float\n",
      "    mean error for original sequence estimate\n",
      "error_dist: list\n",
      "    list of error probabilities for each hamming distance\n",
      "indel_prob: float\n",
      "    insertion/deletion (indel) probability\n",
      "indel_max: integer\n",
      "    maximal indel number\n",
      "trim_length: integer\n",
      "    sequence trim length\n",
      "left_trim_length: integer\n",
      "    trim the first n reads\n",
      "min_size: integer\n",
      "    upper limit on sequence abundance (discard sequences below limit)\n",
      "ref_fp: tuple\n",
      "    filepath(s) to FASTA reference database for artifact removal\n",
      "ref_db_fp: tuple\n",
      "    filepath(s) to SortMeRNA indexed database for artifact removal\n",
      "threads_per_sample: integer, optional\n",
      "    number of threads to use for SortMeRNA/mafft/vsearch\n",
      "    (0 for max available)\n",
      "sim_thresh: float, optional\n",
      "    the minimal similarity for a sequence to the database.\n",
      "    if None, take the defaults (0.65 for negate=False,\n",
      "    0.95 for negate=True)\n",
      "coverage_thresh: float, optional\n",
      "    the minimal coverage for alignment of a sequence to the database.\n",
      "    if None, take the defaults (0.3 for negate=False, 0.95 for negate=True)\n",
      "\n",
      "Return\n",
      "------\n",
      "output_no_chimers_fp : string\n",
      "    filepath to fasta file with no chimeras of None if error encountered\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "876T\n",
      "start the logger for the run\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "level : int, optional\n",
      "    logging.DEBUG, logging.INFO etc. for the log level (between 0-50).\n",
      "filename : str, optional\n",
      "  name of the filename to save the log to or\n",
      "  None (default) to use deblur.log.TIMESTAMP\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "877T\n",
      "Execute the command `cmd`\n",
      "Parameters\n",
      "----------\n",
      "cmd : str\n",
      "    The string containing the command to be run.\n",
      "stdoutfilename : str\n",
      "    Name of the file to save stdout to or None\n",
      "    (default) to not save to file\n",
      "stderrfilename : str\n",
      "    Name of the file to save stderr to or None\n",
      "    (default) to not save to file\n",
      "\n",
      "Returns\n",
      "-------\n",
      "tuple of (str, str, int)\n",
      "    The standard output, standard error and exist status of the\n",
      "    executed command\n",
      "\n",
      "Notes\n",
      "-----\n",
      "This function is ported and modified from QIIME\n",
      "(http://www.qiime.org), previously named\n",
      "qiime_system_call. QIIME is a GPL project, but we obtained permission from\n",
      "the authors of this function to port it to Qiita and keep it under BSD\n",
      "license.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "878T\n",
      "removes the specified nodes from the cluster and returns\n",
      "the remaining nodes \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "879T\n",
      "Create slices to split a UMI into approximately equal size substrings\n",
      "Returns a list of tuples that can be passed to slice function\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "880T\n",
      "Build a dictionary of nearest neighbours using substrings, can be used\n",
      "to reduce the number of pairwise comparisons.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "881T\n",
      "Added by Matt 06/05/17\n",
      "use substring dict to get (approximately) all the nearest neighbours to\n",
      "each in a set of umis.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "882T\n",
      "This will return an iterator that returns tuples of fastq records.\n",
      "At each step it will confirm that the first field of the read name\n",
      "(before the first whitespace character) is identical between the\n",
      "two reads. The response if it is not depends on the value of\n",
      ":param:`strict`. If strict is true an error is returned. If strict\n",
      "is `False` the second file is advanced until a read that matches\n",
      "is found.\n",
      "\n",
      "This allows for protocols where read one contains cell barcodes, and these\n",
      "reads have been filtered and corrected before processing without regard\n",
      "to read2\n",
      "\n",
      "If has_suffix is True, /1 and /2 will be\n",
      "removed from the end of read1 and read2, respectively before\n",
      "checking their names are identical\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "883T\n",
      "open file in *filename* with mode *mode*.\n",
      "\n",
      "If *create* is set, the directory containing filename\n",
      "will be created if it does not exist.\n",
      "\n",
      "gzip - compressed files are recognized by the\n",
      "suffix ``.gz`` and opened transparently.\n",
      "\n",
      "Note that there are differences in the file\n",
      "like objects returned, for example in the\n",
      "ability to seek.\n",
      "\n",
      "returns a file or file-like object.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "884T\n",
      "return a header string with command line options and timestamp\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "885T\n",
      "return a string containing script parameters.\n",
      "\n",
      "Parameters are all variables that start with ``param_``.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "886T\n",
      "return a header string with command line options and\n",
      "timestamp.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "887T\n",
      "stop the experiment.\n",
      "\n",
      "This method performs final book-keeping, closes the output streams\n",
      "and writes the final log messages indicating script completion.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "888T\n",
      "get a temporary file.\n",
      "\n",
      "The file is created and the caller needs to close and delete\n",
      "the temporary file once it is not used any more.\n",
      "\n",
      "Arguments\n",
      "---------\n",
      "dir : string\n",
      "    Directory of the temporary file and if not given is set to the\n",
      "    default temporary location in the global configuration dictionary.\n",
      "shared : bool\n",
      "    If set, the tempory file will be in a shared temporary\n",
      "    location (given by the global configuration directory).\n",
      "suffix : string\n",
      "    Filename suffix\n",
      "\n",
      "Returns\n",
      "-------\n",
      "file : File\n",
      "    A file object of the temporary file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "889T\n",
      "return a temporary filename.\n",
      "\n",
      "The file is created and the caller needs to delete the temporary\n",
      "file once it is not used any more.\n",
      "\n",
      "Arguments\n",
      "---------\n",
      "dir : string\n",
      "    Directory of the temporary file and if not given is set to the\n",
      "    default temporary location in the global configuration dictionary.\n",
      "shared : bool\n",
      "    If set, the tempory file will be in a shared temporary\n",
      "    location.\n",
      "suffix : string\n",
      "    Filename suffix\n",
      "\n",
      "Returns\n",
      "-------\n",
      "filename : string\n",
      "    Absolute pathname of temporary file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "890T\n",
      "extract the identifier from a read and append the UMI and\n",
      "cell barcode before the first space\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "893T\n",
      "extract the umi +/- cell barcode from the read id using the\n",
      "specified separator \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "894T\n",
      "extract the umi from the read id (input as a string) using the\n",
      "specified separator \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "895T\n",
      "extract the umi and cell barcode from the read id (input as a\n",
      "string) using the specified separator \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "896T\n",
      "extract the umi +/- cell barcode from the read name where the barcodes\n",
      "were extracted using umis\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "897T\n",
      "Yields the counts per umi for each gene\n",
      "\n",
      "bc_getter: method to get umi (plus optionally, cell barcode) from\n",
      "read, e.g get_umi_read_id or get_umi_tag\n",
      "\n",
      "\n",
      "TODO: ADD FOLLOWING OPTION\n",
      "\n",
      "skip_regex: skip genes matching this regex. Useful to ignore\n",
      "            unassigned reads (as per get_bundles class above)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "898T\n",
      "Takes a cigar string and finds the first splice position as\n",
      "an offset from the start. To find the 5' end (read coords) of\n",
      "the junction for a reverse read, pass in the reversed cigar tuple\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "899T\n",
      "estimate the number of \"true\" cell barcodes using a gaussian\n",
      "density-based method\n",
      "\n",
      "input:\n",
      "     cell_barcode_counts = dict(key = barcode, value = count)\n",
      "     expect_cells (optional) = define the expected number of cells\n",
      "     cell_number (optional) = define number of cell barcodes to accept\n",
      "     plotfile_prefix = (optional) prefix for plots\n",
      "\n",
      "returns:\n",
      "     List of true barcodes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "902T\n",
      "whitelist_tsv: tab-separated file with whitelisted barcodes. First\n",
      "field should be whitelist barcodes. Second field [optional] should\n",
      "be comma-separated barcodes which are to be corrected to the\n",
      "barcode in the first field.\n",
      "\n",
      "whitelist_tsv2: as above but for read2s\n",
      "getErrorCorrection: extract the second field in whitelist_tsv and\n",
      "return a map of non-whitelist:whitelist\n",
      "\n",
      "deriveErrorCorrection: return a map of non-whitelist:whitelist\n",
      "using a simple edit distance threshold\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "903T\n",
      "Check for errors (substitutions, insertions, deletions) between a barcode\n",
      "and a set of whitelist barcodes.\n",
      "\n",
      "Returns the whitelist barcodes which match the input barcode\n",
      "allowing for errors. Returns as soon as two are identified.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "905T\n",
      "Independent probabilities a, b, c can be combined like this:\n",
      "\n",
      "                         a*b*c\n",
      "combined_prob = -------------------------\n",
      "                a*b*c + (1-a)*(1-b)*(1-c)\n",
      "\n",
      "For a straightforward explanation, see\n",
      "http://www.paulgraham.com/naivebayes.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "906T\n",
      "Parses command line arguments and returns them in a dict.\n",
      "Only used when executing this script without Ursgal.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "907T\n",
      "Combines independent probabilities a, b, c\n",
      "like this:\n",
      "\n",
      "                        a*b*c\n",
      "combined_prob = -------------------------\n",
      "                a*b*c + (1-a)*(1-b)*(1-c)\n",
      "\n",
      "For a straightforward explanation, see\n",
      "http://www.paulgraham.com/naivebayes.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "908T\n",
      "Converts the value of a field to float.\n",
      "If the field contains multiple separated floats,\n",
      "the mean is returned instead. If the field is empty,\n",
      "numpy.NaN is returned.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "909T\n",
      "Converts the value of a field to float.\n",
      "If the field contains multiple separated floats,\n",
      "the naive Bayes combined value is returned instead.\n",
      "If the field is empty, numpy.NaN is returned.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "910T\n",
      "calculate false discovery rate according to FDR Method 2\n",
      "(Kll et al. 2007) as explained by Jones et al. 2009\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "911T\n",
      "Some sequences cannot be distinguished by MS (i.e. sequences\n",
      "where L is replaced by I and vice versa).\n",
      "Such target/decoy pairs are not suitable for training.\n",
      "This function applies rules to a sequence that allow identification\n",
      "of such target/decoy pairs,\n",
      "i.e.\n",
      "    unify_sequence('EILE') == unify_sequence('ELLE')\n",
      "    -> True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "912T\n",
      "Generate spectra from openend mgf handle.\n",
      "\n",
      "Args:\n",
      "    fh (IO): file obj\n",
      "Yields:\n",
      "    spec_dict (dict): Dict contaning spec data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "914T\n",
      "Dynamically adjusts the sliding window size depending on the total\n",
      "length of values. When there are few values (below 1/5 of the\n",
      "window size), the window size is decreased.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "915T\n",
      "Sliding window generator:\n",
      "Slow but readable version using list slicing\n",
      "currently not used.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "916T\n",
      "Sliding window generator.\n",
      "Gives you sliding window functionality without using container\n",
      "types (list, deque etc.) to speed it up. Only works for lists of\n",
      "numbers. Yields the sum of all numbers in the sliding window\n",
      "(= the number of decoys in the sliding window in our case), the\n",
      "central number of the sliding window (required for the test only),\n",
      "and the current length of the sliding window (= total number of\n",
      "PSMs in the sliding window). Used for PEP calculation:\n",
      "PEP_of_PSM = (n_decoys_in_window * 2) / n_total_PSMs_in_window\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "917T\n",
      "Parses command line arguments and returns them in a dict.\n",
      "Only used when executing this script from command line.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "918T\n",
      "(without fancy imports...)\n",
      "n = number of values\n",
      "geo. mean = nth root of all values multiplied with each other\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "919T\n",
      "Given two points in 2D-space (tuples a and b), calculates\n",
      "the slope (m) and intercept (b) of the line connecting\n",
      "both points. Required for FDR Score calculcation.\n",
      "Returns intercept and slope in a tuple.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "920T\n",
      "example:\n",
      "\n",
      "input:  [ 'xtandem', 'msgf', 'omssa' ]\n",
      "output: [{'omssa'}, {'msgf'}, {'msgf', 'omssa'}, {'xtandem'}, {'xtandem', 'omssa'}, {'msgf', 'xtandem'}, {'msgf', 'xtandem', 'omssa'}]\n",
      "from http://stackoverflow.com/questions/464864/python-code-to-pick-out-all-possible-combinations-from-a-list\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "921T\n",
      "calculating the FDR Score according to Jones et al. 2009\n",
      "Algorithm 1.4.b.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "922T\n",
      "reads an input csv and returns a defaultdict with the spectrum title\n",
      "mapping to a sorted list of tuples containing each\n",
      "a) score (from validation_score_field) and\n",
      "b) the whole line dict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "923T\n",
      "calculate false discovery rate according to FDR Method 2\n",
      "(Kll et al. 2007) as explained by Jones et al. 2009\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "924T\n",
      "NOTE:\n",
      "    equal to charge * mz - ( charge * PROTON)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "925T\n",
      "Returns True if the running system's terminal supports color, and False\n",
      "otherwise. Source:\n",
      "https://github.com/django/django/blob/master/django/core/management/color.py\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "926T\n",
      "Function to print current params\n",
      "\n",
      "Keyword Arguments:\n",
      "        params (dict): parameter dict to print\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "927T\n",
      "Normalize the precision in ppm to 1000 Dalton\n",
      "\n",
      "Keyword Arguments:\n",
      "    ppm_value (float): parts per million value to transform\n",
      "    base_mz (float): factor for transformation\n",
      "\n",
      "\n",
      "Returns:\n",
      "    float: value in Dalton\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "928T\n",
      "Convert the precision in Dalton to ppm\n",
      "\n",
      "Keyword Arguments:\n",
      "    da_value (float): Dalton value to transform\n",
      "    base_mz (float): factor for transformation\n",
      "\n",
      "\n",
      "Returns:\n",
      "    float: value in ppm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "929T\n",
      "Calculate m/z function\n",
      "\n",
      "Keyword Arguments:\n",
      "    mass (float): mass for calculating m/z\n",
      "    charge (int): charge for calculating m/z\n",
      "\n",
      "\n",
      "Returns:\n",
      "    float: calculated m/z\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "930T\n",
      "Calculate mass function\n",
      "\n",
      "Keyword Arguments:\n",
      "    mz (float): mz of molecule/peak\n",
      "    charge (int): charge for calculating mass\n",
      "\n",
      "\n",
      "Returns:\n",
      "    float: calculated mass\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "931T\n",
      "Amino acid digest function\n",
      "\n",
      "Keyword Arguments:\n",
      "    sequence (str): amino acid sequence to digest\n",
      "    enzyme (tuple): enzyme properties used for cleavage ('aminoacid(s)', 'N/C(terminus)')\n",
      "                    e.g. ('KR','C') for trypsin\n",
      "    count_missed_cleavages (int): number of miss cleavages allowed\n",
      "\n",
      "Returns:\n",
      "    list: list of digested peptides\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "932T\n",
      "Small function to efficiently parse a file in fasta format.\n",
      "\n",
      "Keyword Arguments:\n",
      "    io (obj): openend file obj (fasta file)\n",
      "\n",
      "Yields:\n",
      "    tuple: fasta_id and sequence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "933T\n",
      "reformats the MQ and Novor peptide string to ursgal format\n",
      "(ac)SSSLM(ox)RPGPSR --> SSSLMRPGPSR#Acetyl:0;Oxidation:5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "934T\n",
      "Returns a counter based on PSM-defining column names (i.e spectrum & peptide,\n",
      "but also score field because sometimes the same PSMs are reported\n",
      "with different scores...).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "935T\n",
      "Merges CSV rows. If the column values are conflicting, they\n",
      "are joined with a character (joinchar).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "936T\n",
      "Rows describing the same PSM (e.g. when two proteins share the\n",
      "same peptide) are merged to one row.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "937T\n",
      "ax = matplotlib axes instance\n",
      "x = x-axis coordinates\n",
      "data = profile data\n",
      "color = color in any way matplotlib accepts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "938T\n",
      "Plot a genome browser like profile\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "fname: string\n",
      "    output file name\n",
      "\n",
      "interval: string\n",
      "    interval to plot in \"chrom:start-end\" format\n",
      "\n",
      "tracks: list\n",
      "    list of filenames\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "939T\n",
      "Merge mirrored profiles based on a chi2 test of the mean profiles \n",
      "Only if the profile is mirrored over all data tracks\n",
      "Returns the labels of the two matched mirrored tracks, if there is at least one match with a p-value\n",
      "greater than the cutoff.\n",
      "If not, return (None, None)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "940T\n",
      "Cluster profiles for heatmap\n",
      "\n",
      "Takes a matrix and clusters either with kmeans or hierarchical clustering.\n",
      "Distance can be either euclidean or pearson. \n",
      "\n",
      "Parameters\n",
      "----------\n",
      "cluster_data :  array_like\n",
      "    Data to cluster.\n",
      "\n",
      "cluster_type : str, optional\n",
      "    Either 'k' for kmeans, 'h' for hierarchical or 'n' for no clustering.\n",
      "    If cluster_type equals None, data is also not clustered.\n",
      "\n",
      "numclusters : int, optional\n",
      "    Number of clusters.\n",
      "\n",
      "dist : str, optional\n",
      "    Distance metric, either 'euclidean' or 'pearson'.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "\n",
      "ind : array\n",
      "    Indices of sorted input.\n",
      "\n",
      "labels : array \n",
      "    Cluster labels.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "941T\n",
      "Reads a BED file, using the fourth column as cluster number\n",
      "Arguments: bedfile - a 4-column BED file\n",
      "Returns: a hash with cluster numbers as key, and a list of genomic locations as value\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "942T\n",
      "Convert SAM to RNF-based FASTQ with respect to argparse parameters.\n",
      "\n",
      "Args:\n",
      "        args (...): Arguments parsed by argparse\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "943T\n",
      "Add another parser for a SAM2RNF-like command.\n",
      "\n",
      "Args:\n",
      "        subparsers (subparsers): File name of the genome from which read tuples are created (FASTA file).\n",
      "        simulator_name (str): Name of the simulator used in comments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "944T\n",
      "Read the combined wig-file generated by Transit\n",
      ":: Filename -> Tuple([Site], [WigData], [Filename])\n",
      "Site :: Integer\n",
      "WigData :: [Number]\n",
      "Filename :: String\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "945T\n",
      "Filename -> ConditionMap\n",
      "ConditionMap :: {Filename: Condition}, [{Filename: Covar}], [{Filename: Interaction}]\n",
      "Condition :: String\n",
      "Covar :: String\n",
      "Interaction :: String\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "946T\n",
      "(Filename, Options) -> [Gene]\n",
      "Gene :: {start, end, rv, gene, strand}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "947T\n",
      "Reduces the data into Bernoulli trials (or 'tosses') based on whether counts were observed or not.\n",
      "\n",
      "Arguments:\n",
      "    data (list): List of numeric data.\n",
      "\n",
      "Returns:\n",
      "    list: Data represented as bernoulli trials with >0 as true.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "948T\n",
      "Return list of all the runs of consecutive non-insertions.\n",
      "\n",
      "Arguments:\n",
      "    data (list): List of numeric data.\n",
      "\n",
      "Returns:\n",
      "    list: List of the length of the runs of non-insertions. Non-zero sites are treated as runs of zero.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "949T\n",
      "Returns a list of the indexes of the start of the runs; complements runs().\n",
      "\n",
      "Arguments:\n",
      "    runs (list): List of numeric data.\n",
      "\n",
      "Returns:\n",
      "    list: List of the index of the runs of non-insertions. Non-zero sites are treated as runs of zero.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "950T\n",
      "Returns the transposon type (himar1/tn5) of the list of wig files.\n",
      "\n",
      "Arguments:\n",
      "    wig_list (list): List of paths to wig files.\n",
      "\n",
      "Returns:\n",
      "    list: List of transposon type (\"himar1\" or \"tn5\").\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "951T\n",
      "Returns boolean list showing whether the given files include empty sites\n",
      "(zero) or not.\n",
      "\n",
      "Arguments:\n",
      "    wig_list (list): List of paths to wig files.\n",
      "\n",
      "Returns:\n",
      "    list: List of boolean values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "952T\n",
      "Returns a tuple of (data, position) containing a matrix of raw read-counts\n",
      "    , and list of coordinates.\n",
      "\n",
      "Arguments:\n",
      "    wig_list (list): List of paths to wig files.\n",
      "\n",
      "Returns:\n",
      "    tuple: Two lists containing data and positions of the wig files given.\n",
      "\n",
      ":Example:\n",
      "\n",
      "    >>> import pytransit.tnseq_tools as tnseq_tools\n",
      "    >>> (data, position) = tnseq_tools.get_data([\"data/glycerol_H37Rv_rep1.wig\", \"data/glycerol_H37Rv_rep2.wig\"])\n",
      "    >>> print(data)\n",
      "    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "\n",
      ".. seealso:: :class:`get_file_types` :class:`combine_replicates` :class:`get_data_zero_fill` :class:`pytransit.norm_tools.normalize_data`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "953T\n",
      "Returns a tuple of (data, position) containing a matrix of raw read counts,\n",
      "    and list of coordinates. Positions that are missing are filled in as zero.\n",
      "\n",
      "Arguments:\n",
      "    wig_list (list): List of paths to wig files.\n",
      "\n",
      "Returns:\n",
      "    tuple: Two lists containing data and positions of the wig files given.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "954T\n",
      "Returns list of data merged together.\n",
      "\n",
      "Arguments:\n",
      "    data (list): List of numeric (replicate) data to be merged.\n",
      "    method (str): How to combine the replicate dataset.\n",
      "\n",
      "Returns:\n",
      "    list: List of numeric dataset now merged together.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "955T\n",
      "Returns statistics for the given wig file with read-counts.\n",
      "\n",
      "Arguments:\n",
      "    path (str): String with the path to the wig file of interest.\n",
      "\n",
      "Returns:\n",
      "    tuple: Tuple with the following statistical measures:\n",
      "        - density\n",
      "        - mean read\n",
      "        - non-zero mean\n",
      "        - non-zero median\n",
      "        - max read\n",
      "        - total reads\n",
      "        - skew\n",
      "        - kurtosis\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "956T\n",
      "Returns a dictionary that maps coordinates to a list of genes that occur at that coordinate.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in .prot_table format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of position to list of genes that share that position.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "957T\n",
      "Returns a dictionary that maps coordinates to a list of genes that occur at that coordinate.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in GFF3 format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of position to list of genes that share that position.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "958T\n",
      "Returns a dictionary that maps coordinates to a list of genes that occur at that coordinate.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in .prot_table or GFF3 format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of position to list of genes that share that position.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "959T\n",
      "Returns a dictionary that maps gene id to gene information.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in .prot_table format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of gene id to tuple of information:\n",
      "        - name\n",
      "        - description\n",
      "        - start coordinate\n",
      "        - end coordinate\n",
      "        - strand\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "960T\n",
      "Returns a dictionary that maps gene id to gene information.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in GFF3 format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of gene id to tuple of information:\n",
      "        - name\n",
      "        - description\n",
      "        - start coordinate\n",
      "        - end coordinate\n",
      "        - strand\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "961T\n",
      "Returns a dictionary that maps gene id to gene information.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in .prot_table or GFF3 format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of gene id to tuple of information:\n",
      "        - name\n",
      "        - description\n",
      "        - start coordinate\n",
      "        - end coordinate\n",
      "        - strand\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "962T\n",
      "Attempts to get mapping of coordinates from galign file.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to .galign file.\n",
      "    reverse (bool): Boolean specifying whether to do A to B or B to A.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of coordinate in one file to another file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "963T\n",
      "Reads in FASTA formatted genome file.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to .galign file.\n",
      "\n",
      "Returns:\n",
      "    string: String with the genomic sequence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "964T\n",
      "Returns the length of the maximum run an item in a given list.\n",
      "\n",
      "Arguments:\n",
      "    lst (list): List of numeric items.\n",
      "    item (float): Number to look for consecutive runs of.\n",
      "\n",
      "Returns:\n",
      "    int: Length of the maximum run of consecutive instances of item.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "965T\n",
      "Expected value of the run of non=insertions (Schilling, 1990):\n",
      "\n",
      "    ER_n =  log(1/p)(nq) + gamma/ln(1/p) -1/2 + r1(n) + E1(n)\n",
      "\n",
      "Arguments:\n",
      "    n (int): Integer representing the number of sites.\n",
      "    pins (float): Floating point number representing the probability of non-insertion.\n",
      "\n",
      "Returns:\n",
      "    float: Size of the expected maximum run.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "966T\n",
      "Variance of the expected run of non-insertons (Schilling, 1990):\n",
      "\n",
      ".. math::\n",
      "\n",
      "    VarR_n =  (pi^2)/(6*ln(1/p)^2) + 1/12 + r2(n) + E2(n)\n",
      "\n",
      "\n",
      "Arguments:\n",
      "    n (int): Integer representing the number of sites.\n",
      "    pnon (float): Floating point number representing the probability of non-insertion.\n",
      "\n",
      "Returns:\n",
      "    float: Variance of the length of the maximum run.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "967T\n",
      "CDF of the Gumbel distribution:\n",
      "\n",
      "    e^(-e^( (u-x)/B))\n",
      "\n",
      "Arguments:\n",
      "    x (int): Length of the max run.\n",
      "    u (float): Location parameter of the Gumbel dist.\n",
      "    B (float): Scale parameter of the Gumbel dist.\n",
      "\n",
      "Returns:\n",
      "    float: Cumulative probability o the Gumbel distribution.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "968T\n",
      "Implements the basic Gumbel analysis of runs of non-insertion, described in Griffin et al. 2011.\n",
      "\n",
      "This analysis method calculates a p-value of observing the maximun run of\n",
      "TA sites without insertions in a row (i.e. a \"run\", r). Unusually long\n",
      "runs are indicative of an essential gene or protein domain. Assumes that\n",
      "there is a constant, global probability of observing an insertion\n",
      "(tantamount to a Bernoulli probability of success).\n",
      "\n",
      "Arguments:\n",
      "    genes_obj (Genes): An object of the Genes class defining the genes.\n",
      "    pins (float): The probability of insertion.\n",
      "\n",
      "Returns:\n",
      "    list: List of lists with results and information for the genes. The elements of the list are as follows:\n",
      "        - ORF ID.\n",
      "        - Gene Name.\n",
      "        - Gene Description.\n",
      "        - Number of TA sites with insertions.\n",
      "        - Number of TA sites.\n",
      "        - Length of largest run of non-insertion.\n",
      "        - Expected run for a gene this size.\n",
      "        - p-value of the observed run.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "969T\n",
      "Return list of all the runs of consecutive non-insertions with the start and end locations.\n",
      "\n",
      "Arguments:\n",
      "    data (list): List of numeric data to check for runs.\n",
      "\n",
      "Returns:\n",
      "    list: List of dictionary from run to length and position information of the tun.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "970T\n",
      "Returns list of genes that occur in a given range of coordinates.\n",
      "\n",
      "Arguments:\n",
      "    pos_hash (dict): Dictionary of position to list of genes.\n",
      "    start (int): Start coordinate of the desired range.\n",
      "    end (int): End coordinate of the desired range.\n",
      "\n",
      "Returns:\n",
      "    list: List of genes that fall within range.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "971T\n",
      "Performs a box-cox transformation to data vector X.\n",
      "WARNING: elements of X should be all positive! \n",
      "Fixed: '>' has changed to '<'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "972T\n",
      "Returns a table of (loglik function, lambda) pairs\n",
      "for the data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "973T\n",
      "Does a permutation test on two sets of data.\n",
      "\n",
      "Performs the resampling / permutation test given two sets of data using a\n",
      "function defining the test statistic and a function defining how to permute\n",
      "the data.\n",
      "\n",
      "Args:\n",
      "    data1: List or numpy array with the first set of observations.\n",
      "    data2: List or numpy array with the second set of observations.\n",
      "    S: Number of permutation tests (or samples) to obtain.\n",
      "    testFunc: Function defining the desired test statistic. Should accept\n",
      "            two lists as arguments. Default is difference in means between\n",
      "            the observations.\n",
      "    permFunc: Function defining the way to permute the data. Should accept\n",
      "            one argument, the combined set of data. Default is random\n",
      "            shuffle.\n",
      "    adaptive: Cuts-off resampling early depending on significance.\n",
      "\n",
      "Data arrays: (data1 and data2)\n",
      "  Regular resampling used to take 1D arrays of counts pooled (flattened) over replicates.\n",
      "    Now 2D arrays are passed in and flatten them.\n",
      "    Uses F_shuffle_flat() and F_sum_diff_flat().\n",
      "  If using library strings, then inputs are 2D arrays of counts for each sample. \n",
      "    Character in lib_str indicates which lib it is in.  Make a dict out of these to pass to permFunc.\n",
      "    Uses F_shuffle_dict_libraries() and F_sum_diff_dict_libraries().\n",
      "  If site_restricted, keep input arrays as 2D and pass to site_restricted_permutation() and F_sum_diff_flat().\n",
      "\n",
      "Returns:\n",
      "    Tuple with described values\n",
      "        - test_obs -- Test statistic of observation.\n",
      "        - mean1 -- Arithmetic mean of first set of data.\n",
      "        - mean2 -- Arithmetic mean of second set of data.\n",
      "        - log2FC -- Normalized log2FC the means.\n",
      "        - pval_ltail -- Lower tail p-value.\n",
      "        - pval_utail -- Upper tail p-value.\n",
      "        - pval_2tail -- Two-tailed p-value.\n",
      "        - test_sample -- List of samples of the test statistic.\n",
      "\n",
      ":Example:\n",
      "    >>> import pytransit.stat_tools as stat_tools\n",
      "    >>> import numpy\n",
      "    >>> X = numpy.random.random(100)\n",
      "    >>> Y = numpy.random.random(100)\n",
      "    >>> (test_obs, mean1, mean2, log2fc, pval_ltail, pval_utail, pval_2tail, test_sample) = stat_tools.resampling(X,Y)\n",
      "    >>> pval_2tail\n",
      "    0.2167\n",
      "    >>> test_sample[:3]\n",
      "    [0.076213992904990535, -0.0052513291091412784, -0.0038425140184765172]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "974T\n",
      "Normalizes the numpy array by the given normalization method.\n",
      "\n",
      "Arguments:\n",
      "    data (numpy array): (K,N) numpy array defining read-counts at N sites\n",
      "        for K datasets.\n",
      "    method (str): Name of the desired normalization method.\n",
      "    wigList (list): List of paths for the desired wig-formatted datasets.\n",
      "    annotationPath (str): Path to the prot_table annotation file.\n",
      "\n",
      "Returns:\n",
      "    numpy array: Array with the normalized data.\n",
      "    list: List containing the normalization factors. Empty if not used.\n",
      "\n",
      ":Example:\n",
      "    >>> import pytransit.norm_tools as norm_tools\n",
      "    >>> import pytransit.tnseq_tools as tnseq_tools\n",
      "    >>> (data, position) = tnseq_tools.get_data([\"transit/data/glycerol_H37Rv_rep1.wig\", \"transit/data/glycerol_H37Rv_rep2.wig\"])\n",
      "    >>> print(data)\n",
      "    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "    (normdata, normfactors) = norm_tools.normalize_data(data, \"TTR\")   # Some methods require annotation and path to wig files.\n",
      "    >>> print(normfactors)\n",
      "    array([[ 1.        ],\n",
      "           [ 0.62862886]])\n",
      "    >> print(normdata)\n",
      "    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "\n",
      ".. note:: Some normalization methods require the wigList and annotationPath arguments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "975T\n",
      "Calculates the observed density of the data.\n",
      "\n",
      "This is used as an estimate insertion density by some normalization methods.\n",
      "May be improved by more sophisticated ways later on.\n",
      "\n",
      "Arguments:\n",
      "    data (numpy array): (N) numpy array defining read-counts at N sites.\n",
      "\n",
      "Returns:\n",
      "    float: Density of the given dataset.\n",
      "\n",
      ":Example:\n",
      "    >>> import pytransit.tnseq_tools as tnseq_tools\n",
      "    >>> import pytransit.norm_tools as norm_tools\n",
      "    >>> (data, position) = tnseq_tools.get_data([\"transit/data/glycerol_H37Rv_rep1.wig\", \"transit/data/glycerol_H37Rv_rep2.wig\"])\n",
      "    >>> print(data)\n",
      "    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "    >>> theta = norm_tools.empirical_theta(data)\n",
      "    >>> print(theta)\n",
      "    0.467133570136\n",
      "\n",
      "\n",
      ".. seealso:: :class:`TTR_factors`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "976T\n",
      "Estimates the trimmed mean of the data.\n",
      "\n",
      "This is used as an estimate of mean count by some normalization methods.\n",
      "May be improved by more sophisticated ways later on.\n",
      "\n",
      "Arguments:\n",
      "    data (numpy array): (N) numpy array defining read-counts at N sites.\n",
      "    t (float): Float specifying fraction of start and end to trim.\n",
      "\n",
      "Returns:\n",
      "    float: (Trimmed) Mean of the given dataset.\n",
      "\n",
      ":Example:\n",
      "    >>> import pytransit.tnseq_tools as tnseq_tools\n",
      "    >>> import pytransit.norm_tools as norm_tools\n",
      "    >>> (data, position) = tnseq_tools.get_data([\"transit/data/glycerol_H37Rv_rep1.wig\", \"transit/data/glycerol_H37Rv_rep2.wig\"])\n",
      "    >>> print(data)\n",
      "    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "    >>> mu = norm_tools.trimmed_empirical_mu(data)\n",
      "    >>> print(mu)\n",
      "    120.73077107\n",
      "\n",
      ".. seealso:: :class:`TTR_factors`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "977T\n",
      "Returns the normalization factors for the data using the zero-inflated\n",
      "negative binomial method.\n",
      "\n",
      "\n",
      "Arguments:\n",
      "    data (numpy array): (K,N) numpy array defining read-counts at N sites\n",
      "        for K datasets.\n",
      "\n",
      "Returns:\n",
      "    numpy array: Array with the normalization factors for the zinfnb method.\n",
      "\n",
      ":Example:\n",
      "    >>> import pytransit.norm_tools as norm_tools\n",
      "    >>> import pytransit.tnseq_tools as tnseq_tools\n",
      "    >>> (data, position) = tnseq_tools.get_data([\"transit/data/glycerol_H37Rv_rep1.wig\", \"transit/data/glycerol_H37Rv_rep2.wig\"])\n",
      "    >>> print(data)\n",
      "    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "    >>> factors = norm_tools.zinfnb_factors(data)\n",
      "    >>> print(factors)\n",
      "    [[ 0.0121883 ]\n",
      "     [ 0.00747111]]\n",
      "\n",
      ".. seealso:: :class:`normalize_data`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "978T\n",
      "Returns factors to normalize the data to the given target value.\n",
      "\n",
      "Arguments:\n",
      "    data (numpy array): (K,N) numpy array defining read-counts at N sites\n",
      "        for K datasets.\n",
      "    target (float): Floating point specifying the target for the mean of the data/\n",
      "\n",
      "Returns:\n",
      "    numpy array: Array with the factors necessary to normalize mean to target.\n",
      "\n",
      ":Example:\n",
      "    >>> import pytransit.norm_tools as norm_tools\n",
      "    >>> import pytransit.tnseq_tools as tnseq_tools\n",
      "    >>> (data, position) = tnseq_tools.get_data([\"transit/data/glycerol_H37Rv_rep1.wig\", \"transit/data/glycerol_H37Rv_rep2.wig\"])\n",
      "    >>> print(data)\n",
      "    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "    >>> factors = norm_tools.norm_to_target(data, 100)\n",
      "    >>> print(factors)\n",
      "    [[ 1.8548104 ]\n",
      "     [ 1.16088726]]\n",
      "\n",
      "\n",
      ".. seealso:: :class:`normalize_data`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "979T\n",
      "Returns a list and a dictionary with positional and keyword arguments.\n",
      "\n",
      "-This function assumes flags must start with a \"-\" and and cannot be a \n",
      "    number (but can include them).\n",
      "\n",
      "-Flags should either be followed by the value they want to be associated \n",
      "    with (i.e. -p 5) or will be assigned a value of True in the dictionary.\n",
      "\n",
      "-The dictionary will map flags to the name given minus ONE \"-\" sign in\n",
      "    front. If you use TWO minus signs in the flag name (i.e. --verbose), \n",
      "    the dictionary key will be the name with ONE minus sign in front \n",
      "    (i.e. {\"-verbose\":True}).\n",
      "\n",
      "\n",
      "Arguments:\n",
      "    rawargs (list): List of positional/keyword arguments. As obtained from\n",
      "                     sys.argv.\n",
      "\n",
      "Returns:\n",
      "    list: List of positional arguments (i.e. arguments without flags),\n",
      "            in order provided.\n",
      "    dict: Dictionary mapping flag (key is flag minus the first \"-\") and\n",
      "            their values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "980T\n",
      "Returns a dictionary that maps coordinates to a list of genes that occur at that coordinate.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in .prot_table or GFF3 format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of position to list of genes that share that position.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "981T\n",
      "Returns a dictionary that maps coordinates to a list of genes that occur at that coordinate.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in .prot_table or GFF3 format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of position to list of genes that share that position.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "982T\n",
      "Returns a dictionary that maps gene id to gene information.\n",
      "\n",
      "Arguments:\n",
      "    path (str): Path to annotation in .prot_table or GFF3 format.\n",
      "\n",
      "Returns:\n",
      "    dict: Dictionary of gene id to tuple of information:\n",
      "        - name\n",
      "        - description\n",
      "        - start coordinate\n",
      "        - end coordinate\n",
      "        - strand\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "983T\n",
      "Normalizes the input datasets and outputs the result in CombinedWig format.\n",
      "\n",
      "Arguments:\n",
      "    dataset_list (list): List of paths to datasets in .wig format\n",
      "    annotationPath (str): Path to annotation in .prot_table or GFF3 format.\n",
      "    outputPath (str): Desired output path.\n",
      "    normchoice (str): Choice for normalization method.\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "984T\n",
      "Normalizes the input datasets and outputs the result in CombinedWig format.\n",
      "\n",
      "Arguments:\n",
      "    dataset_list (list): List of paths to datasets in .wig format\n",
      "    annotationPath (str): Path to annotation in .prot_table or GFF3 format.\n",
      "    outputPath (str): Desired output path.\n",
      "    normchoice (str): Choice for normalization method.\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "985T\n",
      "Returns a tuple of (data, position) containing a matrix of raw read-counts\n",
      "    , and list of coordinates. \n",
      "\n",
      "Arguments:\n",
      "    wig_list (list): List of paths to wig files.\n",
      "    wxobj (object): wxPython GUI object for warnings\n",
      "\n",
      "Returns:\n",
      "    tuple: Two lists containing data and positions of the wig files given.\n",
      "\n",
      ":Example:\n",
      "\n",
      "    >>> import pytransit.tnseq_tools as tnseq_tools\n",
      "    >>> (data, position) = tnseq_tools.get_validated_data([\"data/glycerol_H37Rv_rep1.wig\", \"data/glycerol_H37Rv_rep2.wig\"])\n",
      "    >>> print(data)\n",
      "    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "           [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n",
      "\n",
      ".. seealso:: :class:`get_file_types` :class:`combine_replicates` :class:`get_data_zero_fill` :class:`pytransit.norm_tools.normalize_data`\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "986T\n",
      "Parse a newick phylogeny, provided either via a file or a string. The tree does not need to be bifurcating, and may be rooted or unrooted.\n",
      "Returns a Node object along which sequences may be evolved.  \n",
      "    \n",
      "Trees can either read from a file or given directly to ``read_tree`` as a string. One of these two keyword arguments is required.\n",
      "\n",
      "    1. **file**, the name of the file containing a newick tree for parsing. If this argument is provided in addition to tstring, the tree in the file will be used and tstring will be ignored.\n",
      "    2. **tree**, a newick tree string. If a file is additionally provided, the tstring argument will be ignored.   \n",
      "\n",
      "Optional keyword arguments:\n",
      "    1. **scale_tree** is a float value for scaling all branch lengths by a given multiplier. Default: 1.\n",
      "\n",
      "To implement branch (temporal) heterogeneity, place \"model flags\" at particular nodes within the tree. Model flags can be specified with either underscores (_) or hashtags (#), through one of two paradigms:\n",
      "    + Using trailing and leading symbols, e.g. _flagname_ or #flagname# . Specifying a model flag with this format will cause ALL descendents of that node to also follow this model, unless a new model flag is given downstream.\n",
      "    + Using *only a leading* symbol, e.g. _flagname or #flagname. Specifying a model flag with this format will cause ONLY that branch/edge to use the provided model. Descendent nodes will NOT inherit this model flag. Useful for changing model along a single branch, or towards a single leaf.\n",
      "\n",
      "Model flags may be repeated throughout the tree, but the model associated with each model flag will always be the same. Note that these model flag names **must** have correspondingly named model objects.\n",
      "\n",
      "**IMPORTANT**: Node names must be provided BEFORE a branch length, and model flags be provided AFTER a branch length. For example, this subtree is correct: \"...(taxon1:0.5, taxon2:0.2)<NODENAME>:<BL><MODEL FLAG>)...\". This subtree is *incorrect* and will raise a cryptic error: \"...(taxon1:0.5, taxon2:0.2):<BL><NODENAME><MODEL FLAG>)...\". \n",
      "\n",
      "\n",
      "Examples:\n",
      "    .. code-block:: python\n",
      "        \n",
      "       tree = read_tree(file = \"/path/to/tree/file.tre\")\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762):0.921):0.207);\")\n",
      "       \n",
      "       # Tree containing model flags named m1 and m2, both of which propagate to descendents.\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762_m1_):0.921)_m2_:0.207);\"\n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762#m1#):0.921)#m2#:0.207);\"\n",
      "\n",
      "\n",
      "       # Tree containing model flags named m1 and m2, each of which applies only to that branch.\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660_m1):0.762_m2):0.921):0.207);\"\n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660#m1):0.762#m2):0.921):0.207);\"\n",
      "\n",
      "\n",
      "       # Tree with a node demonstrating how to provide both a node name and model flag\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660)NODENAME:0.762_m1_):0.921):0.207);\" # propagating model flag\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762):0.921)NODENAME:0.207#m1);\" # non-propagating model flag\n",
      "\n",
      "\n",
      "       # Tree containing model flags named m1 and m2, where m1 is branch-specific but m2 is propagating.\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660#m1):0.762#m2#):0.921):0.207);\" \n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660_m1):0.762_m2_):0.921):0.207);\"\n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660_m1):0.762#m2#):0.921):0.207);\"\n",
      "       #or\n",
      "       tree = read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660#m1):0.762_m2_):0.921):0.207);\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "987T\n",
      "Prints a Node object in graphical, nested format. \n",
      "This function takes two arguments:\n",
      "    \n",
      "    1. **tree** is a Node object to print\n",
      "    2. **level** is used internally for printing. DO NOT PROVIDE THIS ARGUMENT.\n",
      "\n",
      "Each node in the tree is represented by a string in the format, \"name   branch.length   model.flag\", and levels are represented by indentation.\n",
      "Names for tree tips are taken directly from the provided tree, and internal node names are assigned automatically by the ``read_tree`` function.\n",
      "The node with a branch length of None will be the root node where sequence evolution will begin.\n",
      "Note that the model.flag field will be None under cases of branch homogeneity.       \n",
      "\n",
      "For example,\n",
      "    .. code-block:: python\n",
      "    \n",
      "       >>> my_tree = newick.read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762):0.921):0.207);\")\n",
      "       >>> print_tree(my_tree)\n",
      "            root None None\n",
      "                t4 0.785 None\n",
      "                    internalNode3 0.207 None\n",
      "                        t3 0.38 None\n",
      "                        internalNode2 0.921 None\n",
      "                            t2 0.806 None\n",
      "                            internalNode1 0.762 None\n",
      "                                t5 0.612 None\n",
      "                                t1 0.66 None\n",
      "    \n",
      "       >>> flagged_tree = newick.read_tree(tree = \"(t4:0.785,(t3:0.380,(t2:0.806,(t5:0.612,t1:0.660):0.762_m1_):0.921_m2_):0.207);\")\n",
      "       >>> newick.print_tree(flagged_tree)  \n",
      "             root None None\n",
      "                t4 0.785 None\n",
      "                internalNode3 0.207 None\n",
      "                    t3 0.38 None\n",
      "                    internalNode2 0.921 m2\n",
      "                        t2 0.806 m2\n",
      "                        internalNode1 0.762 m1\n",
      "                            t5 0.612 m1\n",
      "                            t1 0.66 m1\n",
      "\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "988T\n",
      "Read a model flag id while parsing the tree from the function _parse_tree. Flags must come **after** the branch length associated with that node, before the comma.\n",
      "Model flags can be indicated with either underscores (_) or hash signs (#). There are two strategies:\n",
      "    + Leading and trailing, e.g. #flag# or _flag_ . These flags will automatically propagate to all child branches.\n",
      "    + Trailing only, e.g. #flag or _flag. These flags will be applied *only* to the given branch.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "989T\n",
      "Read a provided internal node name while parsing the tree from the function _parse_tree.\n",
      "Importantly, internal node names *MAY NOT* contain colons!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "990T\n",
      "Recursively parse a newick tree string and convert to a Node object. \n",
      "Uses the functions _read_branch_length(), _read_leaf(), _read_model_flag() during the recursion.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "991T\n",
      "Parameters\n",
      "----------\n",
      "output_file: string or gzip.file_like\n",
      "pdb_input_file: string or gzip.file_like\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "992T\n",
      "Retrieves a SIFTS .xml file, given a PDB ID. Works by modifying the PDBe download URL.\n",
      "Also removes annoying namespace stuff.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "993T\n",
      "Retrieves a PDB file, given a PDB ID. Works by modifying the PDB download URL.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "994T\n",
      "Use Rosetta loopmodel to model missing loops in template structures.\n",
      "Completed templates are stored in templates/structures-modeled-loops\n",
      "\n",
      ":param process_only_these_templates: list of str\n",
      ":param loglevel: str\n",
      ":return:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "995T\n",
      "Parameters\n",
      "----------\n",
      "templates_full_seq: list of BioPython SeqRecord\n",
      "    full UniProt sequence for span of the template (including unresolved residues)\n",
      "process_only_these_templates: list of str\n",
      "overwrite_structures: bool\n",
      "Returns\n",
      "-------\n",
      "missing_residues_list: list of list of OpenMM Residue\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "996T\n",
      "Parameters\n",
      "----------\n",
      "template_full_seq: BioPython SeqRecord\n",
      "    full UniProt sequence for span of the template (including unresolved residues)\n",
      "overwrite_structures: bool\n",
      "Returns\n",
      "-------\n",
      "fixer.missingResidues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "997T\n",
      "Parameters\n",
      "----------\n",
      "templates:  list of BioPython SeqRecord\n",
      "    only the id is used\n",
      "missing_residues: list of list of OpenMM Residue\n",
      "process_only_these_templates: bool\n",
      "overwrite_structures: bool\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "998T\n",
      "Conducts pairwise alignments of target sequences against template sequences.\n",
      "Stores Modeller-compatible 'alignment.pir' files in each model directory,\n",
      "and also outputs a table of model IDs, sorted by sequence identity.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "process_only_these_targets:\n",
      "process_only_these_templates:\n",
      "substitution_matrix: str\n",
      "    Specify an amino acid substitution matrix available from Bio.SubsMat.MatrixInfo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "999T\n",
      "Parameters\n",
      "----------\n",
      "target: BioPython SeqRecord\n",
      "template: BioPython SeqRecord\n",
      "substitution_matrix: str\n",
      "    Specify an amino acid substitution matrix available from Bio.SubsMat.MatrixInfo\n",
      "gap_open: float or int\n",
      "gap_extend: float or int\n",
      "\n",
      "Returns\n",
      "-------\n",
      "alignment: list\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1002T\n",
      "Hacky attempt to get Modeller version by regex searching the installation directory or README file.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1003T\n",
      "Generate build_models metadata for a given target.\n",
      ":param target: BioPython SeqRecord\n",
      ":param target_setup_data:\n",
      ":return: metadata: dict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1004T\n",
      "Cluster models based on RMSD, and filter out non-unique models as\n",
      "determined by a given cutoff.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "cutoff : float\n",
      "    Minimum distance cutoff for RMSD clustering (nm)\n",
      "\n",
      "Runs serially.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1005T\n",
      "Use MSMBuilder to perform RMSD-based regular spatial clustering on a set of models.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "templateids: list of str\n",
      "traj: mdtraj.Trajectory\n",
      "atom_indices: np.array\n",
      "cutoff: float\n",
      "    Minimum distance cutoff for RMSD clustering (nm)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1006T\n",
      "Parameters\n",
      "----------\n",
      "args: dict\n",
      "required_args: list of str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1007T\n",
      "Searches the UniProt database given a search string, and retrieves an XML\n",
      "file, which is returned as a string.\n",
      "maxreadlength is the maximum size in bytes which will be read from the website\n",
      "(default 100MB)\n",
      "Example search string: 'domain:\"Protein kinase\" AND reviewed:yes'\n",
      "\n",
      "The function also removes the xmlns attribute from <uniprot> tag, as this\n",
      "makes xpath searching annoying\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1008T\n",
      "Examples of pdbref chains entries to be parsed:\n",
      "A=65-119             => {'A':[65,119]}\n",
      "A/C/E/G=64-121       => {'A':[64,121], 'B':[64,121], 'C':[64,121], 'D':[64,121]}\n",
      "A=458-778, B=764-778 => {'A':[458,778],'B':[764,778]}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1009T\n",
      "Calculate model quality using MolProbity ``oneline-analysis`` command.\n",
      "\n",
      "For each target, this function outputs a text file named\n",
      "``models/[targetid]/validation_scores_sorted-[method]-[ensembler_stage]`` which contains a list of\n",
      "targetids sorted by validation score. This can be used by the subsequent ``package_models`` command\n",
      "to filter out models below a specified quality threshold.\n",
      "\n",
      "Typically, this should be run after models have been refined to the desired extent (e.g. after\n",
      "implicit or explicit MD refinement)\n",
      "\n",
      "More detailed validation results are written to the individual model directories.\n",
      "\n",
      "MPI-enabled.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    targetids: list of str or str\n",
      "    modeling_stage: str\n",
      "        {None|build_models|refine_implicit_md|refine_explicit_md}\n",
      "        Default: None (automatically selects most advanced stage)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1010T\n",
      "To be used as an lxml XPath extension, for regex searches of attrib values.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "context: set automatically by lxml\n",
      "attrib_values: set automatically by lxml\n",
      "regex_str: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1011T\n",
      "To be used as an lxml XPath extension, for regex searches of attrib values.\n",
      "Parameters\n",
      "----------\n",
      "regex_str: str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1013T\n",
      "Wraps a sequence string to a width of 60.\n",
      "If add_star is set to true, an asterisk will be added\n",
      "to the end of the sequence, for compatibility with\n",
      "Modeller.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1014T\n",
      "Get model IDs for models which exist, for a given ensembler modeling stage.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "ensembler_stage: str\n",
      "    {refine_explicit_md|refine_implicit_md|build_models}\n",
      "targetid: str\n",
      "\n",
      "Returns\n",
      "-------\n",
      "valid_model_ids: list of str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1015T\n",
      "Parameters\n",
      "----------\n",
      "targetid: str\n",
      "seqid_cutoff: float\n",
      "\n",
      "Returns\n",
      "-------\n",
      "selected_templateids: list of str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1016T\n",
      "Parameters\n",
      "----------\n",
      "targetid: str\n",
      "validation_score_cutoff: float\n",
      "validation_score_percentile: float\n",
      "\n",
      "Returns\n",
      "-------\n",
      "selected_templateids: list of str\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1017T\n",
      "Create the input files and directory structure necessary to start a Folding@Home project.\n",
      "\n",
      "MPI-enabled.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "archive : Bool\n",
      "    A .tgz compressed archive will be created for each individual RUN directory.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1018T\n",
      "Calculate PME parameters using scheme similar to OpenMM OpenCL platform.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "system : simtk.openmm.System\n",
      "    The system for which parameters are to be computed.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "alpha : float\n",
      "    The PME alpha parameter\n",
      "nx, ny, nz : int\n",
      "    The grid numbers in each dimension\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1019T\n",
      "Ensure that the PME parameters in an OpenMM system are explicit.\n",
      "If they are not explicit, set them explicitly.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "system : simtk.openmm.System\n",
      "    System for which NonbondedForce PME parameters are to be set explicitly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1020T\n",
      "Build Folding@Home RUN and CLONE subdirectories from (possibly compressed) OpenMM serialized XML files.\n",
      "\n",
      "ARGUMENTS\n",
      "\n",
      "run (int) - run index\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1021T\n",
      "Safely parse a string representing a dict of kwargs to be passed to an API function.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "params_string: str\n",
      "\n",
      "Returns\n",
      "-------\n",
      "dict\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> mydict = parse_api_params_string('{\"a\": 3 / picoseconds, \"b\": \"x\", \"c\": 2.4}')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1022T\n",
      "Safely evaluate simtk quantities passed from CLI, using either Python expression syntax\n",
      "('2 * picoseconds' or '2 / picoseconds') or a more natural syntax ('2 picoseconds').\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "param_value_string: str\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> eval_quantity_string('2 picoseconds')\n",
      "Quantity(value=2, unit=picosecond)\n",
      ">>> eval_quantity_string('2 / picoseconds')\n",
      "Quantity(value=2, unit=/picosecond)\n",
      ">>> eval_quantity_string('2')\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1023T\n",
      "Queries a TargetExplorer DB API database given the URI and a search string,\n",
      "and returns data as a JSON string.\n",
      "maxreadlength is the maximum size in bytes which will be read from the website\n",
      "(default 10MB)\n",
      "The search string uses SQLAlchemy syntax and standard TargetExplorer\n",
      "frontend data fields.\n",
      "Example: 'species=\"Human\"'\n",
      "Or to select all domains in the DB: ''\n",
      "If full_seqs=True, the DB API will also return the full-length canonical\n",
      "isoform sequences.\n",
      "return_data: str e.g. 'seqs' or list e.g. ['domain_seqs', 'seqs']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1025T\n",
      "Gets metadata for a TargetExplorer DB, via the network API.\n",
      "Metadata is returned as a JSON string.\n",
      "maxreadlength is the maximum size in bytes which will be read from the website\n",
      "(default 100kB)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1029T\n",
      ":param dbapi_uri: str\n",
      ":param search_string: str\n",
      ":return: list containing nested lists and dicts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1033T\n",
      "Returns the installation path of a resource file shipped with the code.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "name: str\n",
      "    Name of the file to load (relative to the `ensembler` main code directory).\n",
      "\n",
      "Returns\n",
      "-------\n",
      "installed_filepath: str\n",
      "    absolute path of the installed file\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> pathname = get_installed_resource_filename('tests/example_project/meta0.yaml')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1034T\n",
      "Run MD refinement in implicit solvent.\n",
      "\n",
      "MPI-enabled.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1037T\n",
      "Applies custom residue names to a list of residue names.\n",
      "Acts on `variants` list in-place.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "variants: list of str\n",
      "    typically generated from openmm.app.modeller.addHydrogens\n",
      "custom_variants_dict: dict\n",
      "    keyed by 0-based residue index. Values should be residue name string.\n",
      "    e.g. {35: 'HID'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1038T\n",
      "Solvate models which have been subjected to MD refinement with implicit solvent.\n",
      "\n",
      "MPI-enabled.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1039T\n",
      "Determine distribution of nwaters, and select the value at a certain percentile.\n",
      "If not user-specified, the percentile is set to 100 if there are less than 10 templates, otherwise it is set to 68.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1042T\n",
      "This function reads the gff3 input file and returns the information in an\n",
      "internal data structure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1043T\n",
      "Goal of this function is to append more data to and\n",
      "existing HDF5 data entry.\n",
      "The dimensions other than the appending dimension have to match.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1044T\n",
      "Given a sorted list of introns, return the subset of introns that\n",
      "overlaps that start stop interval\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1045T\n",
      "This function takes a list of bam files and a set of coordinates (chrm, start, stop), to \n",
      "plot a coverage heatmap over all files in that region.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1046T\n",
      "This function takes a gene and its corresponding segment and edge counts to\n",
      "produce a coverage overview plot.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1047T\n",
      "This function takes a list of bam files and a set of coordinates (chrm, start, stop), to \n",
      "plot a coverage overview of that files in that region.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for index, row in data.iterrows():\n",
    "    gpt_output = row['gpt_output']\n",
    "    if len(gpt_output) >= 3 and gpt_output[2] == 'F':\n",
    "        pass\n",
    "    else:\n",
    "        print(str(index) + 'T')\n",
    "        print(row['comments'])\n",
    "        print('\\n\\n\\n\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['True', ['licence', 'name', 'acronym', 'strip', 'return'], '80']\n",
      "Choose the contigs to cover the reference gene.\n",
      "\n",
      "In this stitching iteration we are assembling the exon with gaps.\n",
      "24\n",
      "1026\n",
      "['True', ['licence', 'name', 'acronym', 'strip', 'return'], '80']\n",
      "True\n",
      "['True', ['licence', 'name', 'acronym', 'strip', 'return'], '80']\n",
      "['True', ['licence', 'name', 'acronym', 'strip', 'return'], '80']\n",
      "0       ['True', ['numpy', 'series', 'fraction', 'weig...\n",
      "1       ['True', ['weighted', 'signal', 'array', 'conv...\n",
      "2       ['True', ['convolve', 'weighted', 'array', 'bi...\n",
      "3       ['True', ['window', 'size', 'signal', 'Silverm...\n",
      "4       ['True', ['Kaiser window', 'smooth', 'numeric ...\n",
      "                              ...                        \n",
      "1043    ['True', ['dictionary', 'recursively', 'copy',...\n",
      "1044    ['True', ['BED', 'compressed', 'contigs', 'seq...\n",
      "1045    ['True', ['BED', 'BAM file', 'linearly', 'proc...\n",
      "1046    ['True', ['OptionParser', 'logging', 'config',...\n",
      "1047    ['True', ['list', 'line-breaks', 'minimize', '...\n",
      "Name: gpt_output, Length: 1026, dtype: object\n",
      "1026\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "num_edit = [105, 202, 203, 428, 466, 473, 555, 587, 588, 664, 713, 714, 715, 720, 804, 808, 824, 870, 871, 892, 1001, 1026, 1027, 1028]\n",
    "print(data.loc[555]['gpt_output'])\n",
    "print(data.loc[555]['comments'])\n",
    "print(len(num_edit))\n",
    "run_on_this = data\n",
    "print(len(run_on_this))\n",
    "gpt_value = run_on_this.loc[555]['gpt_output']\n",
    "print(gpt_value)\n",
    "gpt_value = ast.literal_eval(gpt_value)\n",
    "print(gpt_value[0])\n",
    "gpt_value[0] = 'True'\n",
    "print(gpt_value)\n",
    "gpt_value = str(gpt_value)\n",
    "run_on_this.loc[555, 'gpt_output'] = gpt_value\n",
    "print(run_on_this.loc[555]['gpt_output'])\n",
    "\n",
    "for location in num_edit:\n",
    "    gpt_value = run_on_this.loc[location]['gpt_output']\n",
    "    gpt_value = ast.literal_eval(gpt_value)\n",
    "    gpt_value[0] = 'True'\n",
    "    gpt_value = str(gpt_value)\n",
    "    run_on_this.loc[location, 'gpt_output'] = gpt_value\n",
    "print(run_on_this['gpt_output'])\n",
    "print(len(run_on_this['gpt_output']))\n",
    "\n",
    "run_on_this.to_json('/home/ubuntu/Bio-Code-Eval/Bio-Code-Eval-new/final_python_functions_for_testing.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_on_this' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(run_on_this\u001b[39m.\u001b[39mloc[\u001b[39m555\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_on_this' is not defined"
     ]
    }
   ],
   "source": [
    "print(run_on_this.loc[555]['prompt'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b672c9389238a9ffb5ee1758d18c8ffbf737bb80d79b3a7488d1e5058fdac62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
